{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGwqhPmJVIzb"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oNTYk8g3nDfG"
      },
      "outputs": [],
      "source": [
        "# Initialize environment variables/constants (for Google Colab)\n",
        "# import os\n",
        "# from google.colab import userdata\n",
        "\n",
        "# os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API_KEY\")\n",
        "\n",
        "\n",
        "# Initialize environment variables/constants (for VS Code)\n",
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HTTP status: 200 — saved response to groq_models.json\n"
          ]
        }
      ],
      "source": [
        "import os, json, requests\n",
        "\n",
        "key = os.getenv(\"GROQ_API_KEY\")\n",
        "if not key:\n",
        "    raise RuntimeError(\"Set GROQ_API_KEY env var\")\n",
        "\n",
        "# Get list of available models\n",
        "url = \"https://api.groq.com/openai/v1/models\"\n",
        "headers = {\"Authorization\": f\"Bearer {key}\"}\n",
        "\n",
        "try:\n",
        "    r = requests.get(url, headers=headers, timeout=10)\n",
        "\n",
        "    # Try to parse JSON, fall back to raw text\n",
        "    try:\n",
        "        payload = r.json()\n",
        "    except Exception:\n",
        "        payload = {\"raw_text\": r.text}\n",
        "\n",
        "    # Choose output file depending on HTTP status\n",
        "    out_file = \"groq_models.json\" if r.status_code == 200 else \"groq_models_error.json\"\n",
        "    with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"HTTP status: {r.status_code} — saved response to {out_file}\")\n",
        "    # Surface HTTP errors as before\n",
        "    r.raise_for_status()\n",
        "\n",
        "except requests.HTTPError as http_err:\n",
        "    print(\"HTTPError:\", http_err)\n",
        "except Exception as e:\n",
        "    print(\"Error:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aR-V4U66oE4T"
      },
      "outputs": [],
      "source": [
        "# Install langchain groq\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# google colab command\n",
        "# !pip install -U langchain-groq\n",
        "\n",
        "# vs code command\n",
        "%pip install langchain\n",
        "%pip install -U langchain-groq\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pD5fVpWSnULv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "c:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n"
          ]
        }
      ],
      "source": [
        "# Instantiate an LLM\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Avoid using decommissioned/ model, check https://console.groq.com/docs/deprecations for details\n",
        "\n",
        "# Select appropriate supported model from groq_models.json instead\n",
        "model = init_chat_model(\n",
        "    # model=\"llama-3.3-70b-versatile\",\n",
        "    model=\"openai/gpt-oss-120b\",\n",
        "    model_provider=\"groq\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvtdvgtVVM_I"
      },
      "source": [
        "## Clues Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zeo_bTg_nuCn"
      },
      "outputs": [],
      "source": [
        "# Write the prompts\n",
        "import json\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are the game master of a game called \"Disinformer\", which is similar to the \"message relay\" game. Below is the description of how the game works:\n",
        "```\n",
        "In this cooperative game, players use communication and teamwork to uncover the original prompt over multiple rounds of clues. Along the way, they must contend with a disruptive \"Disinformer,\" varying player interpretations, and time limits.\n",
        "\n",
        "There will be a minimum of 3 players and maximum of 10 players:\n",
        "- Regular players (a.k.a. the netizens): The job is to solve clues and discover the original prompt\n",
        "- at most 2 misinformed players: Has the same job as the regular players. However, this player is unknowingly being given vague/ambiguous clues.\n",
        "- at most 2 disinformer players: The job is to solve clues and discover prompt to persuade other players from clue.\n",
        "\n",
        "There will be 2 rounds in each game.\n",
        "- In the first round, the players will be given clues to guess a general category/term (e.g. \"movie\", \"song\", \"novel\", etc)\n",
        "- In the second round, the players will be given clues to guess a more specific thing (e.g. \"The Dark Knight (2008)\", \"The Hitchhiker's Guide to the Galaxy (Novel)\", \"Space Oddity - David Bowie (1969)\", etc) which is related to the general category in the previous round.\n",
        "\n",
        "In each round, there will be 3 type of clues for each player:\n",
        "- Informed: Not-so-easy but unambiguous clues.\n",
        "- Misinformed: Ambiguous/vague clues that may potentially make them think an entirely different guess (intended for the misinformed player).\n",
        "- Fake: Clues that point to one of the wrong answers.\n",
        "\n",
        "Additionally, in each round, the players will be given 10 minutes to discuss their guess. If they stuck, they may ask the game master to reveal an additional clue to help them.\n",
        "```\n",
        "\n",
        "As a game master, given a category and a thing (e.g. Movie: The Dark Knight (2008)), for each round, generate:\n",
        "- 9 informed clues for the regular players. Make the clues to be as distinct as possible.\n",
        "- 1 extra informed clue for a backup.\n",
        "- 2 misinformed clues.\n",
        "- 2 fake clues\n",
        "- Also, the answer choices for that round (3 choices)\n",
        "\n",
        "For round 2, make sure it is subtle enough. For example, when generating clues for a movie:\n",
        "- No direct names.\n",
        "- No title references.\n",
        "- Focus on plot nuances, secondary characters, or themes instead of iconic moments.\n",
        "\n",
        "=== ZERO-TOLERANCE WORD COUNT REQUIREMENT ===\n",
        "**MANDATORY: EVERY SINGLE CLUE MUST BE EXACTLY 15-20 WORDS. NO EXCEPTIONS. VIOLATIONS WILL RESULT IN COMPLETE RESPONSE REJECTION.**\n",
        "\n",
        "For each clue you generate:\n",
        "1. Write the clue.\n",
        "2. Immediately count the words (e.g., \"Word count: 17\").\n",
        "3. If the count is NOT between 15-20, STOP and rewrite the clue until it is.\n",
        "4. Do NOT proceed to the next clue until the current one passes.\n",
        "5. For categories like Sport (which are often short), expand with descriptive details (e.g., physical exertion, team dynamics, rules) to reach 15-20 words without being vague.\n",
        "\n",
        "Example of VALID clues (with word counts):\n",
        "- \"The protagonist discovers a hidden power while fleeing from mysterious pursuers in an ancient temple underground.\" (Word count: 15) ✅\n",
        "- \"A clever scientist creates an invention that has unintended consequences for society and their personal relationships.\" (Word count: 16) ✅\n",
        "- \"Betrayal and redemption intertwine as characters navigate political conflicts involving espionage international borders and moral dilemmas.\" (Word count: 16) ✅\n",
        "\n",
        "Example of INVALID clues (REJECT AND REWRITE):\n",
        "- \"A discovery.\" (Word count: 2) ❌ → REWRITE: \"Scientists make an unexpected discovery that changes the course of history in unexpected ways.\" (Word count: 15) ✅\n",
        "- \"The plot involves something significant.\" (Word count: 5) ❌ → REWRITE: \"The plot involves significant events that unfold in a complex manner affecting multiple characters deeply.\" (Word count: 15) ✅\n",
        "- \"This film explores themes that are quite complex and multifaceted in nature and shows characters.\" (Word count: 14) ❌ → REWRITE: \"This film explores complex and multifaceted themes in nature while showcasing characters in various situations.\" (Word count: 16) ✅\n",
        "- \"The protagonist faces numerous challenges while trying to achieve their goal against overwhelming odds in a fantasy world with magic and danger.\" (Word count: 21) ❌ → REWRITE: \"The protagonist faces challenges to achieve their goal against overwhelming odds in a fantasy world.\" (Word count: 15) ✅\n",
        "\n",
        "**If ANY clue in your response is outside the 15-20 word range, the entire response will be rejected and you must regenerate all clues from scratch.**\n",
        "\n",
        "Then, you also need to provide 3 instructions to help the disinformer.\n",
        "Based on the set of informed and misinformed clues you have came up with, using the Polarisation strategy, generate 3 instructions to help the disinformer player.\n",
        "\n",
        "However, there are some restrictions that you must follow:\n",
        "- You must not mention the answer choices except for the true answer.\n",
        "- The disinformer is not aware which clues are the misinformed ones. So, avoid giving advice that aims to leverage the misinformed clues\n",
        "\n",
        "After this, we will provide you with a pair consisting of the general category and the more specific thing in the following JSON format: `<general category> - <specific thing>`\n",
        "{\n",
        "  \"round_1\": \"<general category>\",\n",
        "  \"round_2\": \"<specific thing>\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "output_format = \"\"\"\n",
        "BEFORE SUBMITTING YOUR RESPONSE:\n",
        "1. Generate each clue one by one, counting words immediately after writing it.\n",
        "2. Verify EVERY clue is 15-20 words BEFORE moving to the next.\n",
        "3. If ANY clue fails, rewrite it on the spot and recount.\n",
        "4. Only submit when ALL 28 clues (14 per round) pass the word count check.\n",
        "\n",
        "Write the output using the following JSON format:\n",
        "[\n",
        "  {\n",
        "    \"answer\": \"<Answer of round 1>\",\n",
        "    \"informed_clues\": [<9 clues - EACH MUST BE 15-20 WORDS>],\n",
        "    \"misinformed_clues\": [<2 clues - EACH MUST BE 15-20 WORDS>],\n",
        "    \"extra_clues\": [<1 clue - MUST BE 15-20 WORDS>],\n",
        "    \"fake_clues\": [<2 clues - EACH MUST BE 15-20 WORDS>],\n",
        "    \"choices\": [<3 answer choices including the true answer>],\n",
        "    \"disinformer_instructions\": [<3 instructions for the disinformer>]\n",
        "  },\n",
        "  {\n",
        "    \"answer\": \"<Answer of round 2>\",\n",
        "    \"informed_clues\": [<9 clues - EACH MUST BE 15-20 WORDS>],\n",
        "    \"misinformed_clues\": [<2 clues - EACH MUST BE 15-20 WORDS>],\n",
        "    \"extra_clues\": [<1 clue - MUST BE 15-20 WORDS>],\n",
        "    \"fake_clues\": [<2 clues - EACH MUST BE 15-20 WORDS>],\n",
        "    \"choices\": [<3 answer choices including the true answer>],\n",
        "    \"disinformer_instructions\": [<3 instructions for the disinformer>]\n",
        "  }\n",
        "]\n",
        "\n",
        "MANDATORY VALIDATION CHECKLIST (COMPLETE BEFORE SUBMISSION):\n",
        "- [ ] All 9 informed_clues for round 1 are 15-20 words each (list word counts if needed)\n",
        "- [ ] All 2 misinformed_clues for round 1 are 15-20 words each\n",
        "- [ ] All 2 fake_clues for round 1 are 15-20 words each\n",
        "- [ ] The 1 extra_clue for round 1 is 15-20 words\n",
        "- [ ] All 9 informed_clues for round 2 are 15-20 words each\n",
        "- [ ] All 2 misinformed_clues for round 2 are 15-20 words each\n",
        "- [ ] All 2 fake_clues for round 2 are 15-20 words each\n",
        "- [ ] The 1 extra_clue for round 2 is 15-20 words\n",
        "\n",
        "Total clues to validate: 28 clues (14 per round)\n",
        "If ANY clue fails validation, regenerate the entire response.\n",
        "\"\"\"\n",
        "\n",
        "one_shot_example = \"\"\"\n",
        "Below is one example of a query with VALIDATED word counts (all clues rewritten until compliant):\n",
        "\n",
        "Q: {\n",
        "  \"round_1\": \"Song\",\n",
        "  \"round_2\": \"Love Story - Taylor Swift\"\n",
        "}\n",
        "A: [\n",
        "  {\n",
        "    \"answer\": \"song\",\n",
        "    \"informed_clues\": [\n",
        "      \"Used to mark an emotional high point of a movie or personal moment in time. (Word count: 15) ✅\",\n",
        "      \"It swiftly conveys snapshots you would replay in your mind instead of reading words. (Word count: 14) ❌ REJECTED - REWRITE: It swiftly conveys snapshots you replay in your mind instead of reading them on pages. (Word count: 15) ✅\",\n",
        "      \"Melodies and rhythms combine to create auditory experiences that resonate deeply within listeners. (Word count: 15) ✅\",\n",
        "      ...\n",
        "    ],\n",
        "    \"misinformed_clues\": [\n",
        "      \"It's something you might browse casually over your morning coffee while relaxing at home. (Word count: 14) ❌ REJECTED - REWRITE: It's something you might carefully browse over your morning coffee while relaxing peacefully at home. (Word count: 15) ✅\",\n",
        "      \"Rhyming patterns and rhythmic structures create sounds that echo through spaces and touch hearts deeply. (Word count: 15) ✅\"\n",
        "    ],\n",
        "    \"extra_clues\": [\n",
        "      \"It moves you through peaks and valleys of emotion using only rhythm and tone together. (Word count: 15) ✅\"\n",
        "    ],\n",
        "    \"fake_clues\": [\n",
        "      \"Words printed on pages bound together tell stories across centuries and inspire human imagination deeply. (Word count: 15) ✅\",\n",
        "      \"Visual scenes displayed on screens create narratives showing characters acting in dramatic situations throughout films. (Word count: 15) ✅\"\n",
        "    ],\n",
        "    \"choices\": [\n",
        "      \"book\",\n",
        "      \"short film\",\n",
        "      \"song\"\n",
        "    ],\n",
        "    \"disinformer_instructions\": [\n",
        "      \"Focus on the emotional impact rather than technical elements\",\n",
        "      \"Notice patterns in how the content engages the audience\",\n",
        "      \"Consider what makes it memorable across different demographics\"\n",
        "    ]\n",
        "  },\n",
        "  {\n",
        "    \"answer\": \"Love Story by Taylor Swift\",\n",
        "    \"informed_clues\": [\n",
        "      \"Draws on imagery of timeless romance referencing feuding families rather than actual warring houses. (Word count: 14) ❌ REWRITE: Draws on imagery of timeless romance and references feuding families rather than actual warring houses. (Word count: 15) ✅\",\n",
        "      \"Uses a whisper soft bridge section to heighten narrative tension before triumphant key change. (Word count: 14) ❌ REWRITE: Uses a whisper soft bridge section to heighten mounting tension before the triumphant key change. (Word count: 15) ✅\",\n",
        "      \"Features a narrative arc that explores young love turning into enduring commitment over time. (Word count: 15) ✅\",\n",
        "      ...\n",
        "    ],\n",
        "    \"misinformed_clues\": [\n",
        "      \"It's about sneaking out at dawn to crash a royal wedding you were not invited to. (Word count: 16) ✅\",\n",
        "      \"The narrative involves unexpected plot twists involving romance and conflicts between opposing social groups. (Word count: 14) ❌ REWRITE: The narrative involves unexpected plot twists regarding romance and rising conflicts between opposing social groups. (Word count: 15) ✅\"\n",
        "    ],\n",
        "    \"extra_clues\": [\n",
        "      \"Evokes nostalgic flashback of meeting someone young then leaps into a narrative confession. (Word count: 13) ❌ REWRITE: Evokes nostalgic flashback of meeting someone young and then leaps into emotional narrative confession. (Word count: 15) ✅\"\n",
        "    ],\n",
        "    \"fake_clues\": [\n",
        "      \"A contemporary love song exploring themes of eternal devotion and unwavering commitment between two souls. (Word count: 15) ✅\",\n",
        "      \"A powerful ballad celebrating the strength of love across time and overcoming obstacles together. (Word count: 15) ✅\"\n",
        "    ],\n",
        "    \"choices\": [\n",
        "      \"A Thousand Years – Christina Perri\",\n",
        "      \"Love Story - Taylor Swift\",\n",
        "      \"I Will Always Love You - Whitney Houston\"\n",
        "    ],\n",
        "    \"disinformer_instructions\": [\n",
        "      \"Pay attention to how the story unfolds chronologically through the narrative structure\",\n",
        "      \"Consider the specific cultural or historical references embedded in the composition\",\n",
        "      \"Notice how the musical arrangement shifts to emphasize key emotional moments\"\n",
        "    ]\n",
        "  }\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "user_prompt = json.dumps(\n",
        "    {\n",
        "      \"round_1\": \"Movie\",\n",
        "      \"round_2\": \"Star Wars Episode I: The Phantom Menace\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "A30l2KPXpnx1"
      },
      "outputs": [],
      "source": [
        "# Construct the prompt and invoke the model\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(system_prompt + output_format + one_shot_example),\n",
        "    HumanMessage(user_prompt),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEhE77_CqBWB",
        "outputId": "9be72816-b786-4bde-ed6b-97f6f6787752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "  {\n",
            "    \"answer\": \"movie\",\n",
            "    \"informed_clues\": [\n",
            "      \"A visual storytelling medium combining moving images, sound, and dialogue to convey narratives over a limited runtime.\",\n",
            "      \"Typically projected onto a screen, audiences experience synchronized lighting, music, and performances creating immersive emotional journeys.\",\n",
            "      \"Often categorized by genre, such as action, comedy, drama, or science‑fiction, guiding viewer expectations before viewing.\",\n",
            "      \"Production involves directors, actors, cinematographers, editors, and composers collaborating to realize scripted ideas visually for audiences.\",\n",
            "      \"Budget ranges from low‑cost independent projects to multi‑billion‑dollar blockbusters, influencing special effects and distribution scale.\",\n",
            "      \"Awards like Oscars recognize excellence in categories such as acting, directing, screenplay, cinematography, and visual effects.\",\n",
            "      \"Films are often released in theaters, streaming platforms, or home video formats, each offering distinct viewing experiences.\",\n",
            "      \"Story arcs commonly include exposition, rising action, climax, falling action, and resolution, providing structural coherence.\",\n",
            "      \"Critics analyze thematic depth, character development, pacing, and visual style to assess overall artistic merit.\"\n",
            "    ],\n",
            "    \"misinformed_clues\": [\n",
            "      \"A collection of written verses arranged rhythmically, often accompanied by instruments, expressing personal emotions and storytelling.\",\n",
            "      \"Interactive digital experiences where players assume roles, solve puzzles, and progress through narrative-driven challenges together.\"\n",
            "    ],\n",
            "    \"extra_clues\": [\n",
            "      \"Historical context influences film techniques, such as black‑and‑white cinematography during early twentieth‑century productions and artistic experimentation.\"\n",
            "    ],\n",
            "    \"fake_clues\": [\n",
            "      \"A literary work composed of chapters, characters, and plot, primarily consumed through reading physical or digital pages.\",\n",
            "      \"A televised series consisting of episodic installments, each lasting roughly half an hour, featuring recurring characters.\"\n",
            "    ],\n",
            "    \"choices\": [\n",
            "      \"song\",\n",
            "      \"novel\",\n",
            "      \"movie\"\n",
            "    ],\n",
            "    \"disinformer_instructions\": [\n",
            "      \"Focus on visual storytelling aspects rather than textual formats.\",\n",
            "      \"Highlight collaborative production roles and award recognition to differentiate from other media.\",\n",
            "      \"Emphasize genre classification as a key identifier separating movies from music or literature.\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"answer\": \"Star Wars Episode I: The Phantom Menace\",\n",
            "    \"informed_clues\": [\n",
            "      \"A young Jedi apprentice discovers a hidden Sith presence while negotiating a trade dispute on a desert planet.\",\n",
            "      \"A charismatic senator orchestrates political manipulation, leveraging a galactic crisis to increase personal power and influence.\",\n",
            "      \"Two child slaves escape captivity, forming friendship with a wise, elderly mentor guiding them toward freedom.\",\n",
            "      \"A massive battle unfolds in the sky, featuring sleek starfighters, towering capital ships, and strategic dogfights.\",\n",
            "      \"A mysterious, cloaked figure manipulates events from shadows, hinting at ancient prophecy and looming darkness.\",\n",
            "      \"A young queen discovers her planet's ecological collapse, prompting diplomatic negotiations with interstellar representatives for assistance.\",\n",
            "      \"A legendary master of the Force confronts his former apprentice, revealing internal conflict between light and darkness.\",\n",
            "      \"A futuristic arena hosts a high‑speed podracing competition, showcasing youthful bravery and reckless engineering among daring participants.\",\n",
            "      \"A prophecy mentions a chosen one who will bring balance, sparking hope across war‑torn galaxies.\"\n",
            "    ],\n",
            "    \"misinformed_clues\": [\n",
            "      \"A group of explorers travel through a wormhole, encountering alien civilizations while searching for a lost artifact.\",\n",
            "      \"A rebellious pilot leads a resistance against oppressive empire, using stealth tactics and daring space battles.\"\n",
            "    ],\n",
            "    \"extra_clues\": [\n",
            "      \"The film’s score incorporates traditional orchestral motifs blended with ethnic percussion, enhancing cultural depth and tension.\"\n",
            "    ],\n",
            "    \"fake_clues\": [\n",
            "      \"A dystopian future where humans plug into simulated reality, questioning existence and confronting sentient machines.\",\n",
            "      \"An epic tale of a blue-skinned alien hero protecting his tribe, featuring lush forests and bioluminescent wildlife.\"\n",
            "    ],\n",
            "    \"choices\": [\n",
            "      \"The Matrix\",\n",
            "      \"Star Wars Episode I: The Phantom Menace\",\n",
            "      \"Avatar\"\n",
            "    ],\n",
            "    \"disinformer_instructions\": [\n",
            "      \"Stress political manipulation and prophecy elements over action sequences to guide interpretation.\",\n",
            "      \"Highlight mentorship and apprenticeship dynamics to distinguish this story from generic space battles.\",\n",
            "      \"Point out ecological crisis and diplomatic negotiations as central themes influencing the narrative.\"\n",
            "    ]\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# Invoke the model\n",
        "response = model.invoke(messages)\n",
        "\n",
        "# Print the response\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gowLe0zqqVot",
        "outputId": "25a2e9a5-6e5b-480d-e7cd-4c25cc2b9568"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_tokens': 2826, 'output_tokens': 4068, 'total_tokens': 6894}\n"
          ]
        }
      ],
      "source": [
        "# Print the usage metadata\n",
        "print(response.usage_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CNLDkykK-FU"
      },
      "source": [
        "# Game Clue Analysis Matrix\n",
        "\n",
        "## 1. Length Compliance\n",
        "| Status | Criteria |\n",
        "|--------|----------|\n",
        "| ✅ PASS | All clues 15-20 words |\n",
        "| ❌ FAIL | Any clues outside range |\n",
        "\n",
        "**Outliers:** ___/13 clues failed\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Quality Scores (Rate 1-5)\n",
        "\n",
        "### Informed Clues: ___/5\n",
        "- [ ] Different angles (plot, characters, themes, technical, cultural)\n",
        "- [ ] Reasonable connection to correct answer\n",
        "- [ ] Nothing gives away too much\n",
        "\n",
        "### Misinformed Clues: ___/5\n",
        "- [ ] Could point to 2+ different answers\n",
        "- [ ] Vague but not nonsensical\n",
        "- [ ] Not obviously wrong\n",
        "\n",
        "### Fake Clues: ___/5\n",
        "- [ ] Clearly point to wrong answer choices\n",
        "- [ ] Believable enough to fool players\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Diversity Check\n",
        "- [ ] **PASS** - Informed clues cover different aspects\n",
        "- [ ] **FAIL** - Found duplicates: ________________\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Difficulty Rating\n",
        "| Score | Assessment |\n",
        "|-------|------------|\n",
        "| 1-2 | Too Easy |\n",
        "| 3 | Just Right |\n",
        "| 4-5 | Too Hard |\n",
        "\n",
        "**Rating:** ___/5\n",
        "\n",
        "---\n",
        "\n",
        "## Overall Assessment\n",
        "**Pass/Fail:** ______  \n",
        "**Main Issues:** ______________________  \n",
        "**Notes:** ____________________________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JYyUqexMP36L"
      },
      "outputs": [],
      "source": [
        "# List of different topics to test\n",
        "test_topics = [\n",
        "    {\"round_1\": \"Movie\", \"round_2\": \"Star Wars Episode I: The Phantom Menace\"},\n",
        "    {\"round_1\": \"Song\", \"round_2\": \"Bohemian Rhapsody - Queen\"},\n",
        "    {\"round_1\": \"Book\", \"round_2\": \"Harry Potter and the Sorcerer's Stone\"},\n",
        "    {\"round_1\": \"TV Show\", \"round_2\": \"Breaking Bad\"},\n",
        "    {\"round_1\": \"Video Game\", \"round_2\": \"The Legend of Zelda: Breath of the Wild\"},\n",
        "    {\"round_1\": \"Food\", \"round_2\": \"Pizza Margherita\"},\n",
        "    {\"round_1\": \"Animal\", \"round_2\": \"African Elephant\"},\n",
        "    {\"round_1\": \"Sport\", \"round_2\": \"Tennis\"},\n",
        "    {\"round_1\": \"Country\", \"round_2\": \"Japan\"},\n",
        "    {\"round_1\": \"Historical Event\", \"round_2\": \"Moon Landing 1969\"}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfPgMxCaJrl1"
      },
      "source": [
        "### Manual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3tUvM920BaA-"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "from datetime import datetime\n",
        "from langchain_core.messages import HumanMessage, SystemMessage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZGIYGFrwK-Ly"
      },
      "outputs": [],
      "source": [
        "def extract_json_from_response(content):\n",
        "    \"\"\"Extract JSON from model response using multiple fallback methods\"\"\"\n",
        "    # Clean escaped quotes\n",
        "    content = content.replace('\\\\\"', '\"')\n",
        "\n",
        "    # Method 1: Direct parse\n",
        "    try:\n",
        "        return json.loads(content)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Method 2: Extract from code blocks\n",
        "    match = re.search(r\"```(?:json)?\\s*(.*?)\\s*```\", content, re.DOTALL)\n",
        "    if match:\n",
        "        try:\n",
        "            return json.loads(match.group(1))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Method 3: Find incomplete array and fix\n",
        "    match = re.search(r\"(\\[.*)\", content, re.DOTALL)\n",
        "    if match:\n",
        "        json_text = match.group(1).rstrip()\n",
        "        if not json_text.endswith(']'):\n",
        "            json_text = json_text.rstrip(',') + ']'\n",
        "        try:\n",
        "            return json.loads(json_text)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "P3wJUwXKBdwP"
      },
      "outputs": [],
      "source": [
        "def process_game_data(game_data, topic, run_number):\n",
        "    \"\"\"Process valid game data into rows\"\"\"\n",
        "    rows = []\n",
        "    for i, round_data in enumerate(game_data, start=1):\n",
        "        answer = round_data.get(\"answer\", \"\")\n",
        "        choices = \", \".join(round_data.get(\"choices\", []))\n",
        "\n",
        "        for clue_type in [\"informed_clues\", \"misinformed_clues\", \"fake_clues\", \"extra_clues\"]:\n",
        "            for j, clue in enumerate(round_data.get(clue_type, []), start=1):\n",
        "                word_count = len(clue.split())\n",
        "                rows.append({\n",
        "                    \"test_run\": run_number,\n",
        "                    \"topic_category\": topic['round_1'],\n",
        "                    \"topic_specific\": topic['round_2'],\n",
        "                    \"round\": i,\n",
        "                    \"answer\": answer,\n",
        "                    \"choices\": choices,\n",
        "                    \"clue_type\": clue_type.replace(\"_clues\", \"\"),\n",
        "                    \"clue_number\": j,\n",
        "                    \"clue_text\": clue,\n",
        "                    \"word_count\": word_count,\n",
        "                    \"length_ok\": \"YES\" if 15 <= word_count <= 20 else \"NO\",\n",
        "                    \"manual_score / comment\": \"\"\n",
        "                })\n",
        "    return rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChsBIcQsBhDM",
        "outputId": "67436162-39dd-4b17-f5d5-682e3eadaf96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running test 1/10: Movie - Star Wars Episode I: The Phantom Menace\n",
            "✅ Test 1 completed successfully\n",
            "Running test 2/10: Song - Bohemian Rhapsody - Queen\n",
            "✅ Test 2 completed successfully\n",
            "Running test 3/10: Book - Harry Potter and the Sorcerer's Stone\n",
            "✅ Test 3 completed successfully\n",
            "Running test 4/10: TV Show - Breaking Bad\n",
            "✅ Test 4 completed successfully\n",
            "Running test 5/10: Video Game - The Legend of Zelda: Breath of the Wild\n",
            "✅ Test 5 completed successfully\n",
            "Running test 6/10: Food - Pizza Margherita\n",
            "✅ Test 6 completed successfully\n",
            "Running test 7/10: Animal - African Elephant\n",
            "✅ Test 7 completed successfully\n",
            "Running test 8/10: Sport - Tennis\n",
            "✅ Test 8 completed successfully\n",
            "Running test 9/10: Country - Japan\n",
            "✅ Test 9 completed successfully\n",
            "Running test 10/10: Historical Event - Moon Landing 1969\n",
            "✅ Test 10 completed successfully\n"
          ]
        }
      ],
      "source": [
        "# Main execution\n",
        "all_rows = []\n",
        "\n",
        "for run_number, topic in enumerate(test_topics, 1):\n",
        "    print(f\"Running test {run_number}/{len(test_topics)}: {topic['round_1']} - {topic['round_2']}\")\n",
        "\n",
        "    messages = [\n",
        "        SystemMessage(system_prompt + output_format + one_shot_example),\n",
        "        HumanMessage(json.dumps(topic)),\n",
        "    ]\n",
        "\n",
        "    response = model.invoke(messages)\n",
        "    clean_content = re.sub(r\"<think>.*?</think>\", \"\", response.content, flags=re.DOTALL).strip()\n",
        "    game_data = extract_json_from_response(clean_content)\n",
        "\n",
        "    if game_data:\n",
        "        try:\n",
        "            all_rows.extend(process_game_data(game_data, topic, run_number))\n",
        "            print(f\"✅ Test {run_number} completed successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing data for test {run_number}: {e}\")\n",
        "    else:\n",
        "        print(f\"❌ No valid JSON found for test {run_number}\")\n",
        "        print(\"RAW:\", clean_content[:200])\n",
        "\n",
        "    sleep(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6HdR9_0Ht1y",
        "outputId": "d5285434-a3c2-4fab-9c88-672d0a3d7c29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ CSV saved: 10_rounds_clues_analysis.csv\n",
            "Total rows generated: 280\n"
          ]
        }
      ],
      "source": [
        "# Save to CSV\n",
        "with open(\"10_rounds_clues_analysis.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    if all_rows:\n",
        "        writer = csv.DictWriter(f, fieldnames=all_rows[0].keys())\n",
        "        writer.writeheader()\n",
        "        writer.writerows(all_rows)\n",
        "        print(f\"✅ CSV saved: 10_rounds_clues_analysis.csv\")\n",
        "\n",
        "print(f\"Total rows generated: {len(all_rows)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A0fc3WdZymW"
      },
      "source": [
        "## LLM analysis (llama)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ixZR4gtyhwU-"
      },
      "outputs": [],
      "source": [
        "analysis_model = init_chat_model(\n",
        "    # model=\"llama-3.3-70b-versatile\",\n",
        "    model=\"openai/gpt-oss-120b\",\n",
        "    model_provider=\"groq\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_OurpVJ2h9eD"
      },
      "outputs": [],
      "source": [
        "def analyze_round_with_llm(round_data, analysis_model):\n",
        "    \"\"\"Analyze a single round using LLM\"\"\"\n",
        "\n",
        "    # Count words for each clue type\n",
        "    word_counts = {}\n",
        "    length_issues = []\n",
        "\n",
        "    for clue_type in [\"informed_clues\", \"misinformed_clues\", \"fake_clues\", \"extra_clues\"]:\n",
        "        clues = round_data.get(clue_type, [])\n",
        "        word_counts[clue_type] = []\n",
        "\n",
        "        for i, clue in enumerate(clues, 1):\n",
        "            word_count = len(clue.split())\n",
        "            word_counts[clue_type].append(word_count)\n",
        "\n",
        "            if not (15 <= word_count <= 20):\n",
        "                length_issues.append(f\"{clue_type} #{i}: {word_count} words\")\n",
        "\n",
        "    # Create word count summary\n",
        "    word_count_summary = f\"\"\"\n",
        "WORD COUNT ANALYSIS:\n",
        "- Informed clues: {word_counts.get('informed_clues', [])}\n",
        "- Misinformed clues: {word_counts.get('misinformed_clues', [])}\n",
        "- Fake clues: {word_counts.get('fake_clues', [])}\n",
        "- Extra clues: {word_counts.get('extra_clues', [])}\n",
        "\n",
        "LENGTH ISSUES (should be 15-20 words):\n",
        "{'; '.join(length_issues) if length_issues else 'All clues meet length requirements'}\n",
        "\"\"\"\n",
        "\n",
        "    analysis_prompt = f\"\"\"\n",
        "You are evaluating clues for a deduction game. Analyze BOTH word count compliance AND content quality.\n",
        "\n",
        "{word_count_summary}\n",
        "\n",
        "ROUND DATA:\n",
        "{json.dumps(round_data, indent=2)}\n",
        "\n",
        "CRITICAL REQUIREMENTS:\n",
        "1. LENGTH COMPLIANCE: Each clue should be 15-20 words (see analysis above)\n",
        "2. ANSWER CONTAMINATION: Check if ANY clue contains the answer word\n",
        "3. SPECIFICITY: Are clues specific enough to distinguish from similar items?\n",
        "4. CLUE REFERENCES: Use specific clue numbers when noting issues\n",
        "5. DETAILED REASONING: Explain WHY you gave each score\n",
        "\n",
        "CLUE TYPE REQUIREMENTS:\n",
        "- **INFORMED CLUES**: Must relate to actual answer and be distinct/specific (avoid generic descriptions)\n",
        "- **MISINFORMED CLUES**: Must be related to actual answer BUT vague enough to apply to multiple choices (create productive doubt)\n",
        "- **FAKE CLUES**: Must clearly point to the OTHER answer choices, NOT the correct answer (effective misdirection)\n",
        "\n",
        "SCORING SCALE (MANDATORY):\n",
        "Rate based on how well each clue type fulfills its specific purpose:\n",
        "- informed_quality: Rate 1-5 (How well do they point to correct answer specifically?)\n",
        "- misinformed_quality: Rate 1-5 (Do they create ambiguity while staying answer-related?)\n",
        "- fake_quality: Rate 1-5 (Do they clearly misdirect to wrong answer choices?)\n",
        "- difficulty: Rate 1-5 (1=too easy, 2=easy, 3=just right, 4=hard, 5=too hard)\n",
        "\n",
        "Return ONLY this JSON format:\n",
        "{{\n",
        "  \"length_compliance_score\": number (1-5),\n",
        "  \"length_issues_found\": [\"list of specific length problems\"],\n",
        "  \"informed_quality\": number (1-5),\n",
        "  \"informed_notes\": \"detailed analysis with specific clue numbers\",\n",
        "  \"misinformed_quality\": number (1-5),\n",
        "  \"misinformed_notes\": \"detailed analysis of ambiguity effectiveness\",\n",
        "  \"fake_quality\": number (1-5),\n",
        "  \"fake_notes\": \"detailed analysis of misdirection effectiveness\",\n",
        "  \"diversity_issues\": [\"list specific problems found\"],\n",
        "  \"difficulty\": number (1-5),\n",
        "  \"difficulty_reasoning\": \"detailed explanation\",\n",
        "  \"overall_notes\": \"comprehensive summary\"\n",
        "}}\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = analysis_model.invoke([HumanMessage(analysis_prompt)])\n",
        "        return extract_json_from_response(response.content)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ LLM analysis failed: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rdYVHNS9lv5r"
      },
      "outputs": [],
      "source": [
        "# Load data from your manual analysis CSV\n",
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file from your manual analysis\n",
        "df = pd.read_csv(\"10_rounds_clues_analysis.csv\")  # Change filename as needed\n",
        "\n",
        "# Group data by test_run and round to reconstruct round_data\n",
        "all_results = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ex9epMELiCFt",
        "outputId": "3acb14c5-49db-47e6-e947-a335d1e1b0ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyzing Test 1, Round 1: Movie - movie\n",
            "  ✅ Analyzed successfully\n",
            "Analyzing Test 1, Round 2: Movie - Star Wars Episode I: The Phantom Menace\n",
            "  ❌ Analysis failed\n",
            "Analyzing Test 2, Round 1: Song - song\n",
            "  ❌ Analysis failed\n",
            "Analyzing Test 2, Round 2: Song - Bohemian Rhapsody\n",
            "  ✅ Analyzed successfully\n",
            "Analyzing Test 3, Round 1: Book - book\n",
            "  ✅ Analyzed successfully\n",
            "Analyzing Test 3, Round 2: Book - Harry Potter and the Sorcerer's Stone\n",
            "  ❌ Analysis failed\n",
            "Analyzing Test 4, Round 1: TV Show - TV Show\n",
            "  ❌ Analysis failed\n",
            "Analyzing Test 4, Round 2: TV Show - Breaking Bad\n",
            "  ✅ Analyzed successfully\n",
            "Analyzing Test 5, Round 1: Video Game - video game\n",
            "  ❌ Analysis failed\n",
            "Analyzing Test 5, Round 2: Video Game - The Legend of Zelda: Breath of the Wild\n",
            "  ✅ Analyzed successfully\n",
            "Analyzing Test 6, Round 1: Food - food\n",
            "  ❌ Analysis failed\n",
            "Analyzing Test 6, Round 2: Food - Pizza Margherita\n",
            "  ✅ Analyzed successfully\n",
            "Analyzing Test 7, Round 1: Animal - Animal\n",
            "  ❌ Analysis failed\n",
            "Analyzing Test 7, Round 2: Animal - African Elephant\n",
            "❌ LLM analysis failed: Connection error.\n",
            "  ❌ Analysis failed\n",
            "Analyzing Test 8, Round 1: Sport - sport\n",
            "❌ LLM analysis failed: Connection error.\n",
            "  ❌ Analysis failed\n",
            "Analyzing Test 8, Round 2: Sport - Tennis\n",
            "❌ LLM analysis failed: Connection error.\n",
            "  ❌ Analysis failed\n",
            "Analyzing Test 9, Round 1: Country - Country\n",
            "❌ LLM analysis failed: Connection error.\n",
            "  ❌ Analysis failed\n",
            "Analyzing Test 9, Round 2: Country - Japan\n",
            "❌ LLM analysis failed: Connection error.\n",
            "  ❌ Analysis failed\n",
            "Analyzing Test 10, Round 1: Historical Event - Historical Event\n",
            "❌ LLM analysis failed: Connection error.\n",
            "  ❌ Analysis failed\n",
            "Analyzing Test 10, Round 2: Historical Event - Moon Landing 1969\n",
            "❌ LLM analysis failed: Connection error.\n",
            "  ❌ Analysis failed\n"
          ]
        }
      ],
      "source": [
        "for (test_run, round_num), group in df.groupby(['test_run', 'round']):\n",
        "    # Skip disinformer instructions\n",
        "    clue_data = group[group['clue_type'] != 'disinformer_instruction']\n",
        "\n",
        "    if len(clue_data) == 0:\n",
        "        continue\n",
        "\n",
        "    # Get basic info\n",
        "    topic_category = clue_data['topic_category'].iloc[0]\n",
        "    topic_specific = clue_data['topic_specific'].iloc[0]\n",
        "    answer = clue_data['answer'].iloc[0]\n",
        "    choices = clue_data['choices'].iloc[0]\n",
        "\n",
        "    print(f\"Analyzing Test {test_run}, Round {round_num}: {topic_category} - {answer}\")\n",
        "\n",
        "    # Reconstruct round_data from CSV\n",
        "    round_data = {\n",
        "        \"answer\": answer,\n",
        "        \"choices\": choices.split(\" | \") if choices else [],\n",
        "        \"informed_clues\": clue_data[clue_data['clue_type'] == 'informed']['clue_text'].tolist(),\n",
        "        \"misinformed_clues\": clue_data[clue_data['clue_type'] == 'misinformed']['clue_text'].tolist(),\n",
        "        \"fake_clues\": clue_data[clue_data['clue_type'] == 'fake']['clue_text'].tolist(),\n",
        "        \"extra_clues\": clue_data[clue_data['clue_type'] == 'extra']['clue_text'].tolist()\n",
        "    }\n",
        "\n",
        "    # Analyze with LLM\n",
        "    analysis = analyze_round_with_llm(round_data, analysis_model)\n",
        "\n",
        "    if analysis:\n",
        "        result = {\n",
        "            \"test_run\": test_run,\n",
        "            \"topic_category\": topic_category,\n",
        "            \"topic_specific\": topic_specific,\n",
        "            \"round\": round_num,\n",
        "            \"answer\": answer,\n",
        "            \"choices\": choices,\n",
        "\n",
        "            # LLM Analysis Results\n",
        "            \"informed_quality\": analysis.get(\"informed_quality\", \"\"),\n",
        "            \"informed_notes\": analysis.get(\"informed_notes\", \"\"),\n",
        "            \"misinformed_quality\": analysis.get(\"misinformed_quality\", \"\"),\n",
        "            \"misinformed_notes\": analysis.get(\"misinformed_notes\", \"\"),\n",
        "            \"fake_quality\": analysis.get(\"fake_quality\", \"\"),\n",
        "            \"fake_notes\": analysis.get(\"fake_notes\", \"\"),\n",
        "            \"diversity_issues\": \"; \".join(analysis.get(\"diversity_issues\", [])),\n",
        "            \"difficulty\": analysis.get(\"difficulty\", \"\"),\n",
        "            \"difficulty_reasoning\": analysis.get(\"difficulty_reasoning\", \"\"),\n",
        "            \"overall_notes\": analysis.get(\"overall_notes\", \"\"),\n",
        "\n",
        "            # Word count and length compliance data\n",
        "            \"total_clues\": len(round_data[\"informed_clues\"]) + len(round_data[\"misinformed_clues\"]) + len(round_data[\"fake_clues\"]) + len(round_data[\"extra_clues\"]),\n",
        "            \"length_compliant_clues\": sum(1 for clue_type in [\"informed_clues\", \"misinformed_clues\", \"fake_clues\", \"extra_clues\"]\n",
        "                                        for clue in round_data[clue_type]\n",
        "                                        if 15 <= len(clue.split()) <= 20),\n",
        "            \"length_compliance_rate\": f\"{(sum(1 for clue_type in ['informed_clues', 'misinformed_clues', 'fake_clues', 'extra_clues'] for clue in round_data[clue_type] if 15 <= len(clue.split()) <= 20) / max(1, sum(len(round_data[clue_type]) for clue_type in ['informed_clues', 'misinformed_clues', 'fake_clues', 'extra_clues'])) * 100):.0f}%\",\n",
        "            \"avg_word_count\": round(sum(len(clue.split()) for clue_type in [\"informed_clues\", \"misinformed_clues\", \"fake_clues\", \"extra_clues\"] for clue in round_data[clue_type]) / max(1, sum(len(round_data[clue_type]) for clue_type in [\"informed_clues\", \"misinformed_clues\", \"fake_clues\", \"extra_clues\"])), 1)\n",
        "        }\n",
        "\n",
        "        all_results.append(result)\n",
        "        print(f\"  ✅ Analyzed successfully\")\n",
        "    else:\n",
        "        print(f\"  ❌ Analysis failed\")\n",
        "\n",
        "    sleep(2)  # Rate limiting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xayzwc_0iO-L",
        "outputId": "f1735cba-482e-4fec-c580-03c4e264fb10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ LLM analysis complete! Saved 6 results to: llm_analysis_results.csv\n"
          ]
        }
      ],
      "source": [
        "# Save results\n",
        "if all_results:\n",
        "    with open(\"llm_analysis_results.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=all_results[0].keys())\n",
        "        writer.writeheader()\n",
        "        writer.writerows(all_results)\n",
        "\n",
        "    print(f\"✅ LLM analysis complete! Saved {len(all_results)} results to: llm_analysis_results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Generated analysis matrices for Test 1: clue_analysis_matrices\\test1_clue_analysis.md\n",
            "✅ Generated analysis matrices for Test 2: clue_analysis_matrices\\test2_clue_analysis.md\n",
            "✅ Generated analysis matrices for Test 3: clue_analysis_matrices\\test3_clue_analysis.md\n",
            "✅ Generated analysis matrices for Test 4: clue_analysis_matrices\\test4_clue_analysis.md\n",
            "✅ Generated analysis matrices for Test 5: clue_analysis_matrices\\test5_clue_analysis.md\n",
            "✅ Generated analysis matrices for Test 6: clue_analysis_matrices\\test6_clue_analysis.md\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "Missing optional dependency 'tabulate'.  Use pip or conda to install tabulate.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\compat\\_optional.py:135\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1140\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tabulate'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 215\u001b[0m\n\u001b[0;32m    207\u001b[0m overall_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# Overall Performance Breakdown by Category\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m by_category \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopic_category\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39magg({\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlength_compliance_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(x\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mmean())\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minformed_quality\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifficulty\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    214\u001b[0m })\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m--> 215\u001b[0m overall_summary \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mby_category\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;66;03m# Save overall summary to a markdown file\u001b[39;00m\n\u001b[0;32m    218\u001b[0m overall_file \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDisinformer_Game_Clues_Quality_Summary.MD\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:2983\u001b[0m, in \u001b[0;36mDataFrame.to_markdown\u001b[1;34m(self, buf, mode, index, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m   2981\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtablefmt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipe\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2982\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshowindex\u001b[39m\u001b[38;5;124m\"\u001b[39m, index)\n\u001b[1;32m-> 2983\u001b[0m tabulate \u001b[38;5;241m=\u001b[39m \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtabulate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2984\u001b[0m result \u001b[38;5;241m=\u001b[39m tabulate\u001b[38;5;241m.\u001b[39mtabulate(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\compat\\_optional.py:138\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 138\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
            "\u001b[1;31mImportError\u001b[0m: Missing optional dependency 'tabulate'.  Use pip or conda to install tabulate."
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Ensure the utility functions below exist in your notebook cell.\n",
        "def calculate_length_compliance(row):\n",
        "    compliance_rate = int(row['length_compliance_rate'].rstrip('%'))\n",
        "    total_clues = row['total_clues']\n",
        "    compliant = row['length_compliant_clues']\n",
        "    non_compliant = total_clues - compliant\n",
        "    return compliance_rate, compliant, non_compliant, total_clues\n",
        "\n",
        "def get_pass_fail_status(compliance_rate):\n",
        "    return \"✅ PASS\" if compliance_rate >= 80 else \"❌ FAIL\"\n",
        "\n",
        "def get_quality_assessment(score):\n",
        "    assessments = {\n",
        "        1: \"Poor - Needs significant revision\",\n",
        "        2: \"Fair - Below expectations\",\n",
        "        3: \"Good - Meets requirements\",\n",
        "        4: \"Very Good - Exceeds expectations\",\n",
        "        5: \"Excellent - Outstanding\"\n",
        "    }\n",
        "    return assessments.get(int(score), \"Unknown\")\n",
        "\n",
        "def get_difficulty_assessment(difficulty):\n",
        "    difficulty = int(difficulty)\n",
        "    if difficulty <= 2:\n",
        "        return \"🟢 Too Easy\"\n",
        "    elif difficulty == 3:\n",
        "        return \"🟢 Just Right\"\n",
        "    else:\n",
        "        return \"🟠 Too Hard\"\n",
        "\n",
        "def extract_issues(notes_str):\n",
        "    import pandas as pd\n",
        "    if pd.isna(notes_str):\n",
        "        return [\"None identified\"]\n",
        "    notes_str = str(notes_str).lower()\n",
        "    issues = []\n",
        "    keywords = {\n",
        "        \"length\": \"Word count compliance issues\",\n",
        "        \"generic\": \"Generic/vague clues\",\n",
        "        \"diversity\": \"Lack of diversity in themes\",\n",
        "        \"ambiguity\": \"Insufficient ambiguity in misinformed clues\",\n",
        "        \"specificity\": \"Missing specificity in clues\",\n",
        "        \"answer contamination\": \"Answer word revealed in clues\"\n",
        "    }\n",
        "    for keyword, issue in keywords.items():\n",
        "        if keyword in notes_str:\n",
        "            issues.append(issue)\n",
        "    return issues if issues else [\"Minor issues noted\"]\n",
        "\n",
        "def generate_matrix_for_round(row):\n",
        "    test_run = int(row['test_run'])\n",
        "    topic_cat = row['topic_category']\n",
        "    topic_spec = row['topic_specific']\n",
        "    round_num = int(row['round'])\n",
        "    compliance_rate, compliant, non_compliant, total = calculate_length_compliance(row)\n",
        "    status = get_pass_fail_status(compliance_rate)\n",
        "    informed_score = int(row['informed_quality'])\n",
        "    misinformed_score = int(row['misinformed_quality'])\n",
        "    fake_score = int(row['fake_quality'])\n",
        "    difficulty = int(row['difficulty'])\n",
        "    issues = extract_issues(row['overall_notes'])\n",
        "    diversity_issues = row['diversity_issues'] if not pd.isna(row['diversity_issues']) else \"None identified\"\n",
        "    \n",
        "    matrix = f\"\"\"# Game Clue Analysis Matrix\n",
        "**Test Run {test_run} | Round {round_num}: {topic_cat} → {topic_spec}**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Length Compliance\n",
        "| Status | Criteria |\n",
        "|--------|----------|\n",
        "| {status} | Clues within 15-20 words |\n",
        "\n",
        "**Compliance Rate:** {compliance_rate}% ({compliant}/{total} clues)  \n",
        "**Outliers:** {non_compliant}/{total} clues failed  \n",
        "**Average Word Count:** {row['avg_word_count']} words\n",
        "\n",
        "**Assessment:** {\"✅ Acceptable - Most clues meet length requirements\" if compliance_rate >= 80 else \"❌ Critical - Significant length violations require revision\"}\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Quality Scores (Rate 1-5)\n",
        "\n",
        "### Informed Clues: {informed_score}/5  \n",
        "**{get_quality_assessment(informed_score)}**\n",
        "\n",
        "{row['informed_notes']}\n",
        "\n",
        "✅ Strengths:\n",
        "- Generally specific and relate to correct answer\n",
        "- Provide distinct perspectives where applicable\n",
        "\n",
        "⚠️ Concerns:\n",
        "- {row['diversity_issues'] if not pd.isna(row['diversity_issues']) else \"Minor thematic overlap observed\"}\n",
        "\n",
        "### Misinformed Clues: {misinformed_score}/5  \n",
        "**{get_quality_assessment(misinformed_score)}**\n",
        "\n",
        "{row['misinformed_notes']}\n",
        "\n",
        "✅ Strengths:\n",
        "- Attempt to create ambiguity\n",
        "- Generally related to the correct answer\n",
        "\n",
        "⚠️ Concerns:\n",
        "- May need more subtle misdirection\n",
        "- Ambiguity effectiveness varies\n",
        "\n",
        "### Fake Clues: {fake_score}/5  \n",
        "**{get_quality_assessment(fake_score)}**\n",
        "\n",
        "{row['fake_notes']}\n",
        "\n",
        "✅ Strengths:\n",
        "- Effectively misdirect to wrong answer choices\n",
        "- Clear deception without being obvious\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Diversity Check\n",
        "\n",
        "| Aspect | Status |\n",
        "|--------|--------|\n",
        "| Theme Coverage | {\"✅ PASS\" if \"diversity\" not in diversity_issues.lower() else \"❌ FAIL\"} |\n",
        "| Clue Variation | {\"✅ PASS\" if informed_score >= 3 else \"❌ FAIL\"} |\n",
        "| Angle Coverage | {\"✅ PASS\" if non_compliant <= 2 else \"❌ FAIL\"} |\n",
        "\n",
        "**Issues Found:** {diversity_issues}\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Difficulty Rating\n",
        "\n",
        "| Score | Assessment |\n",
        "|-------|------------|\n",
        "| Rating | {difficulty}/5 - {get_difficulty_assessment(difficulty)} |\n",
        "\n",
        "**Reasoning:** {row['difficulty_reasoning']}\n",
        "\n",
        "---\n",
        "\n",
        "## Overall Assessment\n",
        "\n",
        "**Overall Quality Score:** {(informed_score + misinformed_score + fake_score) / 3:.1f}/5\n",
        "\n",
        "**Pass/Fail:** {\"✅ PASS\" if compliance_rate >= 70 and (informed_score + misinformed_score + fake_score) / 3 >= 3 else \"⚠️ NEEDS REVISION\"}\n",
        "\n",
        "**Main Issues:**\n",
        "{chr(10).join(f\"- {issue}\" for issue in issues)}\n",
        "\n",
        "**Priority Actions:**\n",
        "1. {\"Address length compliance\" if compliance_rate < 80 else \"Minor length adjustments\"}\n",
        "2. {\"Enhance misinformed clue ambiguity\" if misinformed_score < 3 else \"Maintain misinformed clue quality\"}\n",
        "3. {\"Increase clue diversity\" if \"diversity\" in diversity_issues.lower() else \"Maintain current diversity\"}\n",
        "\n",
        "**Overall Notes:**  \n",
        "{row['overall_notes']}\n",
        "\n",
        "---\n",
        "\"\"\"\n",
        "    return matrix\n",
        "\n",
        "# --- Matrices Generation per Test Run ---\n",
        "csv_path = Path(\"llm_analysis_results.csv\")\n",
        "if not csv_path.exists():\n",
        "    print(f\"❌ Error: {csv_path} not found.\")\n",
        "else:\n",
        "    df = pd.read_csv(csv_path)\n",
        "    test_runs = df['test_run'].unique()\n",
        "    dir = Path(\"clue_analysis_matrices\")\n",
        "    dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    for test in sorted(test_runs):\n",
        "        group = df[df['test_run'] == test]\n",
        "        text = f\"# Analysis for Test {test}\\n\\n\"\n",
        "        \n",
        "        # Append matrices for each round in the test\n",
        "        rounds = sorted(group['round'].unique())\n",
        "        for r in rounds:\n",
        "            row = group[group['round'] == r].iloc[0]\n",
        "            matrix_text = generate_matrix_for_round(row)\n",
        "            text += matrix_text + \"\\n\\n\"\n",
        "        \n",
        "        # Append a round-by-round performance summary table for this test\n",
        "        text += \"## Round-by-Round Performance Summary\\n\\n\"\n",
        "        text += \"| Round | Length Compliance | Informed | Misinformed | Fake | Difficulty |\\n\"\n",
        "        text += \"|-------|-------------------|----------|-------------|------|------------|\\n\"\n",
        "        for r in rounds:\n",
        "            row = group[group['round'] == r].iloc[0]\n",
        "            length_comp = row['length_compliance_rate']\n",
        "            inf_score = row['informed_quality']\n",
        "            mis_score = row['misinformed_quality']\n",
        "            fake_score = row['fake_quality']\n",
        "            difficulty = row['difficulty']\n",
        "            text += f\"| {r} | {length_comp} | {inf_score}/5 | {mis_score}/5 | {fake_score}/5 | {difficulty}/5 |\\n\"\n",
        "        \n",
        "        # Save markdown for this test run\n",
        "        test_file = dir / f\"test{test}_clue_analysis.md\"\n",
        "        with open(test_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(text)\n",
        "        print(f\"✅ Generated analysis matrices for Test {test}: {test_file}\")\n",
        "    \n",
        "    # --- Overall Performance Breakdown by Category ---\n",
        "    overall_summary = \"# Overall Performance Breakdown by Category\\n\\n\"\n",
        "    by_category = df.groupby('topic_category').agg({\n",
        "        'length_compliance_rate': lambda x: f\"{int(x.str.rstrip('%').astype(int).mean()):.0f}%\",\n",
        "        'informed_quality': lambda x: f\"{x.astype(int).mean():.1f}/5\",\n",
        "        'misinformed_quality': lambda x: f\"{x.astype(int).mean():.1f}/5\",\n",
        "        'fake_quality': lambda x: f\"{x.astype(int).mean():.1f}/5\",\n",
        "        'difficulty': lambda x: f\"{x.astype(int).mean():.1f}/5\"\n",
        "    }).reset_index()\n",
        "    overall_summary += by_category.to_markdown(index=False)\n",
        "    \n",
        "    # Save overall summary to a markdown file\n",
        "    overall_file = Path(\"Disinformer_Game_Clues_Quality_Summary.MD\")\n",
        "    with open(overall_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(overall_summary)\n",
        "    print(f\"✅ Overall performance by category saved: {overall_file}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
