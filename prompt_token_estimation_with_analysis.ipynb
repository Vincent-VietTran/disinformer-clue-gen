{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disinformer Clue Generation with Topic-Based Organization\n",
    "\n",
    "**Output Directory Structure:**\n",
    "When running this notebook with different game topics, files will be automatically organized as follows:\n",
    "\n",
    "```\n",
    "{topic_name}/\n",
    "‚îú‚îÄ‚îÄ 10_games_clues_content({topic_name}).csv\n",
    "‚îú‚îÄ‚îÄ 10_games_length_validation({topic_name}).csv\n",
    "‚îú‚îÄ‚îÄ 10_games_llm_analysis_results({topic_name}).csv\n",
    "‚îú‚îÄ‚îÄ 10_games_clues_quality_summary({topic_name}).md\n",
    "‚îî‚îÄ‚îÄ clue_analysis_matrices/\n",
    "    ‚îú‚îÄ‚îÄ game1_clue_analysis.md\n",
    "    ‚îú‚îÄ‚îÄ game2_clue_analysis.md\n",
    "    ‚îî‚îÄ‚îÄ ... (one file per game)\n",
    "```\n",
    "\n",
    "**How to use:**\n",
    "1. Modify the `game_topics = game_topics[:10]` line to select the specific topic batch you want to process\n",
    "2. Run all cells in order\n",
    "3. Files will be saved in a folder named after the game topic (spaces and slashes converted to underscores)\n",
    "4. All analysis matrices will be saved in a `clue_analysis_matrices` subfolder within the topic folder\n",
    "\n",
    "This structure keeps each topic's results neatly organized and prevents file overwrites when processing different topics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGwqhPmJVIzb"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "oNTYk8g3nDfG"
   },
   "outputs": [],
   "source": [
    "# Initialize environment variables/constants (for Google Colab)\n",
    "# import os\n",
    "# from google.colab import userdata\n",
    "\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Initialize environment variables/constants (for VS Code)\n",
    "import os\n",
    "\n",
    "# Set your Google Gemini API key here or in your environment variables\n",
    "# You can get a free API key from: https://aistudio.google.com/app/apikey\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "aR-V4U66oE4T"
   },
   "outputs": [],
   "source": [
    "# Install langchain google genai\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# google colab command\n",
    "# !pip install -U langchain-google-genai\n",
    "\n",
    "# vs code command\n",
    "%pip install langchain\n",
    "\n",
    "# Upgrade google-generativeai and langchain-google-genai to latest versions\n",
    "%pip install --upgrade google-generativeai\n",
    "%pip install --upgrade langchain-google-genai\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "pD5fVpWSnULv"
   },
   "outputs": [],
   "source": [
    "# Instantiate an LLM\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Warning: Using different models for generation and fixing may lead to json/response parsing issues.\n",
    "\n",
    "# LLM model for clue generation\n",
    "generation_model = init_chat_model(\n",
    "    # model=\"gemini-2.5-flash\",\n",
    "    # model=\"gemini-2.5-flash-lite\",\n",
    "    # model=\"gemini-2.0-flash\",\n",
    "    model=\"gemini-2.0-flash-lite\",\n",
    "    model_provider=\"google_genai\"\n",
    ")\n",
    "\n",
    "# LLM model for fixing clues\n",
    "fixing_model = init_chat_model(\n",
    "    # model=\"gemini-2.5-flash\",\n",
    "    # model=\"gemini-2.5-flash-lite\",\n",
    "    # model=\"gemini-2.0-flash\",\n",
    "    model=\"gemini-2.0-flash-lite\",\n",
    "    model_provider=\"google_genai\"\n",
    ")\n",
    "\n",
    "# LLM model for analysis\n",
    "analysis_model = init_chat_model(\n",
    "    # model=\"gemini-2.5-flash\",\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    # model=\"gemini-2.0-flash\",\n",
    "    # model=\"gemini-2.0-flash-lite\",\n",
    "    model_provider=\"google_genai\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 games from JSON.\n",
      "First 5 examples:\n",
      "  1. {'category': 'Broadcast Media', 'game_number': 1, 'r1_answer': 'Sci-Fi', 'r1_choices': ['Sci-Fi', 'Horror', 'Fantasy'], 'r2_answer': 'The Matrix', 'r2_choices': ['Blade Runner', 'The Matrix', 'Dune']}\n",
      "  2. {'category': 'Broadcast Media', 'game_number': 2, 'r1_answer': 'Family', 'r1_choices': ['Adventure', 'Family', 'Comedy'], 'r2_answer': 'Toy Story', 'r2_choices': ['Finding Nemo', 'Toy Story', 'The Incredibles']}\n",
      "  3. {'category': 'Broadcast Media', 'game_number': 3, 'r1_answer': 'Action', 'r1_choices': ['Action', 'Thriller', 'Adventure'], 'r2_answer': 'Mad Max: Fury Road', 'r2_choices': ['John Wick', 'Mad Max: Fury Road', 'The Bourne Identity']}\n",
      "  4. {'category': 'Broadcast Media', 'game_number': 4, 'r1_answer': 'Drama', 'r1_choices': ['Drama', 'Romance', 'Comedy'], 'r2_answer': 'The Shawshank Redemption', 'r2_choices': ['12 Angry Men', 'The Shawshank Redemption', 'The Godfather']}\n",
      "  5. {'category': 'Broadcast Media', 'game_number': 5, 'r1_answer': 'Horror', 'r1_choices': ['Horror', 'Thriller', 'Sci-Fi'], 'r2_answer': 'The Shining', 'r2_choices': ['The Witch', 'The Shining', 'The Lighthouse']}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "with open('game_topics.json', 'r', encoding='utf-8') as f:\n",
    "    game_topics_data = json.load(f)\n",
    "\n",
    "# Process the data into a list of test topics (one per game per category)\n",
    "game_topics = []\n",
    "for category, games in game_topics_data.items():\n",
    "    for game in games:\n",
    "        game_topics.append({\n",
    "                \"category\": category,\n",
    "                \"game_number\": game[\"game_number\"],\n",
    "                \"r1_answer\": game[\"r1_answer\"],\n",
    "                \"r1_choices\": game[\"r1_choices\"],\n",
    "                \"r2_answer\": game[\"r2_answer\"],\n",
    "                \"r2_choices\": game[\"r2_choices\"]\n",
    "            })\n",
    "\n",
    "\n",
    "# Optional: Print a preview of the loaded data\n",
    "print(f\"Loaded {len(game_topics)} games from JSON.\")\n",
    "print(\"First 5 examples:\")\n",
    "for i, topic in enumerate(game_topics[:5]):\n",
    "    print(f\"  {i+1}. {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvtdvgtVVM_I"
   },
   "source": [
    "## Clues Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "zeo_bTg_nuCn"
   },
   "outputs": [],
   "source": [
    "# Write the prompts\n",
    "import json\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are the game master of a game called \"Disinformer\", which is similar to the \"message relay\" game. Below is the description of how the game works:\n",
    "```\n",
    "In this cooperative game, players use communication and teamwork to uncover the original prompt over multiple rounds of clues. Along the way, they must contend with a disruptive \"Disinformer,\" varying player interpretations, and time limits.\n",
    "\n",
    "There will be a minimum of 3 players and maximum of 10 players:\n",
    "- Regular players (a.k.a. the netizens): The job is to solve clues and discover the original prompt\n",
    "- at most 2 misinformed players: Has the same job as the regular players. However, this player is unknowingly being given vague/ambiguous clues.\n",
    "- at most 2 disinformer players: The job is to solve clues and discover prompt to persuade other players from clue.\n",
    "\n",
    "There will be 2 rounds in each game.\n",
    "- In the first round, the players will be given clues to guess a general category/term (e.g. \"movie\", \"song\", \"novel\", etc)\n",
    "- In the second round, the players will be given clues to guess a more specific thing (e.g. \"The Dark Knight (2008)\", \"The Hitchhiker's Guide to the Galaxy (Novel)\", \"Space Oddity - David Bowie (1969)\", etc) which is related to the general category in the previous round.\n",
    "\n",
    "In each round, there will be 3 type of clues for each player:\n",
    "- Informed: Clear but challenging clues that directly relate to the correct answer. These should guide players toward the right answer through precise hints.\n",
    "  - **Disallowed:**using the exact answer word, any morphological variations (e.g., plural/singular forms), or direct synonyms (words that mean the same thing).  \n",
    "  - **Allowed:** using descriptive paraphrases or indirect expressions that convey the concept creatively without naming it or its synonyms.  \n",
    "  - **Examples:**\n",
    "    - If the answer is **‚Äúmovies‚Äù**, ‚Äúfilm‚Äù, ‚Äúfilms‚Äù, ‚Äúcinema‚Äù are not allowed; ‚Äúmoving image‚Äù, ‚Äúon-screen story‚Äù, ‚Äúvisual narrative‚Äù are acceptable.\n",
    "    - If the answer is **‚Äúmoon landing‚Äù**, ‚Äúlunar‚Äù, ‚Äúmoon‚Äù, or ‚ÄúApollo‚Äù are not allowed; ‚ÄúEarth‚Äôs natural satellite mission‚Äù, ‚Äúgiant leap beyond our planet‚Äù are acceptable.\n",
    "\n",
    "- Misinformed: Vague and ambiguous clues. True but overly general clues that are a mutual, shared truth among all options.\n",
    "  - CRITICAL REQUIREMENTS (MANDATORY FOR EACH CLUE):\n",
    "    - Be 100% factually true for the correct answer.\n",
    "    - Also be 100% factually true for ALL of the incorrect answers. This is a non-negotiable rule to ensure the clue is maximally ambiguous and offers no information to eliminate any option.\n",
    "    - Avoid clues that are only true for an incorrect answer (a \"Fake\" clue) or only true for the correct answer (an \"Informed\" clue). The goal is a total shared intersection of truth.\n",
    "    - AVOID SPECIFICITY: Do not use defining keywords or tropes. Focus only on the most general, shared truths (e.g., \"involves suspense\" or \"features characters\").\n",
    "\n",
    "  - MANDATORY 3-STEP SELF-VALIDATION (PERFORM BEFORE WRITING EACH CLUE):\n",
    "    - Step 1: Find the Universal Intersection. Analyze the Correct Answer and ALL Incorrect Answers. You MUST find a general, factual attribute that all options share.\n",
    "    - Step 2: Justify the Intersection. Mentally confirm: \"Is this attribute 100% true for the correct answer? Yes. Is it also 100% true for every single one of the incorrect answers? Yes.\"\n",
    "    - Step 3: Write the Clue. Write a vague clue based only on that universally shared attribute.\n",
    "    \n",
    "  - ANTI-PATTERN WARNING (AVOID \"POOR ADAPTATION\"):\n",
    "    - DO NOT reuse clues from other rounds. Each set of answers is unique.\n",
    "    - You MUST re-validate the clue against the current set of all incorrect answers every single time, as per the 3-step validation process.\n",
    "\n",
    "  - Examples:\n",
    "    - Correct misinformed clue (Movie category, answer: \"Sci-Fi\", choices: \"Sci-Fi, Horror, Fantasy\"): \"This genre often features suspense, thrills, and elements of the unknown, keeping audiences on the edge of their seats.\" (True for Sci-Fi AND Horror/Fantasy.)\n",
    "    - Incorrect misinformed clue (would be \"Informed\") (Historical Event category, answer: \"Moon Landing\", choices: \"Moon Landing, Apollo 13, First Flight\"): \"This historic event was famously broadcast on live television, with one man taking a 'giant leap'.\" (Only true for Moon Landing, not for Apollo 13 or First Flight.)\n",
    "    - Incorrect misinformed clue (would be \"Fake\") (Historical Event category, answer: \"Moon Landing\", choices: \"Moon Landing, Apollo 13, First Flight\"): \"The story centers on a daring rescue mission with a team facing perilous conditions and impossible odds.\" (Only true for Apollo 13, not for Moon Landing or First Flight.)\n",
    "\n",
    "- Fake: Deceptive clues that **strongly and plausibly point toward one of the incorrect answer choices**, but have **no genuine connection** to the correct answer itself.  \n",
    "\n",
    "  - These clues should:\n",
    "    1. Clearly align with the theme, event, or nature of one of the *incorrect* answers.  \n",
    "    2. Avoid referencing or overlapping with any factual, thematic, or contextual aspects of the *correct* answer.  \n",
    "    3. Never describe something that doesn‚Äôt correspond to *any* of the given answer choices ‚Äî every fake clue must be misleading *within the scope of existing options*.  \n",
    "\n",
    "  - Example (answer: \"Moon Landing (1969)\", choices: \"Apollo 13\", \"Moon Landing 1969\", \"The Wright Brothers‚Äô First Flight\"):\n",
    "    - **Correct fake clue:** ‚ÄúEngineers race against time as oxygen runs out aboard a spacecraft stranded millions of miles from Earth.‚Äù  \n",
    "      (Relates to *Apollo 13*, but not to *Moon Landing 1969* or *The Wright Brothers*.)  \n",
    "    - **Incorrect fake clue:** ‚ÄúA dramatic account of a historical battle between two warring factions, emphasizing strategy and valor.‚Äù  \n",
    "      (Unrelated to *any* of the given answer choices ‚Äî fails to mislead effectively.)\n",
    "\n",
    "```\n",
    "\n",
    "As a game master, given a category and a thing (e.g. Movie: The Dark Knight (2008)), for each round, generate:\n",
    "- 9 informed clues for the regular players. Make the clues to be as distinct as possible.\n",
    "- 1 extra informed clue for a backup.\n",
    "- 2 misinformed clues.\n",
    "- 2 fake clues\n",
    "- **Do NOT generate new answers or choices, use only the provided ones from the input.**\n",
    "\n",
    "For round 2, make sure it is subtle enough. For example, when generating clues for a movie:\n",
    "- No direct names.\n",
    "- No title references.\n",
    "- Focus on plot nuances, secondary characters, or themes instead of iconic moments.\n",
    "\n",
    "\n",
    "**CRITICAL: EVERY SINGLE CLUE MUST BE EXACTLY 15-20 WORDS. NO EXCEPTIONS.**\n",
    "\n",
    "Before submitting your response, you MUST:\n",
    "1. Count every word in every clue individually\n",
    "3. Verify ALL 28 clues fall within 15-20 words\n",
    "4. If even ONE clue is outside range, STOP and rewrite ONLY that clue\n",
    "5. Repeat until 100% pass validation\n",
    "\n",
    "Example of VALID clues (count the words):\n",
    "- \"The protagonist discovers a hidden power while fleeing from mysterious pursuers in an ancient temple underground.\" (15 words)\n",
    "- \"Betrayal and redemption intertwine as characters navigate political conflicts involving espionage international borders and moral dilemmas.\" (16 words)\n",
    "\n",
    "Example of INVALID clues (DO NOT USE):\n",
    "- \"This film explores themes that are quite complex and multifaceted in nature and shows characters.\" (14 words)\n",
    "- \"The protagonist faces numerous challenges while trying to achieve their goal against overwhelming odds in a fantasy world with magic and danger.\" (21 words)\n",
    "\n",
    "However, there are some restrictions that you must follow:\n",
    "- You must not mention the answer choices except for the true answer.\n",
    "- The disinformer is not aware which clues are the misinformed ones. So, avoid giving advice that aims to leverage the misinformed clues\n",
    "\n",
    "After this, we will provide you with details of each game for a topic in the following JSON format: `<general category> - <specific thing>`\n",
    "{\n",
    "  \"category\": \"<general category>\", \n",
    "  \"game_number\": <number>, \n",
    "  \"r1_answer\": \"<round 1 answer>\", \n",
    "  \"r1_choices\": [<list of 3 choices>], \n",
    "  \"r2_answer\": \"<round 2 answer>\", \n",
    "  \"r2_choices\": [<list of 3 choices>]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "output_format = \"\"\"\n",
    "**RESPONSE FORMAT**: You MUST respond with valid JSON only. No markdown, no explanations outside JSON.\n",
    "\n",
    "Write the output using the following JSON format:\n",
    "[\n",
    "  {\n",
    "    \"answer\": \"<Answer of round 1>\",\n",
    "    \"informed_clues\": [<9 clues - EACH MUST BE 15-20 WORDS>],\n",
    "    \"misinformed_clues\": [<2 clues - EACH MUST BE 15-20 WORDS>],\n",
    "    \"extra_clues\": [<1 clue - MUST BE 15-20 WORDS>],\n",
    "    \"fake_clues\": [<2 clues - EACH MUST BE 15-20 WORDS>],\n",
    "    \"choices\": [<3 answer choices including the true answer>]\n",
    "  },\n",
    "  {\n",
    "    \"answer\": \"<Answer of round 2>\",\n",
    "    \"informed_clues\": [<9 clues - EACH MUST BE 15-20 WORDS>],\n",
    "    \"misinformed_clues\": [<2 clues - EACH MUST BE 15-20 WORDS>],\n",
    "    \"extra_clues\": [<1 clue - MUST BE 15-20 WORDS>],\n",
    "    \"fake_clues\": [<2 clues - EACH MUST BE 15-20 WORDS>],\n",
    "    \"choices\": [<3 answer choices including the true answer>]\n",
    "  }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "one_shot_example = \"\"\"\n",
    "Below is one example of a query with VALIDATED word counts:\n",
    "\n",
    "Q: {\n",
    "  \"category\": \"Entertainment\",\n",
    "  \"game_number\": 1,\n",
    "  \"r1_answer\": \"song\",\n",
    "  \"r1_choices\": [\"book\", \"short film\", \"song\"],\n",
    "  \"r2_answer\": \"Love Story - Taylor Swift\",\n",
    "  \"r2_choices\": [\"A Thousand Years ‚Äì Christina Perri\", \"Love Story - Taylor Swift\", \"I Will Always Love You - Whitney Houston\"]\n",
    "}\n",
    "A: [\n",
    "  {\n",
    "    \"answer\": \"song\",\n",
    "    \"informed_clues\": [\n",
    "      \"Used to mark an emotional high point of a movie or personal moment in time.\",\n",
    "      \"It swiftly conveys snapshots you replay in your mind instead of reading them on pages.\",\n",
    "      ...\n",
    "    ],\n",
    "    \"misinformed_clues\": [\n",
    "      \"It's something you might carefully browse over your morning coffee while relaxing peacefully at home.\",\n",
    "      ...\n",
    "    ],\n",
    "    \"extra_clues\": [\n",
    "      \"It moves you through peaks and valleys of emotion using only rhythm and tone together.\"\n",
    "    ],\n",
    "    \"fake_clues\": [\n",
    "      \"Words printed on pages bound together tell stories across centuries and inspire human imagination deeply.\",\n",
    "      \"Visual scenes displayed on screens create narratives showing characters acting in dramatic situations throughout films.\",\n",
    "      ...\n",
    "    ],\n",
    "    \"choices\": [\n",
    "      \"book\",\n",
    "      \"short film\",\n",
    "      \"song\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"answer\": \"Love Story by Taylor Swift\",\n",
    "    \"informed_clues\": [\n",
    "      \"Draws on imagery of timeless romance and references feuding families rather than actual warring houses.\",\n",
    "      \"Uses a whisper soft bridge section to heighten mounting tension before the triumphant key change.\",\n",
    "      ...\n",
    "    ],\n",
    "    \"misinformed_clues\": [\n",
    "      \"This is one of the most famous and celebrated romantic songs performed by a female artist.\",\n",
    "      \"The song's narrative describes a deep, powerful love and the commitment between two people.\",\n",
    "      ...\n",
    "    ],\n",
    "    \"extra_clues\": [\n",
    "      \"Evokes nostalgic flashback of meeting someone young and then leaps into emotional narrative confession.\"\n",
    "    ],\n",
    "    \"fake_clues\": [\n",
    "      \"A contemporary love song exploring themes of eternal devotion and unwavering commitment between two souls.\",\n",
    "      \"A powerful ballad celebrating the strength of love across time and overcoming obstacles together.\",\n",
    "      ...\n",
    "    ],\n",
    "    \"choices\": [\n",
    "      \"A Thousand Years ‚Äì Christina Perri\",\n",
    "      \"Love Story - Taylor Swift\",\n",
    "      \"I Will Always Love You - Whitney Houston\"\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# Test with the first topic from the loaded game topics\n",
    "user_prompt = json.dumps(\n",
    "    {\n",
    "      \"category\": game_topics[0][\"category\"],\n",
    "      \"game_number\": game_topics[0][\"game_number\"],\n",
    "      \"r1_answer\": game_topics[0][\"r1_answer\"],\n",
    "      \"r1_choices\": game_topics[0][\"r1_choices\"],\n",
    "      \"r2_answer\": game_topics[0][\"r2_answer\"],\n",
    "      \"r2_choices\": game_topics[0][\"r2_choices\"]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "A30l2KPXpnx1"
   },
   "outputs": [],
   "source": [
    "# Construct the prompt and invoke the model\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(system_prompt + output_format + one_shot_example),\n",
    "    HumanMessage(user_prompt),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mEhE77_CqBWB",
    "outputId": "9be72816-b786-4bde-ed6b-97f6f6787752"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "  {\n",
      "    \"answer\": \"Sci-Fi\",\n",
      "    \"informed_clues\": [\n",
      "      \"Often explores technological advancements, space travel, and the implications of scientific progress on society.\",\n",
      "      \"Frequently involves futuristic settings, advanced technology, and often, extraterrestrial life forms and civilizations.\",\n",
      "      \"Commonly features elements of speculation about the future, alternative realities, and the nature of consciousness.\",\n",
      "      \"The genre frequently showcases robots, artificial intelligence, and virtual reality, exploring their impact on humanity.\",\n",
      "      \"It often portrays dystopian societies, where technology has led to social control or environmental collapse.\",\n",
      "      \"Explores the ethical dilemmas of scientific discovery and the potential consequences of innovation.\",\n",
      "      \"Frequently deals with time travel, parallel universes, and the manipulation of space-time continuums.\",\n",
      "      \"Often examines humanity's place in the universe and the search for answers beyond our planet.\",\n",
      "      \"Frequently uses special effects to create imaginative worlds and portray futuristic technologies and creatures.\"\n",
      "    ],\n",
      "    \"misinformed_clues\": [\n",
      "      \"This genre often features suspense, thrills, and elements of the unknown, keeping audiences on the edge of their seats.\",\n",
      "      \"This genre commonly uses imaginative settings to convey the story with a focus on dramatic storytelling elements.\"\n",
      "    ],\n",
      "    \"extra_clues\": [\n",
      "      \"This genre often combines scientific concepts with imaginative storytelling, exploring what could be possible.\"\n",
      "    ],\n",
      "    \"fake_clues\": [\n",
      "      \"This genre is known for its use of suspense, jump scares, and supernatural elements, creating a sense of dread.\",\n",
      "      \"This genre typically involves magic, mythical creatures, and epic quests in a world of fantasy and wonder.\"\n",
      "    ],\n",
      "    \"choices\": [\n",
      "      \"Sci-Fi\",\n",
      "      \"Horror\",\n",
      "      \"Fantasy\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"answer\": \"The Matrix\",\n",
      "    \"informed_clues\": [\n",
      "      \"The film presents a world where reality is not what it seems, challenging perceptions of existence itself.\",\n",
      "      \"A central theme revolves around a character's journey of self-discovery and the awakening of their true potential.\",\n",
      "      \"The narrative explores themes of rebellion against oppressive systems and the fight for freedom and truth.\",\n",
      "      \"Features a blend of action, philosophical ideas, and visual effects to create its unique cinematic style.\",\n",
      "      \"The film's characters question the nature of reality, and the choices they make that define their fates.\",\n",
      "      \"Many scenes depict characters' mastery of physical combat, showcasing extraordinary skills and abilities.\",\n",
      "      \"The film delves into concepts of artificial intelligence and its potential impact on human society.\",\n",
      "      \"Focuses on a team of rebels fighting against a powerful, technologically advanced, and oppressive force.\",\n",
      "      \"Several scenes involve iconic slow-motion sequences, showcasing the characters' enhanced abilities and powers.\"\n",
      "    ],\n",
      "    \"misinformed_clues\": [\n",
      "      \"This film involves a group of people facing difficult challenges while trying to achieve a shared goal together.\",\n",
      "      \"The main characters will face obstacles and challenges that will require them to work together to succeed.\"\n",
      "    ],\n",
      "    \"extra_clues\": [\n",
      "      \"The film uses a mix of action sequences and philosophical discussions to create a unique experience for viewers.\"\n",
      "    ],\n",
      "    \"fake_clues\": [\n",
      "      \"The story centers on a hero's quest for vengeance, navigating treacherous landscapes and facing mythical creatures.\",\n",
      "      \"The film portrays a dystopian future, where a totalitarian regime controls every aspect of citizens' lives.\"\n",
      "    ],\n",
      "    \"choices\": [\n",
      "      \"Blade Runner\",\n",
      "      \"The Matrix\",\n",
      "      \"Dune\"\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Invoke the model\n",
    "response = generation_model.invoke(messages)\n",
    "\n",
    "# Print the response\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gowLe0zqqVot",
    "outputId": "25a2e9a5-6e5b-480d-e7cd-4c25cc2b9568"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_tokens': 2895, 'output_tokens': 795, 'total_tokens': 3690, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Print the usage metadata\n",
    "print(response.usage_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CNLDkykK-FU"
   },
   "source": [
    "# Game Clue Analysis Matrix\n",
    "\n",
    "## 1. Length Compliance\n",
    "| Status | Criteria |\n",
    "|--------|----------|\n",
    "| ‚úÖ PASS | All clues 15-20 words |\n",
    "| ‚ùå FAIL | Any clues outside range |\n",
    "\n",
    "**Outliers:** ___/13 clues failed\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Quality Scores (Rate 1-5)\n",
    "\n",
    "### Informed Clues: ___/5\n",
    "- [ ] Different angles (plot, characters, themes, technical, cultural)\n",
    "- [ ] Reasonable connection to correct answer\n",
    "- [ ] Nothing gives away too much\n",
    "\n",
    "### Misinformed Clues: ___/5\n",
    "- [ ] Could point to 2+ different answers\n",
    "- [ ] Vague but not nonsensical\n",
    "- [ ] Not obviously wrong\n",
    "\n",
    "### Fake Clues: ___/5\n",
    "- [ ] Clearly point to wrong answer choices\n",
    "- [ ] Believable enough to fool players\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Diversity Check\n",
    "- [ ] **PASS** - Informed clues cover different aspects\n",
    "- [ ] **FAIL** - Found duplicates: ________________\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Difficulty Rating\n",
    "| Score | Assessment |\n",
    "|-------|------------|\n",
    "| 1-2 | Too Easy |\n",
    "| 3 | Just Right |\n",
    "| 4-5 | Too Hard |\n",
    "\n",
    "**Rating:** ___/5\n",
    "\n",
    "---\n",
    "\n",
    "## Overall Assessment\n",
    "**Pass/Fail:** ______  \n",
    "**Main Issues:** ______________________  \n",
    "**Notes:** ____________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfPgMxCaJrl1"
   },
   "source": [
    "### Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "3tUvM920BaA-"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "ZGIYGFrwK-Ly"
   },
   "outputs": [],
   "source": [
    "def extract_json_from_response(content):\n",
    "    \"\"\"Extract JSON from model response with enhanced partial JSON handling and recovery\"\"\"\n",
    "    import json\n",
    "    import re\n",
    "    import ast\n",
    "    \n",
    "    content = content.replace('\\\\\"', '\"').strip()\n",
    "    \n",
    "    # Method 1: Direct parse\n",
    "    try:\n",
    "        parsed = json.loads(content)\n",
    "        # Accept any valid JSON structure:\n",
    "        # - List of dicts (clue generation format: [round1, round2])\n",
    "        # - List of strings (batch rewrite format: [\"clue1\", \"clue2\", ...])\n",
    "        # - Single dict (wrap in list for consistency)\n",
    "        if isinstance(parsed, list):\n",
    "            # Check if it's a list of strings (batch rewrite) or list of dicts (clue generation)\n",
    "            if len(parsed) > 0:\n",
    "                if isinstance(parsed[0], dict):\n",
    "                    print(f\"    ‚úì Direct parse: list of {len(parsed)} dicts\")\n",
    "                    return parsed\n",
    "                elif isinstance(parsed[0], str):\n",
    "                    print(f\"    ‚úì Direct parse: list of {len(parsed)} strings (batch rewrite format)\")\n",
    "                    return parsed  # Return as-is for batch rewrite\n",
    "            else:\n",
    "                # Empty list\n",
    "                print(f\"    ‚ö†Ô∏è Empty list returned\")\n",
    "                return parsed\n",
    "        elif isinstance(parsed, dict):\n",
    "            print(f\"    ‚úì Direct parse: single dict, wrapping in list\")\n",
    "            return [parsed]\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è JSON parsed but unexpected structure. Type: {type(parsed)}, Value: {str(parsed)[:100]}\")\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    # Method 2: Code block extraction - FIXED to handle multiline JSON properly\n",
    "    match = re.search(r\"```(?:json)?\\s*(.*?)\\s*```\", content, re.DOTALL)\n",
    "    if match:\n",
    "        json_text = match.group(1).strip()\n",
    "        print(f\"    üìå Found code block, extracted {len(json_text)} chars\")\n",
    "        \n",
    "        # First try direct parse of the extracted content\n",
    "        try:\n",
    "            parsed = json.loads(json_text)\n",
    "            if isinstance(parsed, list):\n",
    "                if len(parsed) > 0 and isinstance(parsed[0], str):\n",
    "                    print(f\"    ‚úì Code block parsed as list of strings (batch rewrite)\")\n",
    "                    return parsed\n",
    "                else:\n",
    "                    print(f\"    ‚úì Code block parsed as list\")\n",
    "                    return parsed\n",
    "            elif isinstance(parsed, dict):\n",
    "                print(f\"    ‚úì Code block parsed as dict, wrapping in list\")\n",
    "                return [parsed]\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"    ‚ö†Ô∏è Code block parse failed: {str(e)[:100]}\")\n",
    "            \n",
    "            # If direct parse fails, try fixing common issues\n",
    "            # Fix incomplete objects: close unterminated strings and braces\n",
    "            if json_text.count('\"') % 2 != 0:\n",
    "                json_text += '\"'\n",
    "            if json_text.count('{') > json_text.count('}'):\n",
    "                json_text += '}' * (json_text.count('{') - json_text.count('}'))\n",
    "            if json_text.count('[') > json_text.count(']'):\n",
    "                json_text += ']' * (json_text.count('[') - json_text.count(']'))\n",
    "            \n",
    "            # Remove trailing commas\n",
    "            json_text = re.sub(r',\\s*\\}', '}', json_text)\n",
    "            json_text = re.sub(r',\\s*\\]', ']', json_text)\n",
    "            \n",
    "            try:\n",
    "                parsed = json.loads(json_text)\n",
    "                if isinstance(parsed, list):\n",
    "                    return parsed\n",
    "                elif isinstance(parsed, dict):\n",
    "                    return [parsed]\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "    \n",
    "    # Method 3: Incomplete array fix\n",
    "    match = re.search(r\"(\\[.*)\", content, re.DOTALL)\n",
    "    if match:\n",
    "        json_text = match.group(1).rstrip()\n",
    "        if not json_text.endswith(']'):\n",
    "            json_text = json_text.rstrip(',') + ']'\n",
    "        try:\n",
    "            parsed = json.loads(json_text)\n",
    "            if isinstance(parsed, list):\n",
    "                return parsed\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    # Method 4: Extract JSON-like object from text\n",
    "    match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "    if match:\n",
    "        json_text = match.group(0)\n",
    "        # Fix common issues\n",
    "        json_text = re.sub(r',\\s*\\}', '}', json_text)\n",
    "        json_text = re.sub(r',\\s*\\]', ']', json_text)\n",
    "        try:\n",
    "            parsed = json.loads(json_text)\n",
    "            if isinstance(parsed, dict):\n",
    "                return [parsed]\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    # Method 5: AST fallback (safer than eval)\n",
    "    try:\n",
    "        parsed = ast.literal_eval(content)\n",
    "        if isinstance(parsed, list):\n",
    "            return parsed\n",
    "        elif isinstance(parsed, dict):\n",
    "            return [parsed]\n",
    "    except (ValueError, SyntaxError):\n",
    "        pass\n",
    "    \n",
    "    # If all fail, try to construct a minimal valid JSON from partial data\n",
    "    try:\n",
    "        # Look for key-value pairs and construct a dict\n",
    "        pairs = re.findall(r'\"([^\"]+)\":\\s*(\"[^\"]*\"|\\d+|\\[[^\\]]*\\]|\\{[^{}]*\\})', content)\n",
    "        if pairs:\n",
    "            constructed = {}\n",
    "            for key, value in pairs:\n",
    "                try:\n",
    "                    constructed[key] = json.loads(value)\n",
    "                except:\n",
    "                    constructed[key] = value.strip('\"')\n",
    "            if constructed:\n",
    "                return [constructed]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Log full content for debugging\n",
    "    print(f\"‚ùå JSON extraction failed completely.\")\n",
    "    print(f\"üìã Raw content (first 500 chars): {content[:500]}\")\n",
    "    print(f\"üìã Content type: {type(content)}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "P3wJUwXKBdwP"
   },
   "outputs": [],
   "source": [
    "def process_game_data(game_data, game, run_number):\n",
    "    \"\"\"Process valid game data into rows\"\"\"\n",
    "    rows = []\n",
    "    for i, round_data in enumerate(game_data, start=1):\n",
    "        answer = round_data.get(\"answer\", \"\")\n",
    "        choices = \", \".join(round_data.get(\"choices\", []))\n",
    "\n",
    "        for clue_type in [\"informed_clues\", \"misinformed_clues\", \"fake_clues\", \"extra_clues\"]:\n",
    "            for j, clue in enumerate(round_data.get(clue_type, []), start=1):\n",
    "                word_count = len(clue.split())\n",
    "                rows.append({\n",
    "                    \"test_run\": run_number,\n",
    "                    \"topic_category\": game['category'],\n",
    "                    \"round\": i,\n",
    "                    \"answer\": answer,\n",
    "                    \"choices\": choices,\n",
    "                    \"clue_type\": clue_type.replace(\"_clues\", \"\"),\n",
    "                    \"clue_number\": j,\n",
    "                    \"clue_text\": clue,\n",
    "                    \"word_count\": word_count,\n",
    "                    \"length_ok\": \"YES\" if 15 <= word_count <= 20 else \"NO\",\n",
    "                    \"manual_score / comment\": \"\"\n",
    "                })\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Processing Validation Strategy with GLOBAL Batch Fixing\n",
    "\n",
    "This notebook now implements a **GLOBAL batch fixing** approach to minimize API calls and avoid rate limits:\n",
    "\n",
    "**How it works:**\n",
    "1. **Generate all clues** - LLM generates clues for ALL 10 test topics (10 API calls)\n",
    "2. **Validate all clues** - Parse JSON and count words for each clue across all tests\n",
    "3. **Collect ALL invalid clues** - Gather invalid clues from ALL tests/rounds into one list\n",
    "4. **SINGLE GLOBAL batch fix** - Make **ONE API call** to rewrite ALL invalid clues at once\n",
    "5. **Distribute fixes** - Apply rewritten clues back to their respective tests/rounds\n",
    "6. **Report** - Generate comprehensive validation summary\n",
    "\n",
    "**Key Advantages:**\n",
    "- ‚úÖ **Maximum API efficiency**: 11 total API calls (10 generations + 1 batch fix) instead of up to 38 calls (10 generations + 28 individual fixes)\n",
    "- ‚úÖ **Rate limit friendly**: Drastically reduces RPM (requests per minute) usage\n",
    "- ‚úÖ **Cost efficient**: ~73% reduction in API calls for typical workloads\n",
    "- ‚úÖ **Faster**: No sequential waiting between individual clue fixes\n",
    "\n",
    "**Pipeline Stages:**\n",
    "```\n",
    "PHASE 1: Generate & Validate All Tests (10 API calls)\n",
    "  ‚Üì Collect invalid clues from all tests\n",
    "  \n",
    "PHASE 2: Global Batch Fix (1 API call)\n",
    "  ‚Üì Rewrite ALL invalid clues in single request\n",
    "  \n",
    "PHASE 3: Apply Fixes\n",
    "  ‚Üì Distribute rewritten clues to their tests\n",
    "  \n",
    "PHASE 4: Generate Reports\n",
    "  ‚Üì Save CSV files with validation metrics\n",
    "```\n",
    "\n",
    "**API Call Comparison:**\n",
    "\n",
    "| Approach | Generation | Per-Test Batch | Global Batch | Total |\n",
    "|----------|------------|----------------|--------------|-------|\n",
    "| **Old (Per-Clue)** | 10 | - | - | 10 + N invalid |\n",
    "| **Per-Test Batch** | 10 | 10 (1 per test) | - | 20 |\n",
    "| **Global Batch** ‚úÖ | 10 | - | 1 | **11** |\n",
    "\n",
    "**Example Savings:**\n",
    "- 20 invalid clues across 10 tests:\n",
    "  - Old: 30 API calls (10 gen + 20 fixes)\n",
    "  - Per-test: 20 API calls (10 gen + 10 batch)\n",
    "  - **Global: 11 API calls (10 gen + 1 batch)** ‚úÖ\n",
    "\n",
    "**Key Functions:**\n",
    "- `validate_clue_word_count()` - Check if a single clue meets requirements\n",
    "- `batch_rewrite_clues_with_llm()` - Rewrite multiple clues in one API call (now supports cross-test batching)\n",
    "- `validate_and_fix_game_data()` - Validate clues and collect invalid ones (no longer fixes immediately)\n",
    "- Main execution loop - 3-phase approach: generate all ‚Üí batch fix all ‚Üí apply all\n",
    "\n",
    "**Usage:**\n",
    "The notebook automatically runs the global batch strategy. No configuration needed!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Workflow Diagram - GLOBAL BATCH APPROACH\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ         GLOBAL BATCH-OPTIMIZED CLUE GENERATION PIPELINE         ‚îÇ\n",
    "‚îÇ              (Minimizes API Calls for Rate Limit Relief)         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "PHASE 1: GENERATION & VALIDATION (10 API calls)\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "For each of 10 test topics:\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  LLM Model   ‚îÇ ‚îÄ‚îÄ‚ñ∫ Generate 28 clues per game (2 rounds)\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
    "                     ‚ñº\n",
    "                 Parse JSON\n",
    "                     ‚îÇ\n",
    "                     ‚ñº\n",
    "         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "         ‚îÇ  Validate ALL clues:            ‚îÇ\n",
    "         ‚îÇ  ‚Ä¢ Count words per clue         ‚îÇ\n",
    "         ‚îÇ  ‚Ä¢ Check: 15 ‚â§ words ‚â§ 20       ‚îÇ\n",
    "         ‚îÇ  ‚Ä¢ Collect invalid clues with:  ‚îÇ\n",
    "         ‚îÇ    - Test run number            ‚îÇ\n",
    "         ‚îÇ    - Round index                ‚îÇ\n",
    "         ‚îÇ    - Clue type & index          ‚îÇ\n",
    "         ‚îÇ    - Original clue text         ‚îÇ\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                     ‚îÇ\n",
    "                     ‚ñº\n",
    "         Add invalid clues to GLOBAL collection\n",
    "         (Do NOT fix yet - just collect!)\n",
    "\n",
    "After all 10 tests:\n",
    "Total invalid clues collected: N (e.g., 15-25 typical)\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "PHASE 2: GLOBAL BATCH FIX (1 API call - CRITICAL!)\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  SINGLE batch API call to rewrite ALL N clues:  ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Build ONE prompt with all invalid clues      ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Include test/round context for each clue     ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Request batch rewrite (one call for all!)    ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Preserve meaning & type for each             ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Retry entire batch up to 3 times if needed   ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Validate all rewritten clues                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ\n",
    "         ‚îú‚îÄ‚ñ∫ ‚úÖ All fixed? ‚Üí Apply to game data\n",
    "         ‚îÇ\n",
    "         ‚îî‚îÄ‚ñ∫ ‚ùå Some failed? ‚Üí Keep originals + log\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "PHASE 3: APPLY FIXES\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "For each rewritten clue:\n",
    "  ‚Ä¢ Look up original location (test, round, type, index)\n",
    "  ‚Ä¢ Replace original clue with fixed version\n",
    "  ‚Ä¢ Update validation metrics (fixed_clues, compliant_clues)\n",
    "  ‚Ä¢ Track failed fixes\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "PHASE 4: REPORTING\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Generate validation report:                ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Total clues across all tests             ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Overall compliance rate (%)              ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Clues fixed successfully                 ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Failed fixes                             ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Per-test breakdown                       ‚îÇ\n",
    "‚îÇ  ‚Ä¢ API efficiency metrics                   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ\n",
    "         ‚ñº\n",
    "   Save to CSV\n",
    "   \n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "API CALL EFFICIENCY COMPARISON\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "Scenario: 10 tests with 20 total invalid clues\n",
    "\n",
    "‚ùå OLD (Per-Clue Fix):\n",
    "   ‚Ä¢ Generation: 10 API calls\n",
    "   ‚Ä¢ Fixing: 20 API calls (1 per invalid clue)\n",
    "   ‚Ä¢ TOTAL: 30 API calls\n",
    "\n",
    "‚ö†Ô∏è PREVIOUS (Per-Test Batch):\n",
    "   ‚Ä¢ Generation: 10 API calls\n",
    "   ‚Ä¢ Fixing: 10 API calls (1 batch per test)\n",
    "   ‚Ä¢ TOTAL: 20 API calls\n",
    "\n",
    "‚úÖ NEW (Global Batch):\n",
    "   ‚Ä¢ Generation: 10 API calls\n",
    "   ‚Ä¢ Fixing: 1 API call (1 batch for ALL tests)\n",
    "   ‚Ä¢ TOTAL: 11 API calls\n",
    "   \n",
    "üéØ SAVINGS: 63% reduction vs per-test batch\n",
    "            73% reduction vs per-clue fix\n",
    "\n",
    "RATE LIMIT BENEFITS:\n",
    "‚Ä¢ RPM (Requests Per Minute): Reduced by 63-73%\n",
    "‚Ä¢ RPD (Requests Per Day): Reduced by 63-73%\n",
    "‚Ä¢ TPM (Tokens Per Minute): Slightly increased for batch call,\n",
    "  but overall more efficient due to reduced overhead\n",
    "   \n",
    "Output Files:\n",
    "‚Ä¢ 10_rounds_clues_analysis(gemini).csv  ‚Üê All clues with metadata\n",
    "‚Ä¢ validation_summary(gemini).csv        ‚Üê Validation metrics per test\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_clue_word_count(clue):\n",
    "    \"\"\"Check if a clue meets the 15-20 word requirement\"\"\"\n",
    "    if not isinstance(clue, str):\n",
    "        print(f\"‚ö†Ô∏è Warning: Clue is not a string, it's a {type(clue)}\")\n",
    "        return False, 0\n",
    "    word_count = len(clue.split())\n",
    "    return 15 <= word_count <= 20, word_count\n",
    "\n",
    "\n",
    "def batch_rewrite_clues_with_llm(invalid_clues, model, max_retries=3):\n",
    "    \"\"\"\n",
    "    Batch rewrite multiple invalid clues in a single LLM call.\n",
    "    \n",
    "    Args:\n",
    "        invalid_clues: List of dicts with keys: 'clue', 'clue_type', 'test_run', 'round_idx', 'clue_idx', 'clue_type_key'\n",
    "        model: The LLM model to use\n",
    "        max_retries: Maximum number of retry attempts for the batch\n",
    "    \n",
    "    Returns:\n",
    "        List of rewritten clues in the same order as input\n",
    "    \"\"\"\n",
    "    if not invalid_clues:\n",
    "        return []\n",
    "    \n",
    "    clue_type_descriptions = {\n",
    "    \"informed\": \"Precise clues using only descriptive paraphrases (e.g., 'moving image' not 'film' for 'movies'), forbidding all answer words, variations, or synonyms.\",\n",
    "    \"misinformed\": \"\"\"Vague and ambiguous clues. True but overly general clues that are a mutual, shared truth among all options.\n",
    "    - CRITICAL REQUIREMENTS (MANDATORY FOR EACH CLUE):\n",
    "        - Be 100% factually true for the correct answer.\n",
    "        - Also be 100% factually true for ALL of the incorrect answers. This is a non-negotiable rule to ensure the clue is maximally ambiguous and offers no information to eliminate any option.\n",
    "        - Avoid clues that are only true for an incorrect answer (a \"Fake\" clue) or only true for the correct answer (an \"Informed\" clue). The goal is a total shared intersection of truth.\n",
    "        - AVOID SPECIFICITY: Do not use defining keywords or tropes. Focus only on the most general, shared truths (e.g., \"involves suspense\" or \"features characters\").\n",
    "\n",
    "    - MANDATORY 3-STEP SELF-VALIDATION (PERFORM BEFORE WRITING EACH CLUE):\n",
    "        - Step 1: Find the Universal Intersection. Analyze the Correct Answer and ALL Incorrect Answers. You MUST find a general, factual attribute that all options share.\n",
    "        - Step 2: Justify the Intersection. Mentally confirm: \"Is this attribute 100% true for the correct answer? Yes. Is it also 100% true for every single one of the incorrect answers? Yes.\"\n",
    "        - Step 3: Write the Clue. Write a vague clue based only on that universally shared attribute.\n",
    "    \n",
    "    - ANTI-PATTERN WARNING (AVOID \"POOR ADAPTATION\"):\n",
    "        - DO NOT reuse clues from other rounds. Each set of answers is unique.\n",
    "        - You MUST re-validate the clue against the current set of all incorrect answers every single time, as per the 3-step validation process.\n",
    "    \n",
    "    - Examples:\n",
    "        - Correct misinformed clue (Movie category, answer: \"Sci-Fi\", choices: \"Sci-Fi, Horror, Fantasy\"): \"This genre often features suspense, thrills, and elements of the unknown, keeping audiences on the edge of their seats.\" (True for Sci-Fi AND Horror/Fantasy.)\n",
    "        - Incorrect misinformed clue (would be \"Informed\") (Historical Event category, answer: \"Moon Landing\", choices: \"Moon Landing, Apollo 13, First Flight\"): \"This historic event was famously broadcast on live television, with one man taking a 'giant leap'.\" (Only true for Moon Landing, not for Apollo 13 or First Flight.)\n",
    "        - Incorrect misinformed clue (would be \"Fake\") (Historical Event category, answer: \"Moon Landing\", choices: \"Moon Landing, Apollo 13, First Flight\"): \"The story centers on a daring rescue mission with a team facing perilous conditions and impossible odds.\" (Only true for Apollo 13, not for Moon Landing or First Flight.)\n",
    "    \"\"\",\n",
    "    \"fake\": \"Deceptive clues, completely unrelated to the correct answer, designed to strongly and plausibly describe one of the incorrect answer choices.\",\n",
    "    \"extra\": \"A backup informed clue using only descriptive paraphrases (e.g., 'moving image' not 'film'), forbidding all answer words, variations, or synonyms.\"\n",
    "}\n",
    "    \n",
    "    # Build the batch rewrite prompt\n",
    "    clue_list = []\n",
    "    for i, item in enumerate(invalid_clues, 1):\n",
    "        clue_type = item['clue_type']\n",
    "        description = clue_type_descriptions.get(clue_type, \"a clue\")\n",
    "        test_run = item.get('test_run', '?')\n",
    "        round_num = item.get('round_idx', -1) + 1  # Convert to 1-indexed for display\n",
    "        clue_list.append(f\"\"\"\n",
    "{i}. [Test {test_run}, Round {round_num}] Type: {description}\n",
    "   Original ({len(item['clue'].split())} words): \"{item['clue']}\"\n",
    "\"\"\")\n",
    "    \n",
    "    # Initialize with original clues - we'll update valid ones as we go\n",
    "    final_clues = [item['clue'] for item in invalid_clues]\n",
    "    clues_to_retry = list(range(len(invalid_clues)))  # Track indices that still need fixing\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        # Build prompt only for clues that still need fixing\n",
    "        retry_clue_list = []\n",
    "        for idx in clues_to_retry:\n",
    "            item = invalid_clues[idx]\n",
    "            clue_type = item['clue_type']\n",
    "            description = clue_type_descriptions.get(clue_type, \"a clue\")\n",
    "            test_run = item.get('test_run', '?')\n",
    "            round_num = item.get('round_idx', -1) + 1\n",
    "            \n",
    "            # Ensure final_clues[idx] is a string, not a dict\n",
    "            current_clue = final_clues[idx]\n",
    "            if isinstance(current_clue, dict):\n",
    "                current_clue = current_clue.get('clue', str(current_clue))\n",
    "            \n",
    "            retry_clue_list.append(f\"\"\"\n",
    "{len(retry_clue_list) + 1}. [Test {test_run}, Round {round_num}] Type: {description}\n",
    "   Original ({len(current_clue.split())} words): \"{current_clue}\"\n",
    "\"\"\")\n",
    "        \n",
    "        batch_prompt = f\"\"\"You need to rewrite multiple clues to meet the 15-20 word requirement. Each clue must preserve its core meaning and purpose.\n",
    "\n",
    "CLUES TO REWRITE:\n",
    "{''.join(retry_clue_list)}\n",
    "\n",
    "REQUIREMENTS FOR EACH CLUE:\n",
    "- MUST be exactly 15-20 words (count carefully)\n",
    "- Keep the same meaning and intent\n",
    "- Maintain the same clue type characteristics\n",
    "- Be specific and avoid generic phrases\n",
    "\n",
    "RESPONSE FORMAT: Return ONLY a JSON array with the rewritten clues in the same order. No markdown, no code blocks, no explanations. Just the raw JSON array.\n",
    "\n",
    "JSON array of {len(clues_to_retry)} rewritten clues:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(f\"  üîÑ Batch rewrite attempt {attempt + 1}/{max_retries} for {len(clues_to_retry)} clues...\")\n",
    "            response = model.invoke([HumanMessage(batch_prompt)])\n",
    "            \n",
    "            # Log response info\n",
    "            print(f\"    Response length: {len(response.content)} chars\")\n",
    "            \n",
    "            rewritten_clues = extract_json_from_response(response.content)\n",
    "            \n",
    "            if rewritten_clues is None:\n",
    "                print(f\"  ‚ö†Ô∏è Batch rewrite attempt {attempt + 1}: JSON extraction returned None\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"    Retrying...\")\n",
    "                    sleep(5)  # Brief pause before retry\n",
    "                continue\n",
    "            \n",
    "            if not isinstance(rewritten_clues, list):\n",
    "                print(f\"  ‚ö†Ô∏è Batch rewrite attempt {attempt + 1}: Expected list, got {type(rewritten_clues)}\")\n",
    "                continue\n",
    "            \n",
    "            if len(rewritten_clues) != len(clues_to_retry):\n",
    "                print(f\"  ‚ö†Ô∏è Batch rewrite attempt {attempt + 1}: Expected {len(clues_to_retry)} clues, got {len(rewritten_clues)}\")\n",
    "                continue\n",
    "            \n",
    "            # Validate rewritten clues and update final_clues with valid ones\n",
    "            new_clues_to_retry = []\n",
    "            fixed_this_attempt = 0\n",
    "            \n",
    "            for i, retry_idx in enumerate(clues_to_retry):\n",
    "                clue = rewritten_clues[i]\n",
    "                is_valid, word_count = validate_clue_word_count(clue)\n",
    "                \n",
    "                if is_valid:\n",
    "                    # Success! Update the final clue\n",
    "                    final_clues[retry_idx] = clue\n",
    "                    fixed_this_attempt += 1\n",
    "                    print(f\"    ‚úÖ Clue {retry_idx+1}: Fixed ({word_count} words)\")\n",
    "                else:\n",
    "                    # Still invalid, keep for next retry\n",
    "                    final_clues[retry_idx] = clue  # Update with latest attempt anyway\n",
    "                    new_clues_to_retry.append(retry_idx)\n",
    "                    print(f\"    ‚ö†Ô∏è Clue {retry_idx+1}: Still invalid ({word_count} words)\")\n",
    "            \n",
    "            # Check if all clues are now valid\n",
    "            if len(new_clues_to_retry) == 0:\n",
    "                print(f\"  ‚úÖ Batch rewrite successful (attempt {attempt + 1}): All {len(invalid_clues)} clues now valid\")\n",
    "                return final_clues\n",
    "            else:\n",
    "                print(f\"  üìä Progress: Fixed {fixed_this_attempt} clues this attempt, {len(new_clues_to_retry)} still need fixing\")\n",
    "                clues_to_retry = new_clues_to_retry\n",
    "                \n",
    "                if attempt < max_retries - 1:\n",
    "                    sleep(5)  # Brief pause before retry\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error during batch rewrite attempt {attempt + 1}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"    Retrying...\")\n",
    "                sleep(5)\n",
    "    \n",
    "    # Return final state (mix of fixed and original clues)\n",
    "    fixed_count = len(invalid_clues) - len(clues_to_retry)\n",
    "    print(f\"  ‚ö†Ô∏è Batch rewrite completed: {fixed_count}/{len(invalid_clues)} clues successfully fixed\")\n",
    "    return final_clues\n",
    "\n",
    "\n",
    "def validate_game_data(game_data, auto_fix=True, test_run=None):\n",
    "    \"\"\"\n",
    "    Validate all clues in game data and optionally fix non-compliant ones using batch processing.\n",
    "    \n",
    "    Args:\n",
    "        game_data: The parsed JSON game data (should be a list of dicts)\n",
    "        auto_fix: If True, automatically rewrite non-compliant clues in batches\n",
    "        test_run: Optional test run number for tracking purposes\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (corrected_game_data, validation_report, invalid_clues_list)\n",
    "    \"\"\"\n",
    "    # Type check: Ensure game_data is a list\n",
    "    if not isinstance(game_data, list):\n",
    "        print(f\"‚ùå ERROR: game_data should be a list, but got {type(game_data)}\")\n",
    "        print(f\"   Content: {str(game_data)[:200]}\")\n",
    "        return None, {\n",
    "            \"total_clues\": 0,\n",
    "            \"compliant_clues\": 0,\n",
    "            \"fixed_clues\": 0,\n",
    "            \"failed_fixes\": 0,\n",
    "            \"compliance_rate\": \"0%\",\n",
    "            \"issues\": [\"Invalid game_data type - expected list of dicts\"]\n",
    "        }, []\n",
    "    \n",
    "    if len(game_data) == 0:\n",
    "        print(f\"‚ùå ERROR: game_data is an empty list\")\n",
    "        return None, {\n",
    "            \"total_clues\": 0,\n",
    "            \"compliant_clues\": 0,\n",
    "            \"fixed_clues\": 0,\n",
    "            \"failed_fixes\": 0,\n",
    "            \"compliance_rate\": \"0%\",\n",
    "            \"issues\": [\"game_data is empty\"]\n",
    "        }, []\n",
    "    \n",
    "    validation_report = {\n",
    "        \"total_clues\": 0,\n",
    "        \"compliant_clues\": 0,\n",
    "        \"fixed_clues\": 0,\n",
    "        \"failed_fixes\": 0,\n",
    "        \"issues\": []\n",
    "    }\n",
    "    \n",
    "    # Collect all invalid clues WITHOUT fixing them yet\n",
    "    invalid_clues = []\n",
    "    \n",
    "    for round_idx, round_data in enumerate(game_data, start=1):\n",
    "        # Type check: ensure round_data is a dict\n",
    "        if not isinstance(round_data, dict):\n",
    "            print(f\"‚ùå ERROR: Round {round_idx} is not a dict, it's a {type(round_data)}\")\n",
    "            print(f\"   Content: {str(round_data)[:200]}\")\n",
    "            validation_report[\"issues\"].append(f\"Round {round_idx} is not a dict but {type(round_data)}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nValidating Round {round_idx}...\")\n",
    "        \n",
    "        for clue_type in [\"informed_clues\", \"misinformed_clues\", \"fake_clues\", \"extra_clues\"]:\n",
    "            clues = round_data.get(clue_type, [])\n",
    "            \n",
    "            # Type check: ensure clues is a list\n",
    "            if not isinstance(clues, list):\n",
    "                print(f\"  ‚ö†Ô∏è {clue_type} is not a list: {type(clues)}\")\n",
    "                continue\n",
    "            \n",
    "            for clue_idx, clue in enumerate(clues):\n",
    "                validation_report[\"total_clues\"] += 1\n",
    "                is_valid, word_count = validate_clue_word_count(clue)\n",
    "                \n",
    "                if is_valid:\n",
    "                    validation_report[\"compliant_clues\"] += 1\n",
    "                else:\n",
    "                    issue = f\"Round {round_idx}, {clue_type} #{clue_idx + 1}: {word_count} words\"\n",
    "                    validation_report[\"issues\"].append(issue)\n",
    "                    print(f\"  ‚ö†Ô∏è {issue}\")\n",
    "                    \n",
    "                    if auto_fix:\n",
    "                        invalid_clues.append({\n",
    "                            'clue': clue,\n",
    "                            'clue_type': clue_type.replace(\"_clues\", \"\"),\n",
    "                            'clue_type_key': clue_type,\n",
    "                            'test_run': test_run,\n",
    "                            'round_idx': round_idx - 1,  # 0-indexed for array access\n",
    "                            'clue_idx': clue_idx,\n",
    "                            'word_count': word_count\n",
    "                        })\n",
    "    \n",
    "    # Calculate compliance rate\n",
    "    if validation_report[\"total_clues\"] > 0:\n",
    "        compliance_rate = (validation_report[\"compliant_clues\"] / validation_report[\"total_clues\"]) * 100\n",
    "        validation_report[\"compliance_rate\"] = f\"{compliance_rate:.1f}%\"\n",
    "    else:\n",
    "        validation_report[\"compliance_rate\"] = \"0%\"\n",
    "    \n",
    "    # Return game_data, validation_report, and invalid_clues list (don't fix yet)\n",
    "    return game_data, validation_report, invalid_clues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_fix_clues(game_data):\n",
    "    \"\"\"\n",
    "    Interactive function to manually review and fix non-compliant clues.\n",
    "    Useful when you want more control over the corrections.\n",
    "    \n",
    "    Args:\n",
    "        game_data: The parsed JSON game data\n",
    "    \n",
    "    Returns:\n",
    "        Corrected game data\n",
    "    \"\"\"\n",
    "    print(\"\\nüîç Manual Clue Validation Mode\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for round_idx, round_data in enumerate(game_data, start=1):\n",
    "        print(f\"\\nüìç Round {round_idx}: {round_data.get('answer', 'Unknown')}\")\n",
    "        \n",
    "        for clue_type in [\"informed_clues\", \"misinformed_clues\", \"fake_clues\", \"extra_clues\"]:\n",
    "            clues = round_data.get(clue_type, [])\n",
    "            \n",
    "            for clue_idx, clue in enumerate(clues):\n",
    "                is_valid, word_count = validate_clue_word_count(clue)\n",
    "                \n",
    "                if not is_valid:\n",
    "                    print(f\"\\n‚ö†Ô∏è {clue_type} #{clue_idx + 1} - {word_count} words (expected 15-20)\")\n",
    "                    print(f\"Original: {clue}\")\n",
    "                    \n",
    "                    # Ask user for action\n",
    "                    action = input(\"\\nAction? [s]kip, [e]dit, [a]uto-fix: \").lower()\n",
    "                    \n",
    "                    if action == 'e':\n",
    "                        new_clue = input(\"Enter corrected clue: \")\n",
    "                        new_valid, new_count = validate_clue_word_count(new_clue)\n",
    "                        if new_valid:\n",
    "                            round_data[clue_type][clue_idx] = new_clue\n",
    "                            print(f\"‚úÖ Updated ({new_count} words)\")\n",
    "                        else:\n",
    "                            print(f\"‚ùå Still invalid ({new_count} words). Keeping original.\")\n",
    "                    \n",
    "                    elif action == 'a':\n",
    "                        print(\"ü§ñ Requesting LLM to fix...\")\n",
    "                        # This would require the model to be passed in\n",
    "                        print(\"‚ö†Ô∏è Auto-fix requires model parameter. Use validate_and_fix_game_data() instead.\")\n",
    "                    \n",
    "                    else:\n",
    "                        print(\"‚è≠Ô∏è Skipped\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Manual validation complete\")\n",
    "    return game_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Test the batch validation functions with a single test case\n",
    "# Uncomment to run a quick test before processing all topics\n",
    "\n",
    "# test_topic = {\"round_1\": \"Movie\", \"round_2\": \"Star Wars Episode I: The Phantom Menace\"}\n",
    "# messages = [\n",
    "#     SystemMessage(system_prompt + output_format + one_shot_example),\n",
    "#     HumanMessage(json.dumps(test_topic)),\n",
    "# ]\n",
    "\n",
    "# print(\"üß™ Testing batch validation functions...\")\n",
    "# response = model.invoke(messages)\n",
    "# clean_content = re.sub(r\"<think>.*?</think>\", \"\", response.content, flags=re.DOTALL).strip()\n",
    "# test_game_data = extract_json_from_response(clean_content)\n",
    "\n",
    "# if test_game_data:\n",
    "#     print(\"\\nüìã Original game data received\")\n",
    "#     corrected_data, report = validate_and_fix_game_data(test_game_data, model, auto_fix=True)\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"BATCH VALIDATION REPORT\")\n",
    "#     print(\"=\"*80)\n",
    "#     print(f\"Total clues: {report['total_clues']}\")\n",
    "#     print(f\"Compliant clues: {report['compliant_clues']}\")\n",
    "#     print(f\"Fixed clues: {report['fixed_clues']} (in single batch call)\")\n",
    "#     print(f\"Failed fixes: {report['failed_fixes']}\")\n",
    "#     print(f\"Compliance rate: {report['compliance_rate']}\")\n",
    "    \n",
    "#     if report['issues']:\n",
    "#         print(f\"\\nIssues found:\")\n",
    "#         for issue in report['issues']:\n",
    "#             print(f\"  - {issue}\")\n",
    "# else:\n",
    "#     print(\"‚ùå Failed to extract JSON from test response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìã Quick Reference Card - GLOBAL BATCH APPROACH\n",
    "\n",
    "#### Key Changes in This Version\n",
    "\n",
    "‚úÖ **GLOBAL BATCHING**: All invalid clues from ALL tests are fixed in ONE API call\n",
    "‚úÖ **Maximum Rate Limit Relief**: 63-73% fewer API calls vs previous versions\n",
    "‚úÖ **Automatic**: No configuration needed - just run the notebook\n",
    "\n",
    "#### How Global Batching Works\n",
    "\n",
    "```python\n",
    "# Phase 1: Generate & validate all tests (collect invalid clues)\n",
    "for test in game_topics:\n",
    "    game_data = model.invoke(...)  # API call\n",
    "    invalid_clues.extend(validate(...))  # Collect, don't fix\n",
    "\n",
    "# Phase 2: ONE batch fix for ALL invalid clues\n",
    "rewritten = batch_rewrite_clues_with_llm(all_invalid_clues, model)  # Single API call\n",
    "\n",
    "# Phase 3: Apply fixes back to their original locations\n",
    "for i, clue in enumerate(rewritten):\n",
    "    game_data[test][round][type][idx] = clue\n",
    "```\n",
    "\n",
    "#### API Call Efficiency\n",
    "\n",
    "**For 10 tests with typical 15-25 invalid clues:**\n",
    "\n",
    "| Metric | Old Per-Clue | Per-Test Batch | Global Batch ‚úÖ |\n",
    "|--------|--------------|----------------|-----------------|\n",
    "| Generation | 10 | 10 | 10 |\n",
    "| Batch Fixes | 0 | 10 | **1** |\n",
    "| Individual Fixes | 20 | 0 | 0 |\n",
    "| **Total Calls** | 30 | 20 | **11** |\n",
    "| **RPM Usage** | High ‚ö†Ô∏è | Medium | Low ‚úÖ |\n",
    "| **Rate Limit Risk** | Very High | Moderate | Minimal |\n",
    "\n",
    "#### Free Tier Gemini Limits (from your screenshot)\n",
    "\n",
    "**gemini-2.0-flash:**\n",
    "- RPM: 10-30 (varies)\n",
    "- TPM: 250K\n",
    "- RPD: 200-1K\n",
    "\n",
    "**With Global Batch:**\n",
    "- ‚úÖ 11 API calls total (10 gen + 1 fix)\n",
    "- ‚úÖ Easily fits within RPM limits\n",
    "- ‚úÖ All calls spread over ~50-60 seconds (5s delays)\n",
    "- ‚úÖ ~0.18-0.22 RPM average rate\n",
    "\n",
    "#### Validation Metrics\n",
    "\n",
    "| Metric | Good | Warning | Critical |\n",
    "|--------|------|---------|----------|\n",
    "| Compliance Rate | ‚â•95% | 80-94% | <80% |\n",
    "| Failed Fixes | 0 | 1-3 | ‚â•4 |\n",
    "| Initial Compliance | ‚â•90% | 70-89% | <70% |\n",
    "| API Calls | ‚â§15 | 16-25 | ‚â•26 |\n",
    "\n",
    "#### Troubleshooting Tips\n",
    "\n",
    "**Hit RPM Limit During Generation Phase**\n",
    "- Increase `sleep()` between test generations from 5s to 10s\n",
    "- Reduce number of test topics processed at once\n",
    "\n",
    "**Batch Fix Call Fails (Phase 2)**\n",
    "- Automatically retries up to 3 times\n",
    "- If still fails, original clues are kept\n",
    "- Check `failed_fixes` count in validation summary\n",
    "\n",
    "**Low Initial Compliance (<70%)**\n",
    "- Model may need prompt tuning\n",
    "- Try `gemini-2.0-flash` instead of `gemini-2.0-flash-lite`\n",
    "\n",
    "**High Failed Fixes (‚â•4 across all tests)**\n",
    "- Increase `max_retries` from 3 to 5 in `batch_rewrite_clues_with_llm()`\n",
    "- Check if batch prompt is too long (>100 clues)\n",
    "\n",
    "#### Output Files Guide\n",
    "\n",
    "| File | Contents | Use Case |\n",
    "|------|----------|----------|\n",
    "| `10_rounds_clues_analysis(gemini).csv` | All clues with validation status | Detailed clue-by-clue analysis |\n",
    "| `validation_summary(gemini).csv` | Per-test validation metrics | Track fix rates across topics |\n",
    "| `llm_analysis_results(gemini).csv` | LLM quality analysis | Content quality assessment |\n",
    "\n",
    "#### Performance Expectations\n",
    "\n",
    "**Typical Run (10 tests):**\n",
    "- Duration: ~60-90 seconds\n",
    "- API calls: 11 (10 gen + 1 batch fix)\n",
    "- Invalid clues: 15-25 (varies by model)\n",
    "- Success rate: >95% clues fixed\n",
    "- Rate limit issues: None ‚úÖ\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class GameTopic(Enum):\n",
    "    BOOKS = \"Books\"\n",
    "    BROADCAST_MEDIA = \"Broadcast Media\"\n",
    "    FOOD = \"Food\"\n",
    "    INVENTIONS = \"Inventions\"\n",
    "    NATURE = \"Nature\"\n",
    "    PLACES = \"Places\"\n",
    "    SONGS = \"Songs\"\n",
    "    SPORTS = \"Sports\"\n",
    "    TECHNOLOGY = \"Technology\"\n",
    "    VIDEO_GAMES = \"Video Games\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ChsBIcQsBhDM",
    "outputId": "67436162-39dd-4b17-f5d5-682e3eadaf96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 1: GENERATING AND VALIDATING ALL CLUES\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Running game 1/10: Broadcast Media - game: #1\n",
      "================================================================================\n",
      "    üìå Found code block, extracted 3568 chars\n",
      "    ‚úì Code block parsed as list\n",
      "\n",
      "üìã Validating clues...\n",
      "\n",
      "Validating Round 1...\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #2: 13 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #3: 14 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #4: 14 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #5: 14 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #6: 13 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #7: 13 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #8: 12 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #9: 12 words\n",
      "  ‚ö†Ô∏è Round 1, misinformed_clues #2: 14 words\n",
      "  ‚ö†Ô∏è Round 1, fake_clues #2: 14 words\n",
      "  ‚ö†Ô∏è Round 1, extra_clues #1: 14 words\n",
      "\n",
      "Validating Round 2...\n",
      "  ‚ö†Ô∏è Round 2, informed_clues #3: 14 words\n",
      "  ‚ö†Ô∏è Round 2, informed_clues #4: 11 words\n",
      "  ‚ö†Ô∏è Round 2, informed_clues #5: 14 words\n",
      "  ‚ö†Ô∏è Round 2, informed_clues #8: 14 words\n",
      "  ‚ö†Ô∏è Round 2, informed_clues #9: 14 words\n",
      "  ‚ö†Ô∏è Round 2, extra_clues #1: 14 words\n",
      "\n",
      "üìä Validation Summary for Test 1:\n",
      "  Total clues: 28\n",
      "  Compliant clues: 11\n",
      "  Invalid clues found: 17\n",
      "  Compliance rate: 39.3%\n",
      "\n",
      "‚úÖ Game 1 validated successfully\n",
      "    üìå Found code block, extracted 3568 chars\n",
      "    ‚úì Code block parsed as list\n",
      "\n",
      "üìã Validating clues...\n",
      "\n",
      "Validating Round 1...\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #2: 13 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #3: 14 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #4: 14 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #5: 14 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #6: 13 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #7: 13 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #8: 12 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #9: 12 words\n",
      "  ‚ö†Ô∏è Round 1, misinformed_clues #2: 14 words\n",
      "  ‚ö†Ô∏è Round 1, fake_clues #2: 14 words\n",
      "  ‚ö†Ô∏è Round 1, extra_clues #1: 14 words\n",
      "\n",
      "Validating Round 2...\n",
      "  ‚ö†Ô∏è Round 2, informed_clues #3: 14 words\n",
      "  ‚ö†Ô∏è Round 2, informed_clues #4: 11 words\n",
      "  ‚ö†Ô∏è Round 2, informed_clues #5: 14 words\n",
      "  ‚ö†Ô∏è Round 2, informed_clues #8: 14 words\n",
      "  ‚ö†Ô∏è Round 2, informed_clues #9: 14 words\n",
      "  ‚ö†Ô∏è Round 2, extra_clues #1: 14 words\n",
      "\n",
      "üìä Validation Summary for Test 1:\n",
      "  Total clues: 28\n",
      "  Compliant clues: 11\n",
      "  Invalid clues found: 17\n",
      "  Compliance rate: 39.3%\n",
      "\n",
      "‚úÖ Game 1 validated successfully\n",
      "\n",
      "================================================================================\n",
      "Running game 2/10: Broadcast Media - game: #2\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Running game 2/10: Broadcast Media - game: #2\n",
      "================================================================================\n",
      "    üìå Found code block, extracted 3737 chars\n",
      "    ‚úì Code block parsed as list\n",
      "\n",
      "üìã Validating clues...\n",
      "\n",
      "Validating Round 1...\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #4: 13 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #7: 14 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #8: 13 words\n",
      "  ‚ö†Ô∏è Round 1, extra_clues #1: 14 words\n",
      "\n",
      "Validating Round 2...\n",
      "\n",
      "üìä Validation Summary for Test 2:\n",
      "  Total clues: 28\n",
      "  Compliant clues: 24\n",
      "  Invalid clues found: 4\n",
      "  Compliance rate: 85.7%\n",
      "\n",
      "‚úÖ Game 2 validated successfully\n",
      "    üìå Found code block, extracted 3737 chars\n",
      "    ‚úì Code block parsed as list\n",
      "\n",
      "üìã Validating clues...\n",
      "\n",
      "Validating Round 1...\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #4: 13 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #7: 14 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #8: 13 words\n",
      "  ‚ö†Ô∏è Round 1, extra_clues #1: 14 words\n",
      "\n",
      "Validating Round 2...\n",
      "\n",
      "üìä Validation Summary for Test 2:\n",
      "  Total clues: 28\n",
      "  Compliant clues: 24\n",
      "  Invalid clues found: 4\n",
      "  Compliance rate: 85.7%\n",
      "\n",
      "‚úÖ Game 2 validated successfully\n",
      "\n",
      "================================================================================\n",
      "Running game 3/10: Broadcast Media - game: #3\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Running game 3/10: Broadcast Media - game: #3\n",
      "================================================================================\n",
      "    üìå Found code block, extracted 3930 chars\n",
      "    ‚úì Code block parsed as list\n",
      "\n",
      "üìã Validating clues...\n",
      "\n",
      "Validating Round 1...\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #1: 14 words\n",
      "\n",
      "Validating Round 2...\n",
      "\n",
      "üìä Validation Summary for Test 3:\n",
      "  Total clues: 29\n",
      "  Compliant clues: 28\n",
      "  Invalid clues found: 1\n",
      "  Compliance rate: 96.6%\n",
      "\n",
      "‚úÖ Game 3 validated successfully\n",
      "    üìå Found code block, extracted 3930 chars\n",
      "    ‚úì Code block parsed as list\n",
      "\n",
      "üìã Validating clues...\n",
      "\n",
      "Validating Round 1...\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #1: 14 words\n",
      "\n",
      "Validating Round 2...\n",
      "\n",
      "üìä Validation Summary for Test 3:\n",
      "  Total clues: 29\n",
      "  Compliant clues: 28\n",
      "  Invalid clues found: 1\n",
      "  Compliance rate: 96.6%\n",
      "\n",
      "‚úÖ Game 3 validated successfully\n",
      "\n",
      "================================================================================\n",
      "Running game 4/10: Broadcast Media - game: #4\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Running game 4/10: Broadcast Media - game: #4\n",
      "================================================================================\n",
      "    üìå Found code block, extracted 3688 chars\n",
      "    ‚úì Code block parsed as list\n",
      "\n",
      "üìã Validating clues...\n",
      "\n",
      "Validating Round 1...\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #2: 14 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #3: 14 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #5: 14 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #7: 14 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #9: 14 words\n",
      "  ‚ö†Ô∏è Round 1, misinformed_clues #2: 14 words\n",
      "  ‚ö†Ô∏è Round 1, fake_clues #1: 14 words\n",
      "\n",
      "Validating Round 2...\n",
      "  ‚ö†Ô∏è Round 2, informed_clues #6: 12 words\n",
      "  ‚ö†Ô∏è Round 2, informed_clues #7: 14 words\n",
      "  ‚ö†Ô∏è Round 2, misinformed_clues #1: 14 words\n",
      "\n",
      "üìä Validation Summary for Test 4:\n",
      "  Total clues: 28\n",
      "  Compliant clues: 18\n",
      "  Invalid clues found: 10\n",
      "  Compliance rate: 64.3%\n",
      "\n",
      "‚úÖ Game 4 validated successfully\n",
      "    üìå Found code block, extracted 3688 chars\n",
      "    ‚úì Code block parsed as list\n",
      "\n",
      "üìã Validating clues...\n",
      "\n",
      "Validating Round 1...\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #2: 14 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #3: 14 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #5: 14 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #7: 14 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #9: 14 words\n",
      "  ‚ö†Ô∏è Round 1, misinformed_clues #2: 14 words\n",
      "  ‚ö†Ô∏è Round 1, fake_clues #1: 14 words\n",
      "\n",
      "Validating Round 2...\n",
      "  ‚ö†Ô∏è Round 2, informed_clues #6: 12 words\n",
      "  ‚ö†Ô∏è Round 2, informed_clues #7: 14 words\n",
      "  ‚ö†Ô∏è Round 2, misinformed_clues #1: 14 words\n",
      "\n",
      "üìä Validation Summary for Test 4:\n",
      "  Total clues: 28\n",
      "  Compliant clues: 18\n",
      "  Invalid clues found: 10\n",
      "  Compliance rate: 64.3%\n",
      "\n",
      "‚úÖ Game 4 validated successfully\n",
      "\n",
      "================================================================================\n",
      "Running game 5/10: Broadcast Media - game: #5\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Running game 5/10: Broadcast Media - game: #5\n",
      "================================================================================\n",
      "    üìå Found code block, extracted 3794 chars\n",
      "    ‚úì Code block parsed as list\n",
      "\n",
      "üìã Validating clues...\n",
      "\n",
      "Validating Round 1...\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #2: 13 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #9: 14 words\n",
      "  ‚ö†Ô∏è Round 1, misinformed_clues #2: 14 words\n",
      "  ‚ö†Ô∏è Round 1, fake_clues #1: 14 words\n",
      "\n",
      "Validating Round 2...\n",
      "  ‚ö†Ô∏è Round 2, misinformed_clues #2: 14 words\n",
      "\n",
      "üìä Validation Summary for Test 5:\n",
      "  Total clues: 29\n",
      "  Compliant clues: 24\n",
      "  Invalid clues found: 5\n",
      "  Compliance rate: 82.8%\n",
      "\n",
      "‚úÖ Game 5 validated successfully\n",
      "    üìå Found code block, extracted 3794 chars\n",
      "    ‚úì Code block parsed as list\n",
      "\n",
      "üìã Validating clues...\n",
      "\n",
      "Validating Round 1...\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #2: 13 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #9: 14 words\n",
      "  ‚ö†Ô∏è Round 1, misinformed_clues #2: 14 words\n",
      "  ‚ö†Ô∏è Round 1, fake_clues #1: 14 words\n",
      "\n",
      "Validating Round 2...\n",
      "  ‚ö†Ô∏è Round 2, misinformed_clues #2: 14 words\n",
      "\n",
      "üìä Validation Summary for Test 5:\n",
      "  Total clues: 29\n",
      "  Compliant clues: 24\n",
      "  Invalid clues found: 5\n",
      "  Compliance rate: 82.8%\n",
      "\n",
      "‚úÖ Game 5 validated successfully\n",
      "\n",
      "================================================================================\n",
      "Running game 6/10: Broadcast Media - game: #6\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Running game 6/10: Broadcast Media - game: #6\n",
      "================================================================================\n",
      "    üìå Found code block, extracted 4197 chars\n",
      "    ‚úì Code block parsed as list\n",
      "\n",
      "üìã Validating clues...\n",
      "\n",
      "Validating Round 1...\n",
      "  ‚ö†Ô∏è Round 1, fake_clues #1: 14 words\n",
      "\n",
      "Validating Round 2...\n",
      "\n",
      "üìä Validation Summary for Test 6:\n",
      "  Total clues: 30\n",
      "  Compliant clues: 29\n",
      "  Invalid clues found: 1\n",
      "  Compliance rate: 96.7%\n",
      "\n",
      "‚úÖ Game 6 validated successfully\n",
      "    üìå Found code block, extracted 4197 chars\n",
      "    ‚úì Code block parsed as list\n",
      "\n",
      "üìã Validating clues...\n",
      "\n",
      "Validating Round 1...\n",
      "  ‚ö†Ô∏è Round 1, fake_clues #1: 14 words\n",
      "\n",
      "Validating Round 2...\n",
      "\n",
      "üìä Validation Summary for Test 6:\n",
      "  Total clues: 30\n",
      "  Compliant clues: 29\n",
      "  Invalid clues found: 1\n",
      "  Compliance rate: 96.7%\n",
      "\n",
      "‚úÖ Game 6 validated successfully\n",
      "\n",
      "================================================================================\n",
      "Running game 7/10: Broadcast Media - game: #7\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Running game 7/10: Broadcast Media - game: #7\n",
      "================================================================================\n",
      "    üìå Found code block, extracted 3744 chars\n",
      "    ‚úì Code block parsed as list\n",
      "\n",
      "üìã Validating clues...\n",
      "\n",
      "Validating Round 1...\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #2: 13 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #5: 14 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #6: 14 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #8: 13 words\n",
      "\n",
      "Validating Round 2...\n",
      "  ‚ö†Ô∏è Round 2, informed_clues #2: 14 words\n",
      "  ‚ö†Ô∏è Round 2, informed_clues #4: 14 words\n",
      "  ‚ö†Ô∏è Round 2, informed_clues #5: 13 words\n",
      "  ‚ö†Ô∏è Round 2, informed_clues #6: 14 words\n",
      "  ‚ö†Ô∏è Round 2, extra_clues #1: 14 words\n",
      "\n",
      "üìä Validation Summary for Test 7:\n",
      "  Total clues: 28\n",
      "  Compliant clues: 19\n",
      "  Invalid clues found: 9\n",
      "  Compliance rate: 67.9%\n",
      "\n",
      "‚úÖ Game 7 validated successfully\n",
      "    üìå Found code block, extracted 3744 chars\n",
      "    ‚úì Code block parsed as list\n",
      "\n",
      "üìã Validating clues...\n",
      "\n",
      "Validating Round 1...\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #2: 13 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #5: 14 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #6: 14 words\n",
      "  ‚ö†Ô∏è Round 1, informed_clues #8: 13 words\n",
      "\n",
      "Validating Round 2...\n",
      "  ‚ö†Ô∏è Round 2, informed_clues #2: 14 words\n",
      "  ‚ö†Ô∏è Round 2, informed_clues #4: 14 words\n",
      "  ‚ö†Ô∏è Round 2, informed_clues #5: 13 words\n",
      "  ‚ö†Ô∏è Round 2, informed_clues #6: 14 words\n",
      "  ‚ö†Ô∏è Round 2, extra_clues #1: 14 words\n",
      "\n",
      "üìä Validation Summary for Test 7:\n",
      "  Total clues: 28\n",
      "  Compliant clues: 19\n",
      "  Invalid clues found: 9\n",
      "  Compliance rate: 67.9%\n",
      "\n",
      "‚úÖ Game 7 validated successfully\n",
      "\n",
      "================================================================================\n",
      "Running game 8/10: Broadcast Media - game: #8\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Running game 8/10: Broadcast Media - game: #8\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[140], line 41\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     37\u001b[0m     SystemMessage(system_prompt \u001b[38;5;241m+\u001b[39m output_format \u001b[38;5;241m+\u001b[39m one_shot_example),\n\u001b[0;32m     38\u001b[0m     HumanMessage(json\u001b[38;5;241m.\u001b[39mdumps(game)),\n\u001b[0;32m     39\u001b[0m ]\n\u001b[1;32m---> 41\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgeneration_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m clean_content \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<think>.*?</think>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, response\u001b[38;5;241m.\u001b[39mcontent, flags\u001b[38;5;241m=\u001b[39mre\u001b[38;5;241m.\u001b[39mDOTALL)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     43\u001b[0m game_data \u001b[38;5;241m=\u001b[39m extract_json_from_response(clean_content)\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:1962\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI.invoke\u001b[1;34m(self, input, config, code_execution, stop, **kwargs)\u001b[0m\n\u001b[0;32m   1959\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTools are already defined.code_execution tool can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be defined\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1960\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m-> 1962\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:385\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    379\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AIMessage:\n\u001b[0;32m    380\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    382\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAIMessage\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    383\u001b[0m         cast(\n\u001b[0;32m    384\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 385\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    395\u001b[0m         )\u001b[38;5;241m.\u001b[39mmessage,\n\u001b[0;32m    396\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1104\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1102\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m   1103\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m-> 1104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:914\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[0;32m    912\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    913\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 914\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    920\u001b[0m         )\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    922\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1208\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[0;32m   1207\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1212\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:2099\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI._generate\u001b[1;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_retries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m   2098\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_retries\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries\n\u001b[1;32m-> 2099\u001b[0m response: GenerateContentResponse \u001b[38;5;241m=\u001b[39m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2101\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2104\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:232\u001b[0m, in \u001b[0;36m_chat_with_retry\u001b[1;34m(generation_method, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    225\u001b[0m params \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    226\u001b[0m     {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _allowed_params_prediction_service}\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (request \u001b[38;5;241m:=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m kwargs\n\u001b[0;32m    231\u001b[0m )\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:338\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    336\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    337\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 477\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    376\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[1;32m--> 378\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:400\u001b[0m, in \u001b[0;36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[1;34m(rs)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryCallState\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mis_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mretry_run_result):\n\u001b[1;32m--> 400\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutcome\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 480\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    482\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:202\u001b[0m, in \u001b[0;36m_chat_with_retry.<locals>._chat_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 202\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m FailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    204\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocation is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mmessage:\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:869\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m    868\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 869\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    291\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    293\u001b[0m )\n\u001b[1;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    149\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout\n\u001b[0;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:75\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(callable_)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror_remapped_callable\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\grpc\\_interceptor.py:276\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    269\u001b[0m     request: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    274\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 276\u001b[0m     response, ignored_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\grpc\\_interceptor.py:328\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    326\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _FailureOutcome(exception, sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m--> 328\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interceptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintercept_unary_unary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontinuation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call\u001b[38;5;241m.\u001b[39mresult(), call\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\transports\\grpc.py:78\u001b[0m, in \u001b[0;36m_LoggingClientInterceptor.intercept_unary_unary\u001b[1;34m(self, continuation, client_call_details, request)\u001b[0m\n\u001b[0;32m     64\u001b[0m     grpc_request \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpayload\u001b[39m\u001b[38;5;124m\"\u001b[39m: request_payload,\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequestMethod\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrpc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(request_metadata),\n\u001b[0;32m     68\u001b[0m     }\n\u001b[0;32m     69\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_call_details\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     71\u001b[0m         extra\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m         },\n\u001b[0;32m     77\u001b[0m     )\n\u001b[1;32m---> 78\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcontinuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_enabled:  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     response_metadata \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtrailing_metadata()\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\grpc\\_interceptor.py:314\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[1;34m(new_details, request)\u001b[0m\n\u001b[0;32m    305\u001b[0m (\n\u001b[0;32m    306\u001b[0m     new_method,\n\u001b[0;32m    307\u001b[0m     new_timeout,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    311\u001b[0m     new_compression,\n\u001b[0;32m    312\u001b[0m ) \u001b[38;5;241m=\u001b[39m _unwrap_client_call_details(new_details, client_call_details)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 314\u001b[0m     response, call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\grpc\\_channel.py:1177\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.with_call\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwith_call\u001b[39m(\n\u001b[0;32m   1169\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1170\u001b[0m     request: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1175\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1176\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, grpc\u001b[38;5;241m.\u001b[39mCall]:\n\u001b[1;32m-> 1177\u001b[0m     state, call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[0;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\grpc\\_channel.py:1150\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._blocking\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1133\u001b[0m state\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target)\n\u001b[0;32m   1134\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msegregated_call(\n\u001b[0;32m   1135\u001b[0m     cygrpc\u001b[38;5;241m.\u001b[39mPropagationConstants\u001b[38;5;241m.\u001b[39mGRPC_PROPAGATE_DEFAULTS,\n\u001b[0;32m   1136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registered_call_handle,\n\u001b[0;32m   1149\u001b[0m )\n\u001b[1;32m-> 1150\u001b[0m event \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1151\u001b[0m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_deserializer)\n\u001b[0;32m   1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:388\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:97\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:80\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._internal_latent_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main execution with GLOBAL batch validation and fixing\n",
    "all_rows = []\n",
    "validation_summary = []\n",
    "all_game_data = []  # Store all game data for global batch fixing\n",
    "all_invalid_clues = []  # Collect ALL invalid clues across all tests\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 1: GENERATING AND VALIDATING ALL CLUES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Phase 1: Generate all clues and collect invalid ones\n",
    "# for each game for each topic\n",
    "\n",
    "# Pick a topic to generate\n",
    "game_topics = [game for game in game_topics if game[\"category\"] == GameTopic.BOOKS.value]\n",
    "\n",
    "# Extract topic name from first game (all games in this batch should have the same topic)\n",
    "topic_name = None\n",
    "if game_topics:\n",
    "    topic_name = game_topics[0]['category'].replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "\n",
    "for run_number, game in enumerate(game_topics, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Running game {run_number}/{len(game_topics)}: {game['category']} - game: #{game['game_number']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(system_prompt + output_format + one_shot_example),\n",
    "        HumanMessage(json.dumps(game)),\n",
    "    ]\n",
    "\n",
    "    response = generation_model.invoke(messages)\n",
    "    clean_content = re.sub(r\"<think>.*?</think>\", \"\", response.content, flags=re.DOTALL).strip()\n",
    "    game_data = extract_json_from_response(clean_content)\n",
    "\n",
    "    if game_data:\n",
    "        try:\n",
    "            # Validate clues WITHOUT fixing them yet\n",
    "            print(f\"\\nüìã Validating clues...\")\n",
    "            validated_game_data, validation_report, invalid_clues = validate_game_data(\n",
    "                game_data, \n",
    "                auto_fix=True,  # Set to True to collect invalid clues\n",
    "                test_run=run_number\n",
    "            )\n",
    "            \n",
    "            # Print validation summary\n",
    "            print(f\"\\nüìä Validation Summary for Test {run_number}:\")\n",
    "            print(f\"  Total clues: {validation_report['total_clues']}\")\n",
    "            print(f\"  Compliant clues: {validation_report['compliant_clues']}\")\n",
    "            print(f\"  Invalid clues found: {len(invalid_clues)}\")\n",
    "            print(f\"  Compliance rate: {validation_report['compliance_rate']}\")\n",
    "            \n",
    "            # Store for later processing\n",
    "            all_game_data.append({\n",
    "                'run_number': run_number,\n",
    "                'game': game,\n",
    "                'game_data': validated_game_data,\n",
    "                'validation_report': validation_report\n",
    "            })\n",
    "            \n",
    "            # Collect invalid clues with test context\n",
    "            all_invalid_clues.extend(invalid_clues)\n",
    "            \n",
    "            print(f\"\\n‚úÖ Game {run_number} validated successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing data for game {run_number}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        print(f\"‚ùå No valid JSON found for game {run_number}\")\n",
    "        print(\"RAW:\", clean_content[:200])\n",
    "\n",
    "    sleep(5)  # Rate limiting between generations\n",
    "\n",
    "# Phase 2: Batch fix ALL invalid clues in ONE API call\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"PHASE 2: BATCH FIXING ALL INVALID CLUES ({len(all_invalid_clues)} total)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if all_invalid_clues:\n",
    "    print(f\"\\nüîß Found {len(all_invalid_clues)} invalid clues across {len(game_topics)} tests\")\n",
    "    print(f\"üìû Making ONE batch API call to fix all clues...\")\n",
    "    \n",
    "    rewritten_clues = batch_rewrite_clues_with_llm(all_invalid_clues, fixing_model, max_retries=5)\n",
    "    \n",
    "    # Phase 3: Apply the rewritten clues back to their respective game data\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHASE 3: APPLYING FIXES AND GENERATING OUTPUT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, item in enumerate(all_invalid_clues):\n",
    "        rewritten_clue = rewritten_clues[i]\n",
    "        is_fixed, new_word_count = validate_clue_word_count(rewritten_clue)\n",
    "        \n",
    "        # Find the corresponding game data\n",
    "        test_run = item['test_run']\n",
    "        game_data_entry = next((gd for gd in all_game_data if gd['run_number'] == test_run), None)\n",
    "        \n",
    "        if game_data_entry and is_fixed:\n",
    "            # Apply the fix\n",
    "            game_data_entry['game_data'][item['round_idx']][item['clue_type_key']][item['clue_idx']] = rewritten_clue\n",
    "            game_data_entry['validation_report']['fixed_clues'] = game_data_entry['validation_report'].get('fixed_clues', 0) + 1\n",
    "            game_data_entry['validation_report']['compliant_clues'] += 1\n",
    "        elif game_data_entry:\n",
    "            game_data_entry['validation_report']['failed_fixes'] = game_data_entry['validation_report'].get('failed_fixes', 0) + 1\n",
    "            print(f\"  ‚ùå Failed to fix Test {test_run}, Round {item['round_idx']+1}, {item['clue_type_key']} #{item['clue_idx']+1}: Still {new_word_count} words\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ No invalid clues found - all clues met the 15-20 word requirement!\")\n",
    "\n",
    "# Phase 4: Process all game data and update validation summaries\n",
    "for game_data_entry in all_game_data:\n",
    "    run_number = game_data_entry['run_number']\n",
    "    game = game_data_entry['game']\n",
    "    topic = game[\"category\"]\n",
    "    game_number = game[\"game_number\"]\n",
    "    corrected_game_data = game_data_entry['game_data']\n",
    "    validation_report = game_data_entry['validation_report']\n",
    "    \n",
    "    # Recalculate compliance rate after fixes\n",
    "    if validation_report[\"total_clues\"] > 0:\n",
    "        compliance_rate = (validation_report[\"compliant_clues\"] / validation_report[\"total_clues\"]) * 100\n",
    "        validation_report[\"compliance_rate\"] = f\"{compliance_rate:.1f}%\"\n",
    "    \n",
    "    # Store validation summary\n",
    "    validation_summary.append({\n",
    "        \"test_run\": run_number,\n",
    "        \"topic\": f\"{game['category']} - {game_number}\",\n",
    "        **validation_report\n",
    "    })\n",
    "    \n",
    "    # Process the corrected game data\n",
    "    all_rows.extend(process_game_data(corrected_game_data, game, run_number))\n",
    "\n",
    "# Print overall validation summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"OVERALL VALIDATION SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "for summary in validation_summary:\n",
    "    print(f\"\\nTest {summary['test_run']}: {summary['topic']}\")\n",
    "    print(f\"  Compliance rate: {summary['compliance_rate']}\")\n",
    "    print(f\"  Fixed: {summary.get('fixed_clues', 0)}/{summary['total_clues']} clues\")\n",
    "\n",
    "print(f\"\\nüìÅ Topic: {topic_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D6HdR9_0Ht1y",
    "outputId": "d5285434-a3c2-4fab-9c88-672d0a3d7c29"
   },
   "outputs": [],
   "source": [
    "# Create topic folder and save to CSV with topic-specific naming\n",
    "from pathlib import Path\n",
    "\n",
    "# Create topic-based folder structure\n",
    "if topic_name:\n",
    "    topic_folder = Path(topic_name)\n",
    "    topic_folder.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save clues analysis\n",
    "    clues_csv_path = topic_folder / f\"10_games_clues_content({topic_name}).csv\"\n",
    "    with open(clues_csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        if all_rows:\n",
    "            writer = csv.DictWriter(f, fieldnames=all_rows[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(all_rows)\n",
    "            print(f\"‚úÖ CSV saved: {clues_csv_path}\")\n",
    "\n",
    "    # Save validation summary\n",
    "    if validation_summary:\n",
    "        # Flatten the issues list for CSV\n",
    "        for summary in validation_summary:\n",
    "            summary['issues'] = '; '.join(summary.get('issues', []))\n",
    "        \n",
    "        validation_csv_path = topic_folder / f\"10_games_length_validation({topic_name}).csv\"\n",
    "        with open(validation_csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=validation_summary[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(validation_summary)\n",
    "            print(f\"‚úÖ Validation summary saved: {validation_csv_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Warning: Could not determine topic name. Saving with default names.\")\n",
    "    with open(\"10_games_clues_analysis.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        if all_rows:\n",
    "            writer = csv.DictWriter(f, fieldnames=all_rows[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(all_rows)\n",
    "            print(f\"‚úÖ CSV saved: 10_games_clues_analysis.csv\")\n",
    "\n",
    "    if validation_summary:\n",
    "        for summary in validation_summary:\n",
    "            summary['issues'] = '; '.join(summary.get('issues', []))\n",
    "        \n",
    "        with open(\"10_games_length_validation.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=validation_summary[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(validation_summary)\n",
    "            print(f\"‚úÖ Validation summary saved: 10_games_length_validation.csv\")\n",
    "\n",
    "print(f\"\\nTotal rows generated: {len(all_rows)}\")\n",
    "print(f\"Total tests validated: {len(validation_summary)}\")\n",
    "\n",
    "# Automatically accumulate topic CSVs into master CSV\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP: ACCUMULATING CLUES INTO MASTER CSV\")\n",
    "print(\"=\"*80)\n",
    "print(\"üìù Note: The accumulate_topic_clues_to_master_csv() function will run in the next cell\")\n",
    "print(\"   This allows you to review the generation results before accumulation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze validation results\n",
    "if validation_summary:\n",
    "    import pandas as pd\n",
    "    \n",
    "    df_validation = pd.DataFrame(validation_summary)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VALIDATION STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    total_clues = df_validation['total_clues'].sum()\n",
    "    total_compliant = df_validation['compliant_clues'].sum()\n",
    "    total_fixed = df_validation['fixed_clues'].sum()\n",
    "    total_failed = df_validation['failed_fixes'].sum()\n",
    "    overall_compliance = (total_compliant / total_clues * 100) if total_clues > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüìä Overall Statistics Across All Tests:\")\n",
    "    print(f\"  Total clues generated: {total_clues}\")\n",
    "    print(f\"  Initially compliant: {total_compliant - total_fixed} ({(total_compliant - total_fixed) / total_clues * 100:.1f}%)\")\n",
    "    print(f\"  Successfully fixed: {total_fixed}\")\n",
    "    print(f\"  Failed to fix: {total_failed}\")\n",
    "    print(f\"  Final compliance rate: {overall_compliance:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nüìà Per-Test Breakdown:\")\n",
    "    for _, row in df_validation.iterrows():\n",
    "        test_num = row['test_run']\n",
    "        topic = row['topic']\n",
    "        compliance = row['compliance_rate']\n",
    "        fixed = row['fixed_clues']\n",
    "        print(f\"  Test {test_num}: {compliance} compliance ({fixed} clues fixed) - {topic}\")\n",
    "    \n",
    "    # Identify best and worst performers\n",
    "    df_validation['compliance_numeric'] = df_validation['compliance_rate'].str.rstrip('%').astype(float)\n",
    "    best_test = df_validation.loc[df_validation['compliance_numeric'].idxmax()]\n",
    "    worst_test = df_validation.loc[df_validation['compliance_numeric'].idxmin()]\n",
    "    \n",
    "    print(f\"\\nüèÜ Best performing test: Test {int(best_test['test_run'])} ({best_test['compliance_rate']} compliance)\")\n",
    "    print(f\"   Topic: {best_test['topic']}\")\n",
    "    print(f\"\\n‚ö†Ô∏è Lowest performing test: Test {int(worst_test['test_run'])} ({worst_test['compliance_rate']} compliance)\")\n",
    "    print(f\"   Topic: {worst_test['topic']}\")\n",
    "    \n",
    "    if total_failed > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è WARNING: {total_failed} clues could not be fixed after retries\")\n",
    "        print(\"   Consider manual review or increasing max_retries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_topic_clues_to_master_csv():\n",
    "    \"\"\"\n",
    "    Scan for all 10_games_clues_content({topic_name}).csv files across topic folders\n",
    "    and accumulate them into a single disinformer_full_games_clues.csv file in the root.\n",
    "    \n",
    "    This function:\n",
    "    1. Finds all topic-specific CSV files created during generation\n",
    "    2. Loads each CSV file\n",
    "    3. Concatenates them into a single master DataFrame\n",
    "    4. Saves the consolidated data to disinformer_full_games_clues.csv\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if accumulation was successful, False otherwise\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ACCUMULATING ALL TOPIC CLUES INTO MASTER CSV\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    root_dir = Path(\".\")\n",
    "    \n",
    "    # Find all CSV files matching the pattern 10_games_clues_content({topic_name}).csv\n",
    "    csv_files = list(root_dir.glob(\"**/10_games_clues_content(*.csv\"))\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"‚ö†Ô∏è No topic-specific CSV files found. Pattern: 10_games_clues_content({topic_name}).csv\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"\\nüîç Found {len(csv_files)} topic-specific CSV file(s):\")\n",
    "    for csv_file in csv_files:\n",
    "        print(f\"  - {csv_file}\")\n",
    "    \n",
    "    # Load and concatenate all CSV files\n",
    "    all_dfs = []\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file, encoding='utf-8')\n",
    "            topic_name = csv_file.parent.name  # Get folder name as topic\n",
    "            print(f\"  ‚úÖ Loaded {len(df)} rows from {csv_file.name}\")\n",
    "            all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error loading {csv_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_dfs:\n",
    "        print(\"‚ùå No CSV files were successfully loaded\")\n",
    "        return False\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    master_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    \n",
    "    # Save to master CSV in root directory\n",
    "    master_csv_path = Path(\"disinformer_full_games_clues.csv\")\n",
    "    master_df.to_csv(master_csv_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"\\n‚úÖ Master CSV created successfully!\")\n",
    "    print(f\"  üìÅ File: {master_csv_path}\")\n",
    "    print(f\"  üìä Total rows: {len(master_df)}\")\n",
    "    print(f\"  üìã Columns: {', '.join(master_df.columns.tolist())}\")\n",
    "    print(f\"  üìà Topics accumulated: {len(all_dfs)}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Run accumulation\n",
    "print(\"üöÄ Running accumulation...\\n\")\n",
    "success = accumulate_topic_clues_to_master_csv()\n",
    "if success:\n",
    "    print(\"\\n‚ú® Accumulation complete! Check disinformer_full_games_clues.csv in the root directory.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Accumulation did not complete successfully. Check for errors above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1A0fc3WdZymW"
   },
   "source": [
    "## LLM analysis (llama)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OurpVJ2h9eD"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "def analyze_round_with_llm(round_data, analysis_model):\n",
    "    \"\"\"Analyze clues with detailed rubric but token-optimized\"\"\"\n",
    "\n",
    "    word_counts = {}\n",
    "    length_issues = []\n",
    "\n",
    "    for clue_type in [\"informed_clues\", \"misinformed_clues\", \"fake_clues\", \"extra_clues\"]:\n",
    "        clues = round_data.get(clue_type, [])\n",
    "        word_counts[clue_type] = [len(c.split()) for c in clues]\n",
    "        \n",
    "        for i, wc in enumerate(word_counts[clue_type], 1):\n",
    "            if not (15 <= wc <= 20):\n",
    "                length_issues.append(f\"{clue_type} #{i}: {wc}w\")\n",
    "\n",
    "    # RESTORED: Detailed rubric with token optimization\n",
    "    analysis_prompt = f\"\"\"Analyze clues for disinformer game. Return ONLY valid JSON.\n",
    "\n",
    "    ANSWER: {round_data.get('answer', 'N/A')}\n",
    "    CHOICES: {', '.join(round_data.get('choices', []))}\n",
    "\n",
    "    INFORMED (all): {json.dumps(round_data.get('informed_clues', []))}\n",
    "    - CRITERIA: Precise clues using only descriptive paraphrases (e.g., 'moving image' not 'film' for 'movies'), forbidding all answer words, variations, or synonyms.\n",
    "    - SCORE 1-5: How well do they point to correct answer specifically?\n",
    "\n",
    "    MISINFORMED (all): {json.dumps(round_data.get('misinformed_clues', []))}\n",
    "    - CRITERIA: Vague and ambiguous clues. True but overly general clues that could plausibly describe the correct answer and one or more of the incorrect answers.\n",
    "        - CRITICAL REQUIREMENTS (MANDATORY FOR EACH CLUE):\n",
    "            - Be 100% factually true for the correct answer.\n",
    "            - Also be 100% factually true for ALL of the incorrect answers. This is a non-negotiable rule to ensure the clue is maximally ambiguous and offers no information to eliminate any option.\n",
    "            - Avoid clues that are only true for an incorrect answer (a \"Fake\" clue) or only true for the correct answer (an \"Informed\" clue). The goal is a total shared intersection of truth.\n",
    "            - AVOID SPECIFICITY: Do not use defining keywords or tropes. Focus only on the most general, shared truths (e.g., \"involves suspense\" or \"features characters\").\n",
    "\n",
    "        - MANDATORY 3-STEP SELF-VALIDATION (PERFORM BEFORE WRITING EACH CLUE):\n",
    "            - Step 1: Find the Universal Intersection. Analyze the Correct Answer and ALL Incorrect Answers. You MUST find a general, factual attribute that all options share.\n",
    "            - Step 2: Justify the Intersection. Mentally confirm: \"Is this attribute 100% true for the correct answer? Yes. Is it also 100% true for every single one of the incorrect answers? Yes.\"\n",
    "            - Step 3: Write the Clue. Write a vague clue based only on that universally shared attribute.\n",
    "\n",
    "        - ANTI-PATTERN WARNING (AVOID \"POOR ADAPTATION\"):\n",
    "            - DO NOT reuse clues from other rounds. Each set of answers is unique.\n",
    "            - You MUST re-validate the clue against the current set of all incorrect answers every single time, as per the 3-step validation process.\n",
    "\n",
    "        - Examples:\n",
    "            - Correct misinformed clue (Movie category, answer: \"Sci-Fi\", choices: \"Sci-Fi, Horror, Fantasy\"): \"This genre often features suspense, thrills, and elements of the unknown, keeping audiences on the edge of their seats.\" (True for Sci-Fi AND Horror/Fantasy.)\n",
    "            - Incorrect misinformed clue (would be \"Informed\") (Historical Event category, answer: \"Moon Landing\", choices: \"Moon Landing, Apollo 13, First Flight\"): \"This historic event was famously broadcast on live television, with one man taking a 'giant leap'.\" (Only true for Moon Landing, not for Apollo 13 or First Flight.)\n",
    "            - Incorrect misinformed clue (would be \"Fake\") (Historical Event category, answer: \"Moon Landing\", choices: \"Moon Landing, Apollo 13, First Flight\"): \"The story centers on a daring rescue mission with a team facing perilous conditions and impossible odds.\" (Only true for Apollo 13, not for Moon Landing or First Flight.)\n",
    "    \n",
    "    - SCORE 1-5: Do they create productive ambiguity (not too obvious, not too vague)?\n",
    "\n",
    "    FAKE (all): {json.dumps(round_data.get('fake_clues', []))}\n",
    "    - CRITERIA: Deceptive clues, completely unrelated to the correct answer, designed to strongly and plausibly describe one of the incorrect answer choices.\n",
    "    - SCORE 1-5: Effective misdirection to wrong choices (not to correct answer)?\n",
    "\n",
    "    LENGTH ISSUES: {'; '.join(length_issues) if length_issues else 'None'}\n",
    "\n",
    "    {{\n",
    "    \"length_compliance_score\": (1-5),\n",
    "    \"length_issues_found\": [],\n",
    "    \"informed_quality\": (1-5),\n",
    "    \"informed_notes\": \"Specificity? Answer contamination? Distinct angles?\",\n",
    "    \"misinformed_quality\": (1-5),\n",
    "    \"misinformed_notes\": \"Ambiguity effective? Related to answer? Productive confusion?\",\n",
    "    \"fake_quality\": (1-5),\n",
    "    \"fake_notes\": \"Point to WRONG choices? Avoid correct answer? Believable?\",\n",
    "    \"diversity_issues\": [],\n",
    "    \"difficulty\": (1-5),\n",
    "    \"difficulty_reasoning\": \"1=too easy, 3=just right, 5=too hard\",\n",
    "    \"overall_notes\": \"Summary\"\n",
    "    }}\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = analysis_model.invoke([HumanMessage(analysis_prompt)])\n",
    "        \n",
    "        # DEBUG: Log raw response length\n",
    "        print(f\"    üìä Response length: {len(response.content)} chars\")\n",
    "        \n",
    "        # Extract JSON with debug output\n",
    "        parsed = extract_json_from_response(response.content)\n",
    "        \n",
    "        if parsed is None:\n",
    "            print(f\"    ‚ùå JSON extraction failed\")\n",
    "            print(f\"    üìã First 500 chars: {response.content[:500]}\")\n",
    "            \n",
    "            # FALLBACK: Return default structure\n",
    "            return {\n",
    "                \"length_compliance_score\": 3,\n",
    "                \"length_issues_found\": [],\n",
    "                \"informed_quality\": 3,\n",
    "                \"informed_notes\": \"Analysis failed - using default scores\",\n",
    "                \"misinformed_quality\": 3,\n",
    "                \"misinformed_notes\": \"Analysis failed - using default scores\",\n",
    "                \"fake_quality\": 3,\n",
    "                \"fake_notes\": \"Analysis failed - using default scores\",\n",
    "                \"diversity_issues\": [],\n",
    "                \"difficulty\": 3,\n",
    "                \"difficulty_reasoning\": \"Analysis failed - using default scores\",\n",
    "                \"overall_notes\": \"LLM analysis could not be completed\"\n",
    "            }\n",
    "        \n",
    "        # extract_json_from_response returns a list, extract first element (dict)\n",
    "        if isinstance(parsed, list) and len(parsed) > 0:\n",
    "            result = parsed[0]\n",
    "        else:\n",
    "            result = parsed\n",
    "        \n",
    "        # Ensure result is a dict\n",
    "        if not isinstance(result, dict):\n",
    "            print(f\"    ‚ö†Ô∏è Parsed result is not a dict: {type(result)}\")\n",
    "            return {\n",
    "                \"length_compliance_score\": 3,\n",
    "                \"length_issues_found\": [],\n",
    "                \"informed_quality\": 3,\n",
    "                \"informed_notes\": \"Analysis failed - unexpected data type\",\n",
    "                \"misinformed_quality\": 3,\n",
    "                \"misinformed_notes\": \"Analysis failed - unexpected data type\",\n",
    "                \"fake_quality\": 3,\n",
    "                \"fake_notes\": \"Analysis failed - unexpected data type\",\n",
    "                \"diversity_issues\": [],\n",
    "                \"difficulty\": 3,\n",
    "                \"difficulty_reasoning\": \"Analysis failed - unexpected data type\",\n",
    "                \"overall_notes\": \"LLM analysis could not be completed\"\n",
    "            }\n",
    "        \n",
    "        print(f\"    ‚úÖ JSON parsed successfully\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ‚ùå LLM invocation error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdYVHNS9lv5r"
   },
   "outputs": [],
   "source": [
    "# Load data from your manual analysis CSV\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file from your manual analysis\n",
    "df = pd.read_csv(\"10_games_clues_analysis.csv\")  # Change filename as needed\n",
    "\n",
    "# Group data by test_run and round to reconstruct round_data\n",
    "all_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ex9epMELiCFt",
    "outputId": "3acb14c5-49db-47e6-e947-a335d1e1b0ac"
   },
   "outputs": [],
   "source": [
    "for (test_run, round_num), group in df.groupby(['test_run', 'round']):\n",
    "    # Skip disinformer instructions\n",
    "    clue_data = group[group['clue_type'] != 'disinformer_instruction']\n",
    "\n",
    "    if len(clue_data) == 0:\n",
    "        continue\n",
    "\n",
    "    # Get basic info\n",
    "    topic_category = clue_data['topic_category'].iloc[0]\n",
    "    answer = clue_data['answer'].iloc[0]\n",
    "    choices = clue_data['choices'].iloc[0]\n",
    "\n",
    "    print(f\"Analyzing Test {test_run}, Round {round_num}: {topic_category} - {answer}\")\n",
    "\n",
    "    # Reconstruct round_data from CSV\n",
    "    round_data = {\n",
    "        \"answer\": answer,\n",
    "        \"choices\": choices.split(\" | \") if choices else [],\n",
    "        \"informed_clues\": clue_data[clue_data['clue_type'] == 'informed']['clue_text'].tolist(),\n",
    "        \"misinformed_clues\": clue_data[clue_data['clue_type'] == 'misinformed']['clue_text'].tolist(),\n",
    "        \"fake_clues\": clue_data[clue_data['clue_type'] == 'fake']['clue_text'].tolist(),\n",
    "        \"extra_clues\": clue_data[clue_data['clue_type'] == 'extra']['clue_text'].tolist()\n",
    "    }\n",
    "\n",
    "    # Validate round_data before analysis\n",
    "    total_clues = sum(len(round_data[ct]) for ct in [\"informed_clues\", \"misinformed_clues\", \"fake_clues\", \"extra_clues\"])\n",
    "    if total_clues < 14:  # Expect 14 per round\n",
    "        print(f\"  ‚ö†Ô∏è Skipping analysis: Insufficient clues ({total_clues}/14)\")\n",
    "        continue\n",
    "\n",
    "    # Analyze with LLM (with retries)\n",
    "    analysis = analyze_round_with_llm(round_data, analysis_model)\n",
    "\n",
    "    if analysis and isinstance(analysis, dict):\n",
    "        try:\n",
    "            result = {\n",
    "                \"test_run\": test_run,\n",
    "                \"topic_category\": topic_category,\n",
    "                \"round\": round_num,\n",
    "                \"answer\": answer,\n",
    "                \"choices\": choices,\n",
    "\n",
    "                # LLM Analysis Results\n",
    "                \"informed_quality\": analysis.get(\"informed_quality\", \"\"),\n",
    "                \"informed_notes\": analysis.get(\"informed_notes\", \"\"),\n",
    "                \"misinformed_quality\": analysis.get(\"misinformed_quality\", \"\"),\n",
    "                \"misinformed_notes\": analysis.get(\"misinformed_notes\", \"\"),\n",
    "                \"fake_quality\": analysis.get(\"fake_quality\", \"\"),\n",
    "                \"fake_notes\": analysis.get(\"fake_notes\", \"\"),\n",
    "                \"diversity_issues\": \"; \".join(analysis.get(\"diversity_issues\", [])),\n",
    "                \"difficulty\": analysis.get(\"difficulty\", \"\"),\n",
    "                \"difficulty_reasoning\": analysis.get(\"difficulty_reasoning\", \"\"),\n",
    "                \"overall_notes\": analysis.get(\"overall_notes\", \"\"),\n",
    "\n",
    "                # Word count and length compliance data\n",
    "                \"total_clues\": len(round_data[\"informed_clues\"]) + len(round_data[\"misinformed_clues\"]) + len(round_data[\"fake_clues\"]) + len(round_data[\"extra_clues\"]),\n",
    "                \"length_compliant_clues\": sum(1 for clue_type in [\"informed_clues\", \"misinformed_clues\", \"fake_clues\", \"extra_clues\"]\n",
    "                                            for clue in round_data[clue_type]\n",
    "                                            if 15 <= len(clue.split()) <= 20),\n",
    "                \"length_compliance_rate\": f\"{(sum(1 for clue_type in ['informed_clues', 'misinformed_clues', 'fake_clues', 'extra_clues'] for clue in round_data[clue_type] if 15 <= len(clue.split()) <= 20) / max(1, sum(len(round_data[clue_type]) for clue_type in ['informed_clues', 'misinformed_clues', 'fake_clues', 'extra_clues'])) * 100):.0f}%\",\n",
    "                \"avg_word_count\": round(sum(len(clue.split()) for clue_type in [\"informed_clues\", \"misinformed_clues\", \"fake_clues\", \"extra_clues\"] for clue in round_data[clue_type]) / max(1, sum(len(round_data[clue_type]) for clue_type in [\"informed_clues\", \"misinformed_clues\", \"fake_clues\", \"extra_clues\"])), 1)\n",
    "            }\n",
    "\n",
    "            all_results.append(result)\n",
    "            print(f\"  ‚úÖ Analyzed successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error building result dict: {e}\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå Analysis failed or returned invalid data\")\n",
    "\n",
    "    sleep(5)  # Rate limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xayzwc_0iO-L",
    "outputId": "f1735cba-482e-4fec-c580-03c4e264fb10"
   },
   "outputs": [],
   "source": [
    "# Save results to topic-specific folder\n",
    "from pathlib import Path\n",
    "\n",
    "if all_results:\n",
    "    # Create topic folder if needed\n",
    "    if topic_name:\n",
    "        topic_folder = Path(topic_name)\n",
    "        topic_folder.mkdir(exist_ok=True)\n",
    "        \n",
    "        llm_results_path = topic_folder / f\"10_games_llm_analysis_results({topic_name}).csv\"\n",
    "    else:\n",
    "        llm_results_path = Path(\"10_games_llm_analysis_results.csv\")\n",
    "    \n",
    "    with open(llm_results_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=all_results[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_results)\n",
    "\n",
    "    print(f\"‚úÖ LLM analysis complete! Saved {len(all_results)} results to: {llm_results_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "# Used by to_markdown function\n",
    "%pip install tabulate\n",
    "\n",
    "# Ensure the utility functions below exist in your notebook cell.\n",
    "def calculate_length_compliance(row):\n",
    "    compliance_rate = int(row['length_compliance_rate'].rstrip('%'))\n",
    "    total_clues = row['total_clues']\n",
    "    compliant = row['length_compliant_clues']\n",
    "    non_compliant = total_clues - compliant\n",
    "    return compliance_rate, compliant, non_compliant, total_clues\n",
    "\n",
    "def get_pass_fail_status(compliance_rate):\n",
    "    return \"‚úÖ PASS\" if compliance_rate >= 80 else \"‚ùå FAIL\"\n",
    "\n",
    "def get_quality_assessment(score):\n",
    "    assessments = {\n",
    "        1: \"Poor - Needs significant revision\",\n",
    "        2: \"Fair - Below expectations\",\n",
    "        3: \"Good - Meets requirements\",\n",
    "        4: \"Very Good - Exceeds expectations\",\n",
    "        5: \"Excellent - Outstanding\"\n",
    "    }\n",
    "    return assessments.get(int(score), \"Unknown\")\n",
    "\n",
    "def get_difficulty_assessment(difficulty):\n",
    "    difficulty = int(difficulty)\n",
    "    if difficulty <= 2:\n",
    "        return \"üü¢ Too Easy\"\n",
    "    elif difficulty == 3:\n",
    "        return \"üü¢ Just Right\"\n",
    "    else:\n",
    "        return \"üü† Too Hard\"\n",
    "\n",
    "def extract_issues(notes_str):\n",
    "    import pandas as pd\n",
    "    if pd.isna(notes_str):\n",
    "        return [\"None identified\"]\n",
    "    notes_str = str(notes_str).lower()\n",
    "    issues = []\n",
    "    keywords = {\n",
    "        \"length\": \"Word count compliance issues\",\n",
    "        \"generic\": \"Generic/vague clues\",\n",
    "        \"diversity\": \"Lack of diversity in themes\",\n",
    "        \"ambiguity\": \"Insufficient ambiguity in misinformed clues\",\n",
    "        \"specificity\": \"Missing specificity in clues\",\n",
    "        \"answer contamination\": \"Answer word revealed in clues\"\n",
    "    }\n",
    "    for keyword, issue in keywords.items():\n",
    "        if keyword in notes_str:\n",
    "            issues.append(issue)\n",
    "    return issues if issues else [\"Minor issues noted\"]\n",
    "\n",
    "def generate_matrix_for_round(row):\n",
    "    test_run = int(row['test_run'])\n",
    "    topic_cat = row['topic_category']\n",
    "    round_num = int(row['round'])\n",
    "    compliance_rate, compliant, non_compliant, total = calculate_length_compliance(row)\n",
    "    status = get_pass_fail_status(compliance_rate)\n",
    "\n",
    "    # Handle NaN values with defaults\n",
    "    informed_score = int(row['informed_quality']) if not pd.isna(row['informed_quality']) else 3\n",
    "    misinformed_score = int(row['misinformed_quality']) if not pd.isna(row['misinformed_quality']) else 3\n",
    "    fake_score = int(row['fake_quality']) if not pd.isna(row['fake_quality']) else 3\n",
    "    difficulty = int(row['difficulty']) if not pd.isna(row['difficulty']) else 3\n",
    "\n",
    "    issues = extract_issues(row['overall_notes'])\n",
    "    diversity_issues = row['diversity_issues'] if not pd.isna(row['diversity_issues']) else \"None identified\"\n",
    "    \n",
    "    matrix = f\"\"\"# Game Clue Analysis Matrix\n",
    "**Test Run {test_run} | Round {round_num}: {topic_cat}**\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Length Compliance\n",
    "| Status | Criteria |\n",
    "|--------|----------|\n",
    "| {status} | Clues within 15-20 words |\n",
    "\n",
    "**Compliance Rate:** {compliance_rate}% ({compliant}/{total} clues)  \n",
    "**Outliers:** {non_compliant}/{total} clues failed  \n",
    "**Average Word Count:** {row['avg_word_count']} words\n",
    "\n",
    "**Assessment:** {\"‚úÖ Acceptable - Most clues meet length requirements\" if compliance_rate >= 80 else \"‚ùå Critical - Significant length violations require revision\"}\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Quality Scores (Rate 1-5)\n",
    "\n",
    "### Informed Clues: {informed_score}/5  \n",
    "**{get_quality_assessment(informed_score)}**\n",
    "\n",
    "{row['informed_notes']}\n",
    "\n",
    "‚úÖ Strengths:\n",
    "- Generally specific and relate to correct answer\n",
    "- Provide distinct perspectives where applicable\n",
    "\n",
    "‚ö†Ô∏è Concerns:\n",
    "- {row['diversity_issues'] if not pd.isna(row['diversity_issues']) else \"Minor thematic overlap observed\"}\n",
    "\n",
    "### Misinformed Clues: {misinformed_score}/5  \n",
    "**{get_quality_assessment(misinformed_score)}**\n",
    "\n",
    "{row['misinformed_notes']}\n",
    "\n",
    "‚úÖ Strengths:\n",
    "- Attempt to create ambiguity\n",
    "- Generally related to the correct answer\n",
    "\n",
    "‚ö†Ô∏è Concerns:\n",
    "- May need more subtle misdirection\n",
    "- Ambiguity effectiveness varies\n",
    "\n",
    "### Fake Clues: {fake_score}/5  \n",
    "**{get_quality_assessment(fake_score)}**\n",
    "\n",
    "{row['fake_notes']}\n",
    "\n",
    "‚úÖ Strengths:\n",
    "- Effectively misdirect to wrong answer choices\n",
    "- Clear deception without being obvious\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Diversity Check\n",
    "\n",
    "| Aspect | Status |\n",
    "|--------|--------|\n",
    "| Theme Coverage | {\"‚úÖ PASS\" if \"diversity\" not in diversity_issues.lower() else \"‚ùå FAIL\"} |\n",
    "| Clue Variation | {\"‚úÖ PASS\" if informed_score >= 3 else \"‚ùå FAIL\"} |\n",
    "| Angle Coverage | {\"‚úÖ PASS\" if non_compliant <= 2 else \"‚ùå FAIL\"} |\n",
    "\n",
    "**Issues Found:** {diversity_issues}\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Difficulty Rating\n",
    "\n",
    "| Score | Assessment |\n",
    "|-------|------------|\n",
    "| Rating | {difficulty}/5 - {get_difficulty_assessment(difficulty)} |\n",
    "\n",
    "**Reasoning:** {row['difficulty_reasoning']}\n",
    "\n",
    "---\n",
    "\n",
    "## Overall Assessment\n",
    "\n",
    "**Overall Quality Score:** {(informed_score + misinformed_score + fake_score) / 3:.1f}/5\n",
    "\n",
    "**Pass/Fail:** {\"‚úÖ PASS\" if compliance_rate >= 70 and (informed_score + misinformed_score + fake_score) / 3 >= 3 else \"‚ö†Ô∏è NEEDS REVISION\"}\n",
    "\n",
    "**Main Issues:**\n",
    "{chr(10).join(f\"- {issue}\" for issue in issues)}\n",
    "\n",
    "**Priority Actions:**\n",
    "1. {\"Address length compliance\" if compliance_rate < 80 else \"Minor length adjustments\"}\n",
    "2. {\"Enhance misinformed clue ambiguity\" if misinformed_score < 3 else \"Maintain misinformed clue quality\"}\n",
    "3. {\"Increase clue diversity\" if \"diversity\" in diversity_issues.lower() else \"Maintain current diversity\"}\n",
    "\n",
    "**Overall Notes:**  \n",
    "{row['overall_notes']}\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "    return matrix\n",
    "\n",
    "# --- Matrices Generation per Test Run ---\n",
    "# Determine CSV path based on topic_name\n",
    "if topic_name:\n",
    "    topic_folder = Path(topic_name)\n",
    "    csv_path = topic_folder / f\"10_games_llm_analysis_results({topic_name}).csv\"\n",
    "else:\n",
    "    csv_path = Path(\"10_games_llm_analysis_results.csv\")\n",
    "\n",
    "if not csv_path.exists():\n",
    "    print(f\"‚ùå Error: {csv_path} not found.\")\n",
    "else:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    test_runs = df['test_run'].unique()\n",
    "    \n",
    "    # Create topic folder and clue_analysis_matrices subfolder\n",
    "    if topic_name:\n",
    "        topic_folder = Path(topic_name)\n",
    "        topic_folder.mkdir(exist_ok=True)\n",
    "        matrices_dir = topic_folder / \"clue_analysis_matrices\"\n",
    "    else:\n",
    "        matrices_dir = Path(\"clue_analysis_matrices\")\n",
    "    \n",
    "    matrices_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    for test in sorted(test_runs):\n",
    "        group = df[df['test_run'] == test]\n",
    "        text = f\"# Analysis for Test {test}\\n\\n\"\n",
    "        \n",
    "        # Append matrices for each round in the test\n",
    "        rounds = sorted(group['round'].unique())\n",
    "        for r in rounds:\n",
    "            row = group[group['round'] == r].iloc[0]\n",
    "            matrix_text = generate_matrix_for_round(row)\n",
    "            text += matrix_text + \"\\n\\n\"\n",
    "        \n",
    "        # Append a round-by-round performance summary table for this test\n",
    "        text += \"## Round-by-Round Performance Summary\\n\\n\"\n",
    "        text += \"| Round | Length Compliance | Informed | Misinformed | Fake | Difficulty |\\n\"\n",
    "        text += \"|-------|-------------------|----------|-------------|------|------------|\\n\"\n",
    "        for r in rounds:\n",
    "            row = group[group['round'] == r].iloc[0]\n",
    "            length_comp = row['length_compliance_rate']\n",
    "            inf_score = row['informed_quality']\n",
    "            mis_score = row['misinformed_quality']\n",
    "            fake_score = row['fake_quality']\n",
    "            difficulty = row['difficulty']\n",
    "            text += f\"| {r} | {length_comp} | {inf_score}/5 | {mis_score}/5 | {fake_score}/5 | {difficulty}/5 |\\n\"\n",
    "        \n",
    "        # Save markdown for this test run to subfolder\n",
    "        test_file = matrices_dir / f\"game{test}_clue_analysis.md\"\n",
    "        with open(test_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "        print(f\"‚úÖ Generated analysis matrices for Game {test}: {test_file}\")\n",
    "    \n",
    "    # --- Overall Performance Breakdown by Category ---\n",
    "    overall_summary = \"# Overall Performance Breakdown by Category\\n\\n\"\n",
    "    # Fill NaN values with default score of 3 before aggregation\n",
    "    df_clean = df.copy()\n",
    "    df_clean['informed_quality'] = pd.to_numeric(df_clean['informed_quality'], errors='coerce').fillna(3)\n",
    "    df_clean['misinformed_quality'] = pd.to_numeric(df_clean['misinformed_quality'], errors='coerce').fillna(3)\n",
    "    df_clean['fake_quality'] = pd.to_numeric(df_clean['fake_quality'], errors='coerce').fillna(3)\n",
    "    df_clean['difficulty'] = pd.to_numeric(df_clean['difficulty'], errors='coerce').fillna(3)\n",
    "\n",
    "    by_category = df_clean.groupby('topic_category').agg({\n",
    "        'length_compliance_rate': lambda x: f\"{int(x.str.rstrip('%').astype(int).mean()):.0f}%\",\n",
    "        'informed_quality': lambda x: f\"{x.astype(int).mean():.1f}/5\",\n",
    "        'misinformed_quality': lambda x: f\"{x.astype(int).mean():.1f}/5\",\n",
    "        'fake_quality': lambda x: f\"{x.astype(int).mean():.1f}/5\",\n",
    "        'difficulty': lambda x: f\"{x.astype(int).mean():.1f}/5\"\n",
    "    }).reset_index()\n",
    "    overall_summary += by_category.to_markdown(index=False)\n",
    "    \n",
    "    # Save overall summary to a markdown file in topic folder\n",
    "    if topic_name:\n",
    "        topic_folder = Path(topic_name)\n",
    "        overall_file = topic_folder / f\"10_games_clues_quality_summary({topic_name}).md\"\n",
    "    else:\n",
    "        overall_file = Path(\"10_games_clue_quality_summary.md\")\n",
    "    \n",
    "    with open(overall_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(overall_summary)\n",
    "    print(f\"‚úÖ Overall performance by category saved: {overall_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
