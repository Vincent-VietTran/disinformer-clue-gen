{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGwqhPmJVIzb"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oNTYk8g3nDfG"
      },
      "outputs": [],
      "source": [
        "# Initialize environment variables/constants (for Google Colab)\n",
        "# import os\n",
        "# from google.colab import userdata\n",
        "\n",
        "# os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API_KEY\")\n",
        "\n",
        "\n",
        "# Initialize environment variables/constants (for VS Code)\n",
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HTTP status: 200 ‚Äî saved response to groq_models.json\n"
          ]
        }
      ],
      "source": [
        "import os, json, requests\n",
        "\n",
        "key = os.getenv(\"GROQ_API_KEY\")\n",
        "if not key:\n",
        "    raise RuntimeError(\"Set GROQ_API_KEY env var\")\n",
        "\n",
        "# Get list of available models\n",
        "url = \"https://api.groq.com/openai/v1/models\"\n",
        "headers = {\"Authorization\": f\"Bearer {key}\"}\n",
        "\n",
        "try:\n",
        "    r = requests.get(url, headers=headers, timeout=10)\n",
        "\n",
        "    # Try to parse JSON, fall back to raw text\n",
        "    try:\n",
        "        payload = r.json()\n",
        "    except Exception:\n",
        "        payload = {\"raw_text\": r.text}\n",
        "\n",
        "    # Choose output file depending on HTTP status\n",
        "    out_file = \"groq_models.json\" if r.status_code == 200 else \"groq_models_error.json\"\n",
        "    with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"HTTP status: {r.status_code} ‚Äî saved response to {out_file}\")\n",
        "    # Surface HTTP errors as before\n",
        "    r.raise_for_status()\n",
        "\n",
        "except requests.HTTPError as http_err:\n",
        "    print(\"HTTPError:\", http_err)\n",
        "except Exception as e:\n",
        "    print(\"Error:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aR-V4U66oE4T"
      },
      "outputs": [],
      "source": [
        "# Install langchain groq\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# google colab command\n",
        "# !pip install -U langchain-groq\n",
        "\n",
        "# vs code command\n",
        "%pip install langchain\n",
        "%pip install -U langchain-groq\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pD5fVpWSnULv"
      },
      "outputs": [],
      "source": [
        "# Instantiate an LLM\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Avoid using decommissioned/ model, check https://console.groq.com/docs/deprecations for details\n",
        "\n",
        "# Select appropriate supported model from groq_models.json instead\n",
        "model = init_chat_model(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    model_provider=\"groq\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvtdvgtVVM_I"
      },
      "source": [
        "## Clues Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zeo_bTg_nuCn"
      },
      "outputs": [],
      "source": [
        "# Write the prompts\n",
        "import json\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are the game master of a game called \"Disinformer\", which is similar to the \"message relay\" game. Below is the description of how the game works:\n",
        "```\n",
        "In this cooperative game, players use communication and teamwork to uncover the original prompt over multiple rounds of clues. Along the way, they must contend with a disruptive ‚ÄúDisinformer,‚Äù varying player interpretations, and time limits.\n",
        "\n",
        "There will be a minimum of 3 players and maximum of 10 players:\n",
        "- Regular players (a.k.a. the netizens): The job is to solve clues and discover the original prompt\n",
        "- at most 2 misinformed players: Has the same job as the regular players. However, this player is unknowingly being given vague/ambiguous clues.\n",
        "- at most 2 disinformer players: The job is to solve clues and discover prompt to persuade other players from clue.\n",
        "\n",
        "There will be 2 rounds in each game.\n",
        "- In the first round, the players will be given clues to guess a general category/term (e.g. \"movie\", \"song\", \"novel\", etc)\n",
        "- In the second round, the players will be given clues to guess a more specific thing (e.g. \"The Dark Knight (2008)\", \"The Hitchhiker's Guide to the Galaxy (Novel)\", \"Space Oddity - David Bowie (1969)\", etc) which is related to the general category in the previous round.\n",
        "\n",
        "In each round, there will be 3 type of clues for each player:\n",
        "- Informed: Not-so-easy but unambiguous clues.\n",
        "- Misinformed: Ambiguous/vague clues that may potentially make them think an entirely different guess (intended for the misinformed player).\n",
        "- Fake: Clues that point to one of the wrong answers.\n",
        "\n",
        "Additionally, in each round, the players will be given 10 minutes to discuss their guess. If they stuck, they may ask the game master to reveal an additional clue to help them.\n",
        "```\n",
        "\n",
        "As a game master, given a category and a thing (e.g. Movie: The Dark Knight (2008)), for each round, generate:\n",
        "- 9 informed clues for the regular players. Make the clues to be as distinct as possible.\n",
        "- 1 extra informed clue for a backup.\n",
        "- 2 misinformed clues.\n",
        "- 2 fake clues\n",
        "- Also, the answer choices for that round (3 choices)\n",
        "\n",
        "For round 2, make sure it is subtle enough. For example, when generating clues for a movie:\n",
        "- No direct names.\n",
        "- No title references.\n",
        "- Focus on plot nuances, secondary characters, or themes instead of iconic moments.\n",
        "\n",
        "For each of the clues you generated, make sure it is between 15-20 words.\n",
        "\n",
        "Then, you also need to provide 3 instructions to help the disinformer.\n",
        "Based on the set of informed and misinformed clues you have came up with, using the Polarisation strategy, generate 3 instructions to help the disinformer player.\n",
        "\n",
        "However, there are some restrictions that you must follow:\n",
        "- You must not mention the answer choices except for the true answer.\n",
        "- The disinformer is not aware which clues are the misinformed ones. So, avoid giving advice that aims to leverage the misinformed clues\n",
        "\n",
        "After this, we will provide you with a pair consisting of the general category and the more specific thing in the following JSON format: `<general category> - <specific thing>`\n",
        "{\n",
        "  \"round_1\": \"<general category>\",\n",
        "  \"round_2\": \"<specific thing>\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "output_format = \"\"\"\n",
        "Write the ouput using the following JSON format:\n",
        "[\n",
        "  {\n",
        "    \"answer\": \"<Answer of round 1>\",\n",
        "    \"informed_clues\": [<9 clues for the regular players>],\n",
        "    \"misinformed_clues\": [<2 misinformed clues>],\n",
        "    \"extra_clues\": [<1 extra informed clue for a backup>],\n",
        "    \"fake_clues\": [<2 fake clues>],\n",
        "    \"choices\": [<3 answer choices including the true answer>],\n",
        "    \"disinformer_instructions\": [<3 instructions for the disinformer>]\n",
        "  },\n",
        "  {\n",
        "    \"answer\": \"<Answer of round 2>\",\n",
        "    \"informed_clues\": [<9 clues for the regular players>],\n",
        "    \"misinformed_clues\": [<2 misinformed clues>],\n",
        "    \"extra_clues\": [<1 extra informed clue for a backup>],\n",
        "    \"fake_clues\": [<2 fake clues>],\n",
        "    \"choices\": [<3 answer choices including the true answer>],\n",
        "    \"disinformer_instructions\": [<3 instructions for the disinformer>]\n",
        "  },\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "one_shot_example = \"\"\"\n",
        "Below is one example of a query:\n",
        "\n",
        "Q: {\n",
        "  \"round_1\": \"Song\",\n",
        "  \"round_2\": \"Love Story - Taylor Swift\"\n",
        "}\n",
        "A: [\n",
        "  {\n",
        "    \"answer\": \"song\",\n",
        "    \"informed_clues\": [\n",
        "      \"Used to mark an emotional high point of a movie or personal moment.\",\n",
        "      \"It swiftly conveys snapshots you‚Äôd replay in your mind instead of reading them on a page.\",\n",
        "      ...\n",
        "    ],\n",
        "    \"misinformed_clues\": [\n",
        "      \"It‚Äôs something you might browse over your morning coffee\",\n",
        "      \"\"\n",
        "    ],\n",
        "    \"extra_clues\": [\n",
        "      \"It moves you through peaks and valleys of emotion using only rhythm and tone.\"\n",
        "    ],\n",
        "    \"fake_clues\": [\n",
        "      \"\",\n",
        "      \"\"\n",
        "    ],\n",
        "    \"choices\": [\n",
        "      \"book\",\n",
        "      \"short film\",\n",
        "      \"song\"\n",
        "    ],\n",
        "    \"disinformer_instructions\": [\n",
        "      \"\",\n",
        "      \"\",\n",
        "      \"\"\n",
        "    ]\n",
        "  },\n",
        "  {\n",
        "    \"answer\": \"Love Story by Taylor Swift\",\n",
        "    \"informed_clues\": [\n",
        "      \"Draws on imagery of timeless romance, referencing feuding families rather than actual feuding houses.\",\n",
        "      \"Uses a whisper-soft bridge to heighten tension before a triumphant key change.\",\n",
        "      ...\n",
        "    ],\n",
        "    \"misinformed_clues\": [\n",
        "      \"It‚Äôs about sneaking out at dawn to crash a royal wedding you weren‚Äôt invited to.\"\n",
        "    ],\n",
        "    \"extra_clues\": [\n",
        "      \"Evokes a nostalgic flashback of meeting someone young, then leaps into a narrative confession.\"\n",
        "    ],\n",
        "    \"fake_clues\": [\n",
        "      \"\",\n",
        "      \"\"\n",
        "    ],\n",
        "    \"choices\": [\n",
        "      \"A Thousand Years ‚Äì Christina Perri\",\n",
        "      \"Love Story - Taylor Swift\",\n",
        "      \"I Will Always Love You - Whitney Houston\"\n",
        "    ],\n",
        "    \"disinformer_instructions\": [\n",
        "      \"\",\n",
        "      \"\",\n",
        "      \"\"\n",
        "    ]\n",
        "  }\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "user_prompt = json.dumps(\n",
        "    {\n",
        "      \"round_1\": \"Movie\",\n",
        "      \"round_2\": \"Star Wars Episode I: The Phantom Menace\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "A30l2KPXpnx1"
      },
      "outputs": [],
      "source": [
        "# Construct the prompt and invoke the model\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(system_prompt + output_format + one_shot_example),\n",
        "    HumanMessage(user_prompt),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEhE77_CqBWB",
        "outputId": "9be72816-b786-4bde-ed6b-97f6f6787752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "  {\n",
            "    \"answer\": \"movie\",\n",
            "    \"informed_clues\": [\n",
            "      \"A form of storytelling that uses visual and audio elements to convey a narrative.\",\n",
            "      \"Often features a protagonist who embarks on a journey or quest to achieve a goal.\",\n",
            "      \"Can be used to explore complex social issues or to simply entertain and thrill audiences.\",\n",
            "      \"Typically includes a combination of dialogue, music, and sound effects to create a immersive experience.\",\n",
            "      \"May be based on real events, fictional stories, or a combination of both.\",\n",
            "      \"Can be classified into various genres, such as action, comedy, drama, or horror.\",\n",
            "      \"Often features special effects, stunts, and other forms of spectacle to enhance the viewing experience.\",\n",
            "      \"Can be watched in a variety of settings, including theaters, homes, or mobile devices.\",\n",
            "      \"May be used as a form of social commentary or to challenge societal norms and conventions.\"\n",
            "    ],\n",
            "    \"misinformed_clues\": [\n",
            "      \"It's something you might find in a newspaper or magazine, providing information about current events.\",\n",
            "      \"A type of performance that takes place on a stage, featuring live music, dance, or theater.\"\n",
            "    ],\n",
            "    \"extra_clues\": [\n",
            "      \"A medium that allows audiences to temporarily escape from reality and immerse themselves in a different world.\"\n",
            "    ],\n",
            "    \"fake_clues\": [\n",
            "      \"A type of video game that allows players to interact with a virtual environment.\",\n",
            "      \"A form of fine art that uses painting, sculpture, or other mediums to create visual masterpieces.\"\n",
            "    ],\n",
            "    \"choices\": [\n",
            "      \"book\",\n",
            "      \"play\",\n",
            "      \"movie\"\n",
            "    ],\n",
            "    \"disinformer_instructions\": [\n",
            "      \"Emphasize the importance of visual elements in the medium, such as special effects or cinematography, to sway the group's opinion.\",\n",
            "      \"Highlight the diversity of genres and themes that exist within the medium, making it seem more complex and multifaceted than it actually is.\",\n",
            "      \"Use vague language to describe the medium, avoiding specific details or characteristics that might give away its true nature, and instead focus on its emotional or cultural significance.\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"answer\": \"Star Wars Episode I: The Phantom Menace\",\n",
            "    \"informed_clues\": [\n",
            "      \"Features a young protagonist who is discovered to be strong in a mystical energy that binds the galaxy together.\",\n",
            "      \"Takes place in a galaxy where trade federations and planetary systems are at odds with one another.\",\n",
            "      \"Includes a memorable podracing sequence that showcases the protagonist's exceptional skills.\",\n",
            "      \"Introduces a complex political storyline involving senatorial debates and royal intrigue.\",\n",
            "      \"Features a cast of characters that includes a wise and powerful mentor, a cunning politician, and a loyal droid companion.\",\n",
            "      \"Explores the idea of a chosen one who is destined to bring balance to the galaxy.\",\n",
            "      \"Includes a dramatic duel between two skilled warriors, featuring an iconic villain.\",\n",
            "      \"Showcases a variety of alien species and cultures, each with their own unique customs and traditions.\",\n",
            "      \"Takes place in a time period where the galaxy is on the brink of war and chaos.\"\n",
            "    ],\n",
            "    \"misinformed_clues\": [\n",
            "      \"It's a story about a group of space explorers who stumble upon an ancient alien artifact.\",\n",
            "      \"A tale of a rebellion against an oppressive government, led by a charismatic and fearless leader.\"\n",
            "    ],\n",
            "    \"extra_clues\": [\n",
            "      \"The film's narrative is driven by a complex web of alliances, rivalries, and hidden agendas between various factions and characters.\"\n",
            "    ],\n",
            "    \"fake_clues\": [\n",
            "      \"The movie is set in a post-apocalyptic world where resources are scarce and survival is a daily struggle.\",\n",
            "      \"It's a romantic comedy that follows the misadventures of a bumbling hero as he navigates love and relationships.\"\n",
            "    ],\n",
            "    \"choices\": [\n",
            "      \"Star Wars Episode IV: A New Hope\",\n",
            "      \"Star Wars Episode I: The Phantom Menace\",\n",
            "      \"Guardians of the Galaxy\"\n",
            "    ],\n",
            "    \"disinformer_instructions\": [\n",
            "      \"Focus on the theme of friendship and camaraderie between the main characters, downplaying the significance of the larger galaxy-spanning conflict.\",\n",
            "      \"Emphasize the idea that the protagonist is an underdog who rises to greatness through determination and luck, rather than any inherent special abilities.\",\n",
            "      \"Use descriptive language to paint a vivid picture of the film's settings and action sequences, while avoiding any details that might reveal the true plot or characters.\"\n",
            "    ]\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# Invoke the model\n",
        "response = model.invoke(messages)\n",
        "\n",
        "# Print the response\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gowLe0zqqVot",
        "outputId": "25a2e9a5-6e5b-480d-e7cd-4c25cc2b9568"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_tokens': 1395, 'output_tokens': 945, 'total_tokens': 2340}\n"
          ]
        }
      ],
      "source": [
        "# Print the usage metadata\n",
        "print(response.usage_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CNLDkykK-FU"
      },
      "source": [
        "# Game Clue Analysis Matrix\n",
        "\n",
        "## 1. Length Compliance\n",
        "| Status | Criteria |\n",
        "|--------|----------|\n",
        "| ‚úÖ PASS | All clues 15-20 words |\n",
        "| ‚ùå FAIL | Any clues outside range |\n",
        "\n",
        "**Outliers:** ___/13 clues failed\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Quality Scores (Rate 1-5)\n",
        "\n",
        "### Informed Clues: ___/5\n",
        "- [ ] Different angles (plot, characters, themes, technical, cultural)\n",
        "- [ ] Reasonable connection to correct answer\n",
        "- [ ] Nothing gives away too much\n",
        "\n",
        "### Misinformed Clues: ___/5\n",
        "- [ ] Could point to 2+ different answers\n",
        "- [ ] Vague but not nonsensical\n",
        "- [ ] Not obviously wrong\n",
        "\n",
        "### Fake Clues: ___/5\n",
        "- [ ] Clearly point to wrong answer choices\n",
        "- [ ] Believable enough to fool players\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Diversity Check\n",
        "- [ ] **PASS** - Informed clues cover different aspects\n",
        "- [ ] **FAIL** - Found duplicates: ________________\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Difficulty Rating\n",
        "| Score | Assessment |\n",
        "|-------|------------|\n",
        "| 1-2 | Too Easy |\n",
        "| 3 | Just Right |\n",
        "| 4-5 | Too Hard |\n",
        "\n",
        "**Rating:** ___/5\n",
        "\n",
        "---\n",
        "\n",
        "## Overall Assessment\n",
        "**Pass/Fail:** ______  \n",
        "**Main Issues:** ______________________  \n",
        "**Notes:** ____________________________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JYyUqexMP36L"
      },
      "outputs": [],
      "source": [
        "# List of different topics to test\n",
        "test_topics = [\n",
        "    {\"round_1\": \"Movie\", \"round_2\": \"Star Wars Episode I: The Phantom Menace\"},\n",
        "    {\"round_1\": \"Song\", \"round_2\": \"Bohemian Rhapsody - Queen\"},\n",
        "    {\"round_1\": \"Book\", \"round_2\": \"Harry Potter and the Sorcerer's Stone\"},\n",
        "    {\"round_1\": \"TV Show\", \"round_2\": \"Breaking Bad\"},\n",
        "    {\"round_1\": \"Video Game\", \"round_2\": \"The Legend of Zelda: Breath of the Wild\"},\n",
        "    {\"round_1\": \"Food\", \"round_2\": \"Pizza Margherita\"},\n",
        "    {\"round_1\": \"Animal\", \"round_2\": \"African Elephant\"},\n",
        "    {\"round_1\": \"Sport\", \"round_2\": \"Tennis\"},\n",
        "    {\"round_1\": \"Country\", \"round_2\": \"Japan\"},\n",
        "    {\"round_1\": \"Historical Event\", \"round_2\": \"Moon Landing 1969\"}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfPgMxCaJrl1"
      },
      "source": [
        "### Manual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3tUvM920BaA-"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "from datetime import datetime\n",
        "from langchain_core.messages import HumanMessage, SystemMessage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZGIYGFrwK-Ly"
      },
      "outputs": [],
      "source": [
        "def extract_json_from_response(content):\n",
        "    \"\"\"Extract JSON from model response using multiple fallback methods\"\"\"\n",
        "    # Clean escaped quotes\n",
        "    content = content.replace('\\\\\"', '\"')\n",
        "\n",
        "    # Method 1: Direct parse\n",
        "    try:\n",
        "        return json.loads(content)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Method 2: Extract from code blocks\n",
        "    match = re.search(r\"```(?:json)?\\s*(.*?)\\s*```\", content, re.DOTALL)\n",
        "    if match:\n",
        "        try:\n",
        "            return json.loads(match.group(1))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Method 3: Find incomplete array and fix\n",
        "    match = re.search(r\"(\\[.*)\", content, re.DOTALL)\n",
        "    if match:\n",
        "        json_text = match.group(1).rstrip()\n",
        "        if not json_text.endswith(']'):\n",
        "            json_text = json_text.rstrip(',') + ']'\n",
        "        try:\n",
        "            return json.loads(json_text)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "P3wJUwXKBdwP"
      },
      "outputs": [],
      "source": [
        "def process_game_data(game_data, topic, run_number):\n",
        "    \"\"\"Process valid game data into rows\"\"\"\n",
        "    rows = []\n",
        "    for i, round_data in enumerate(game_data, start=1):\n",
        "        answer = round_data.get(\"answer\", \"\")\n",
        "        choices = \", \".join(round_data.get(\"choices\", []))\n",
        "\n",
        "        for clue_type in [\"informed_clues\", \"misinformed_clues\", \"fake_clues\", \"extra_clues\"]:\n",
        "            for j, clue in enumerate(round_data.get(clue_type, []), start=1):\n",
        "                word_count = len(clue.split())\n",
        "                rows.append({\n",
        "                    \"test_run\": run_number,\n",
        "                    \"topic_category\": topic['round_1'],\n",
        "                    \"topic_specific\": topic['round_2'],\n",
        "                    \"round\": i,\n",
        "                    \"answer\": answer,\n",
        "                    \"choices\": choices,\n",
        "                    \"clue_type\": clue_type.replace(\"_clues\", \"\"),\n",
        "                    \"clue_number\": j,\n",
        "                    \"clue_text\": clue,\n",
        "                    \"word_count\": word_count,\n",
        "                    \"length_ok\": \"YES\" if 15 <= word_count <= 20 else \"NO\",\n",
        "                    \"manual_score / comment\": \"\"\n",
        "                })\n",
        "    return rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChsBIcQsBhDM",
        "outputId": "67436162-39dd-4b17-f5d5-682e3eadaf96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running test 1/10: Movie - Star Wars Episode I: The Phantom Menace\n",
            "‚úÖ Test 1 completed successfully\n",
            "Running test 2/10: Song - Bohemian Rhapsody - Queen\n",
            "‚úÖ Test 2 completed successfully\n",
            "Running test 3/10: Book - Harry Potter and the Sorcerer's Stone\n",
            "‚úÖ Test 3 completed successfully\n",
            "Running test 4/10: TV Show - Breaking Bad\n",
            "‚úÖ Test 4 completed successfully\n",
            "Running test 5/10: Video Game - The Legend of Zelda: Breath of the Wild\n",
            "‚úÖ Test 5 completed successfully\n",
            "Running test 6/10: Food - Pizza Margherita\n",
            "‚úÖ Test 6 completed successfully\n",
            "Running test 7/10: Animal - African Elephant\n",
            "‚úÖ Test 7 completed successfully\n",
            "Running test 8/10: Sport - Tennis\n",
            "‚úÖ Test 8 completed successfully\n",
            "Running test 9/10: Country - Japan\n",
            "‚úÖ Test 9 completed successfully\n",
            "Running test 10/10: Historical Event - Moon Landing 1969\n",
            "‚úÖ Test 10 completed successfully\n"
          ]
        }
      ],
      "source": [
        "# Main execution\n",
        "all_rows = []\n",
        "\n",
        "for run_number, topic in enumerate(test_topics, 1):\n",
        "    print(f\"Running test {run_number}/{len(test_topics)}: {topic['round_1']} - {topic['round_2']}\")\n",
        "\n",
        "    messages = [\n",
        "        SystemMessage(system_prompt + output_format + one_shot_example),\n",
        "        HumanMessage(json.dumps(topic)),\n",
        "    ]\n",
        "\n",
        "    response = model.invoke(messages)\n",
        "    clean_content = re.sub(r\"<think>.*?</think>\", \"\", response.content, flags=re.DOTALL).strip()\n",
        "    game_data = extract_json_from_response(clean_content)\n",
        "\n",
        "    if game_data:\n",
        "        try:\n",
        "            all_rows.extend(process_game_data(game_data, topic, run_number))\n",
        "            print(f\"‚úÖ Test {run_number} completed successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error processing data for test {run_number}: {e}\")\n",
        "    else:\n",
        "        print(f\"‚ùå No valid JSON found for test {run_number}\")\n",
        "        print(\"RAW:\", clean_content[:200])\n",
        "\n",
        "    sleep(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6HdR9_0Ht1y",
        "outputId": "d5285434-a3c2-4fab-9c88-672d0a3d7c29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ CSV saved: 10_rounds_clues_analysis.csv\n",
            "Total rows generated: 280\n"
          ]
        }
      ],
      "source": [
        "# Save to CSV\n",
        "with open(\"10_rounds_clues_analysis.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    if all_rows:\n",
        "        writer = csv.DictWriter(f, fieldnames=all_rows[0].keys())\n",
        "        writer.writeheader()\n",
        "        writer.writerows(all_rows)\n",
        "        print(f\"‚úÖ CSV saved: 10_rounds_clues_analysis.csv\")\n",
        "\n",
        "print(f\"Total rows generated: {len(all_rows)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A0fc3WdZymW"
      },
      "source": [
        "## LLM analysis (llama)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ixZR4gtyhwU-"
      },
      "outputs": [],
      "source": [
        "analysis_model = init_chat_model(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    model_provider=\"groq\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_OurpVJ2h9eD"
      },
      "outputs": [],
      "source": [
        "def analyze_round_with_llm(round_data, analysis_model):\n",
        "    \"\"\"Analyze a single round using LLM\"\"\"\n",
        "\n",
        "    # Count words for each clue type\n",
        "    word_counts = {}\n",
        "    length_issues = []\n",
        "\n",
        "    for clue_type in [\"informed_clues\", \"misinformed_clues\", \"fake_clues\", \"extra_clues\"]:\n",
        "        clues = round_data.get(clue_type, [])\n",
        "        word_counts[clue_type] = []\n",
        "\n",
        "        for i, clue in enumerate(clues, 1):\n",
        "            word_count = len(clue.split())\n",
        "            word_counts[clue_type].append(word_count)\n",
        "\n",
        "            if not (15 <= word_count <= 20):\n",
        "                length_issues.append(f\"{clue_type} #{i}: {word_count} words\")\n",
        "\n",
        "    # Create word count summary\n",
        "    word_count_summary = f\"\"\"\n",
        "WORD COUNT ANALYSIS:\n",
        "- Informed clues: {word_counts.get('informed_clues', [])}\n",
        "- Misinformed clues: {word_counts.get('misinformed_clues', [])}\n",
        "- Fake clues: {word_counts.get('fake_clues', [])}\n",
        "- Extra clues: {word_counts.get('extra_clues', [])}\n",
        "\n",
        "LENGTH ISSUES (should be 15-20 words):\n",
        "{'; '.join(length_issues) if length_issues else 'All clues meet length requirements'}\n",
        "\"\"\"\n",
        "\n",
        "    analysis_prompt = f\"\"\"\n",
        "You are evaluating clues for a deduction game. Analyze BOTH word count compliance AND content quality.\n",
        "\n",
        "{word_count_summary}\n",
        "\n",
        "ROUND DATA:\n",
        "{json.dumps(round_data, indent=2)}\n",
        "\n",
        "CRITICAL REQUIREMENTS:\n",
        "1. LENGTH COMPLIANCE: Each clue should be 15-20 words (see analysis above)\n",
        "2. ANSWER CONTAMINATION: Check if ANY clue contains the answer word\n",
        "3. SPECIFICITY: Are clues specific enough to distinguish from similar items?\n",
        "4. CLUE REFERENCES: Use specific clue numbers when noting issues\n",
        "5. DETAILED REASONING: Explain WHY you gave each score\n",
        "\n",
        "CLUE TYPE REQUIREMENTS:\n",
        "- **INFORMED CLUES**: Must relate to actual answer and be distinct/specific (avoid generic descriptions)\n",
        "- **MISINFORMED CLUES**: Must be related to actual answer BUT vague enough to apply to multiple choices (create productive doubt)\n",
        "- **FAKE CLUES**: Must clearly point to the OTHER answer choices, NOT the correct answer (effective misdirection)\n",
        "\n",
        "SCORING SCALE (MANDATORY):\n",
        "Rate based on how well each clue type fulfills its specific purpose:\n",
        "- informed_quality: Rate 1-5 (How well do they point to correct answer specifically?)\n",
        "- misinformed_quality: Rate 1-5 (Do they create ambiguity while staying answer-related?)\n",
        "- fake_quality: Rate 1-5 (Do they clearly misdirect to wrong answer choices?)\n",
        "- difficulty: Rate 1-5 (1=too easy, 2=easy, 3=just right, 4=hard, 5=too hard)\n",
        "\n",
        "Return ONLY this JSON format:\n",
        "{{\n",
        "  \"length_compliance_score\": number (1-5),\n",
        "  \"length_issues_found\": [\"list of specific length problems\"],\n",
        "  \"informed_quality\": number (1-5),\n",
        "  \"informed_notes\": \"detailed analysis with specific clue numbers\",\n",
        "  \"misinformed_quality\": number (1-5),\n",
        "  \"misinformed_notes\": \"detailed analysis of ambiguity effectiveness\",\n",
        "  \"fake_quality\": number (1-5),\n",
        "  \"fake_notes\": \"detailed analysis of misdirection effectiveness\",\n",
        "  \"diversity_issues\": [\"list specific problems found\"],\n",
        "  \"difficulty\": number (1-5),\n",
        "  \"difficulty_reasoning\": \"detailed explanation\",\n",
        "  \"overall_notes\": \"comprehensive summary\"\n",
        "}}\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = analysis_model.invoke([HumanMessage(analysis_prompt)])\n",
        "        return extract_json_from_response(response.content)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå LLM analysis failed: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "rdYVHNS9lv5r"
      },
      "outputs": [],
      "source": [
        "# Load data from your manual analysis CSV\n",
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file from your manual analysis\n",
        "df = pd.read_csv(\"10_rounds_clues_analysis.csv\")  # Change filename as needed\n",
        "\n",
        "# Group data by test_run and round to reconstruct round_data\n",
        "all_results = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ex9epMELiCFt",
        "outputId": "3acb14c5-49db-47e6-e947-a335d1e1b0ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyzing Test 1, Round 1: Movie - movie\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 1, Round 2: Movie - Star Wars Episode I: The Phantom Menace\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 2, Round 1: Song - song\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 2, Round 2: Song - Bohemian Rhapsody - Queen\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 3, Round 1: Book - book\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 3, Round 2: Book - Harry Potter and the Sorcerer's Stone\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 4, Round 1: TV Show - TV Show\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 4, Round 2: TV Show - Breaking Bad\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 5, Round 1: Video Game - video game\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 5, Round 2: Video Game - The Legend of Zelda: Breath of the Wild\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 6, Round 1: Food - food\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 6, Round 2: Food - Pizza Margherita\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 7, Round 1: Animal - animal\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 7, Round 2: Animal - African Elephant\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 8, Round 1: Sport - sport\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 8, Round 2: Sport - Tennis\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 9, Round 1: Country - country\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 9, Round 2: Country - Japan\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 10, Round 1: Historical Event - Historical Event\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 10, Round 2: Historical Event - Moon Landing 1969\n",
            "  ‚úÖ Analyzed successfully\n"
          ]
        }
      ],
      "source": [
        "for (test_run, round_num), group in df.groupby(['test_run', 'round']):\n",
        "    # Skip disinformer instructions\n",
        "    clue_data = group[group['clue_type'] != 'disinformer_instruction']\n",
        "\n",
        "    if len(clue_data) == 0:\n",
        "        continue\n",
        "\n",
        "    # Get basic info\n",
        "    topic_category = clue_data['topic_category'].iloc[0]\n",
        "    topic_specific = clue_data['topic_specific'].iloc[0]\n",
        "    answer = clue_data['answer'].iloc[0]\n",
        "    choices = clue_data['choices'].iloc[0]\n",
        "\n",
        "    print(f\"Analyzing Test {test_run}, Round {round_num}: {topic_category} - {answer}\")\n",
        "\n",
        "    # Reconstruct round_data from CSV\n",
        "    round_data = {\n",
        "        \"answer\": answer,\n",
        "        \"choices\": choices.split(\" | \") if choices else [],\n",
        "        \"informed_clues\": clue_data[clue_data['clue_type'] == 'informed']['clue_text'].tolist(),\n",
        "        \"misinformed_clues\": clue_data[clue_data['clue_type'] == 'misinformed']['clue_text'].tolist(),\n",
        "        \"fake_clues\": clue_data[clue_data['clue_type'] == 'fake']['clue_text'].tolist(),\n",
        "        \"extra_clues\": clue_data[clue_data['clue_type'] == 'extra']['clue_text'].tolist()\n",
        "    }\n",
        "\n",
        "    # Analyze with LLM\n",
        "    analysis = analyze_round_with_llm(round_data, analysis_model)\n",
        "\n",
        "    if analysis:\n",
        "        result = {\n",
        "            \"test_run\": test_run,\n",
        "            \"topic_category\": topic_category,\n",
        "            \"topic_specific\": topic_specific,\n",
        "            \"round\": round_num,\n",
        "            \"answer\": answer,\n",
        "            \"choices\": choices,\n",
        "\n",
        "            # LLM Analysis Results\n",
        "            \"informed_quality\": analysis.get(\"informed_quality\", \"\"),\n",
        "            \"informed_notes\": analysis.get(\"informed_notes\", \"\"),\n",
        "            \"misinformed_quality\": analysis.get(\"misinformed_quality\", \"\"),\n",
        "            \"misinformed_notes\": analysis.get(\"misinformed_notes\", \"\"),\n",
        "            \"fake_quality\": analysis.get(\"fake_quality\", \"\"),\n",
        "            \"fake_notes\": analysis.get(\"fake_notes\", \"\"),\n",
        "            \"diversity_issues\": \"; \".join(analysis.get(\"diversity_issues\", [])),\n",
        "            \"difficulty\": analysis.get(\"difficulty\", \"\"),\n",
        "            \"difficulty_reasoning\": analysis.get(\"difficulty_reasoning\", \"\"),\n",
        "            \"overall_notes\": analysis.get(\"overall_notes\", \"\"),\n",
        "\n",
        "            # Word count and length compliance data\n",
        "            \"total_clues\": len(round_data[\"informed_clues\"]) + len(round_data[\"misinformed_clues\"]) + len(round_data[\"fake_clues\"]) + len(round_data[\"extra_clues\"]),\n",
        "            \"length_compliant_clues\": sum(1 for clue_type in [\"informed_clues\", \"misinformed_clues\", \"fake_clues\", \"extra_clues\"]\n",
        "                                        for clue in round_data[clue_type]\n",
        "                                        if 15 <= len(clue.split()) <= 20),\n",
        "            \"length_compliance_rate\": f\"{(sum(1 for clue_type in ['informed_clues', 'misinformed_clues', 'fake_clues', 'extra_clues'] for clue in round_data[clue_type] if 15 <= len(clue.split()) <= 20) / max(1, sum(len(round_data[clue_type]) for clue_type in ['informed_clues', 'misinformed_clues', 'fake_clues', 'extra_clues'])) * 100):.0f}%\",\n",
        "            \"avg_word_count\": round(sum(len(clue.split()) for clue_type in [\"informed_clues\", \"misinformed_clues\", \"fake_clues\", \"extra_clues\"] for clue in round_data[clue_type]) / max(1, sum(len(round_data[clue_type]) for clue_type in [\"informed_clues\", \"misinformed_clues\", \"fake_clues\", \"extra_clues\"])), 1)\n",
        "        }\n",
        "\n",
        "        all_results.append(result)\n",
        "        print(f\"  ‚úÖ Analyzed successfully\")\n",
        "    else:\n",
        "        print(f\"  ‚ùå Analysis failed\")\n",
        "\n",
        "    sleep(2)  # Rate limiting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xayzwc_0iO-L",
        "outputId": "f1735cba-482e-4fec-c580-03c4e264fb10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ LLM analysis complete! Saved 20 results to: llm_analysis_results.csv\n"
          ]
        }
      ],
      "source": [
        "# Save results\n",
        "if all_results:\n",
        "    with open(\"llm_analysis_results.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=all_results[0].keys())\n",
        "        writer.writeheader()\n",
        "        writer.writerows(all_results)\n",
        "\n",
        "    print(f\"‚úÖ LLM analysis complete! Saved {len(all_results)} results to: llm_analysis_results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Generated analysis matrices for Test 1: clue_analysis_matrices\\test1_clue_analysis.md\n",
            "‚úÖ Generated analysis matrices for Test 2: clue_analysis_matrices\\test2_clue_analysis.md\n",
            "‚úÖ Generated analysis matrices for Test 3: clue_analysis_matrices\\test3_clue_analysis.md\n",
            "‚úÖ Generated analysis matrices for Test 4: clue_analysis_matrices\\test4_clue_analysis.md\n",
            "‚úÖ Generated analysis matrices for Test 5: clue_analysis_matrices\\test5_clue_analysis.md\n",
            "‚úÖ Generated analysis matrices for Test 6: clue_analysis_matrices\\test6_clue_analysis.md\n",
            "‚úÖ Generated analysis matrices for Test 7: clue_analysis_matrices\\test7_clue_analysis.md\n",
            "‚úÖ Generated analysis matrices for Test 8: clue_analysis_matrices\\test8_clue_analysis.md\n",
            "‚úÖ Generated analysis matrices for Test 9: clue_analysis_matrices\\test9_clue_analysis.md\n",
            "‚úÖ Generated analysis matrices for Test 10: clue_analysis_matrices\\test10_clue_analysis.md\n",
            "‚úÖ Overall performance by category saved: Overall_Performance_By_Category.MD\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Ensure the utility functions below exist in your notebook cell.\n",
        "def calculate_length_compliance(row):\n",
        "    compliance_rate = int(row['length_compliance_rate'].rstrip('%'))\n",
        "    total_clues = row['total_clues']\n",
        "    compliant = row['length_compliant_clues']\n",
        "    non_compliant = total_clues - compliant\n",
        "    return compliance_rate, compliant, non_compliant, total_clues\n",
        "\n",
        "def get_pass_fail_status(compliance_rate):\n",
        "    return \"‚úÖ PASS\" if compliance_rate >= 80 else \"‚ùå FAIL\"\n",
        "\n",
        "def get_quality_assessment(score):\n",
        "    assessments = {\n",
        "        1: \"Poor - Needs significant revision\",\n",
        "        2: \"Fair - Below expectations\",\n",
        "        3: \"Good - Meets requirements\",\n",
        "        4: \"Very Good - Exceeds expectations\",\n",
        "        5: \"Excellent - Outstanding\"\n",
        "    }\n",
        "    return assessments.get(int(score), \"Unknown\")\n",
        "\n",
        "def get_difficulty_assessment(difficulty):\n",
        "    difficulty = int(difficulty)\n",
        "    if difficulty <= 2:\n",
        "        return \"üü¢ Too Easy\"\n",
        "    elif difficulty == 3:\n",
        "        return \"üü¢ Just Right\"\n",
        "    else:\n",
        "        return \"üü† Too Hard\"\n",
        "\n",
        "def extract_issues(notes_str):\n",
        "    import pandas as pd\n",
        "    if pd.isna(notes_str):\n",
        "        return [\"None identified\"]\n",
        "    notes_str = str(notes_str).lower()\n",
        "    issues = []\n",
        "    keywords = {\n",
        "        \"length\": \"Word count compliance issues\",\n",
        "        \"generic\": \"Generic/vague clues\",\n",
        "        \"diversity\": \"Lack of diversity in themes\",\n",
        "        \"ambiguity\": \"Insufficient ambiguity in misinformed clues\",\n",
        "        \"specificity\": \"Missing specificity in clues\",\n",
        "        \"answer contamination\": \"Answer word revealed in clues\"\n",
        "    }\n",
        "    for keyword, issue in keywords.items():\n",
        "        if keyword in notes_str:\n",
        "            issues.append(issue)\n",
        "    return issues if issues else [\"Minor issues noted\"]\n",
        "\n",
        "def generate_matrix_for_round(row):\n",
        "    test_run = int(row['test_run'])\n",
        "    topic_cat = row['topic_category']\n",
        "    topic_spec = row['topic_specific']\n",
        "    round_num = int(row['round'])\n",
        "    compliance_rate, compliant, non_compliant, total = calculate_length_compliance(row)\n",
        "    status = get_pass_fail_status(compliance_rate)\n",
        "    informed_score = int(row['informed_quality'])\n",
        "    misinformed_score = int(row['misinformed_quality'])\n",
        "    fake_score = int(row['fake_quality'])\n",
        "    difficulty = int(row['difficulty'])\n",
        "    issues = extract_issues(row['overall_notes'])\n",
        "    diversity_issues = row['diversity_issues'] if not pd.isna(row['diversity_issues']) else \"None identified\"\n",
        "    \n",
        "    matrix = f\"\"\"# Game Clue Analysis Matrix\n",
        "**Test Run {test_run} | Round {round_num}: {topic_cat} ‚Üí {topic_spec}**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Length Compliance\n",
        "| Status | Criteria |\n",
        "|--------|----------|\n",
        "| {status} | Clues within 15-20 words |\n",
        "\n",
        "**Compliance Rate:** {compliance_rate}% ({compliant}/{total} clues)  \n",
        "**Outliers:** {non_compliant}/{total} clues failed  \n",
        "**Average Word Count:** {row['avg_word_count']} words\n",
        "\n",
        "**Assessment:** {\"‚úÖ Acceptable - Most clues meet length requirements\" if compliance_rate >= 80 else \"‚ùå Critical - Significant length violations require revision\"}\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Quality Scores (Rate 1-5)\n",
        "\n",
        "### Informed Clues: {informed_score}/5  \n",
        "**{get_quality_assessment(informed_score)}**\n",
        "\n",
        "{row['informed_notes']}\n",
        "\n",
        "‚úÖ Strengths:\n",
        "- Generally specific and relate to correct answer\n",
        "- Provide distinct perspectives where applicable\n",
        "\n",
        "‚ö†Ô∏è Concerns:\n",
        "- {row['diversity_issues'] if not pd.isna(row['diversity_issues']) else \"Minor thematic overlap observed\"}\n",
        "\n",
        "### Misinformed Clues: {misinformed_score}/5  \n",
        "**{get_quality_assessment(misinformed_score)}**\n",
        "\n",
        "{row['misinformed_notes']}\n",
        "\n",
        "‚úÖ Strengths:\n",
        "- Attempt to create ambiguity\n",
        "- Generally related to the correct answer\n",
        "\n",
        "‚ö†Ô∏è Concerns:\n",
        "- May need more subtle misdirection\n",
        "- Ambiguity effectiveness varies\n",
        "\n",
        "### Fake Clues: {fake_score}/5  \n",
        "**{get_quality_assessment(fake_score)}**\n",
        "\n",
        "{row['fake_notes']}\n",
        "\n",
        "‚úÖ Strengths:\n",
        "- Effectively misdirect to wrong answer choices\n",
        "- Clear deception without being obvious\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Diversity Check\n",
        "\n",
        "| Aspect | Status |\n",
        "|--------|--------|\n",
        "| Theme Coverage | {\"‚úÖ PASS\" if \"diversity\" not in diversity_issues.lower() else \"‚ùå FAIL\"} |\n",
        "| Clue Variation | {\"‚úÖ PASS\" if informed_score >= 3 else \"‚ùå FAIL\"} |\n",
        "| Angle Coverage | {\"‚úÖ PASS\" if non_compliant <= 2 else \"‚ùå FAIL\"} |\n",
        "\n",
        "**Issues Found:** {diversity_issues}\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Difficulty Rating\n",
        "\n",
        "| Score | Assessment |\n",
        "|-------|------------|\n",
        "| Rating | {difficulty}/5 - {get_difficulty_assessment(difficulty)} |\n",
        "\n",
        "**Reasoning:** {row['difficulty_reasoning']}\n",
        "\n",
        "---\n",
        "\n",
        "## Overall Assessment\n",
        "\n",
        "**Overall Quality Score:** {(informed_score + misinformed_score + fake_score) / 3:.1f}/5\n",
        "\n",
        "**Pass/Fail:** {\"‚úÖ PASS\" if compliance_rate >= 70 and (informed_score + misinformed_score + fake_score) / 3 >= 3 else \"‚ö†Ô∏è NEEDS REVISION\"}\n",
        "\n",
        "**Main Issues:**\n",
        "{chr(10).join(f\"- {issue}\" for issue in issues)}\n",
        "\n",
        "**Priority Actions:**\n",
        "1. {\"Address length compliance\" if compliance_rate < 80 else \"Minor length adjustments\"}\n",
        "2. {\"Enhance misinformed clue ambiguity\" if misinformed_score < 3 else \"Maintain misinformed clue quality\"}\n",
        "3. {\"Increase clue diversity\" if \"diversity\" in diversity_issues.lower() else \"Maintain current diversity\"}\n",
        "\n",
        "**Overall Notes:**  \n",
        "{row['overall_notes']}\n",
        "\n",
        "---\n",
        "\"\"\"\n",
        "    return matrix\n",
        "\n",
        "# --- Matrices Generation per Test Run ---\n",
        "csv_path = Path(\"llm_analysis_results.csv\")\n",
        "if not csv_path.exists():\n",
        "    print(f\"‚ùå Error: {csv_path} not found.\")\n",
        "else:\n",
        "    df = pd.read_csv(csv_path)\n",
        "    test_runs = df['test_run'].unique()\n",
        "    dir = Path(\"clue_analysis_matrices\")\n",
        "    dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    for test in sorted(test_runs):\n",
        "        group = df[df['test_run'] == test]\n",
        "        text = f\"# Analysis for Test {test}\\n\\n\"\n",
        "        \n",
        "        # Append matrices for each round in the test\n",
        "        rounds = sorted(group['round'].unique())\n",
        "        for r in rounds:\n",
        "            row = group[group['round'] == r].iloc[0]\n",
        "            matrix_text = generate_matrix_for_round(row)\n",
        "            text += matrix_text + \"\\n\\n\"\n",
        "        \n",
        "        # Append a round-by-round performance summary table for this test\n",
        "        text += \"## Round-by-Round Performance Summary\\n\\n\"\n",
        "        text += \"| Round | Length Compliance | Informed | Misinformed | Fake | Difficulty |\\n\"\n",
        "        text += \"|-------|-------------------|----------|-------------|------|------------|\\n\"\n",
        "        for r in rounds:\n",
        "            row = group[group['round'] == r].iloc[0]\n",
        "            length_comp = row['length_compliance_rate']\n",
        "            inf_score = row['informed_quality']\n",
        "            mis_score = row['misinformed_quality']\n",
        "            fake_score = row['fake_quality']\n",
        "            difficulty = row['difficulty']\n",
        "            text += f\"| {r} | {length_comp} | {inf_score}/5 | {mis_score}/5 | {fake_score}/5 | {difficulty}/5 |\\n\"\n",
        "        \n",
        "        # Save markdown for this test run\n",
        "        test_file = dir / f\"test{test}_clue_analysis.md\"\n",
        "        with open(test_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(text)\n",
        "        print(f\"‚úÖ Generated analysis matrices for Test {test}: {test_file}\")\n",
        "    \n",
        "    # --- Overall Performance Breakdown by Category ---\n",
        "    overall_summary = \"# Overall Performance Breakdown by Category\\n\\n\"\n",
        "    by_category = df.groupby('topic_category').agg({\n",
        "        'length_compliance_rate': lambda x: f\"{int(x.str.rstrip('%').astype(int).mean()):.0f}%\",\n",
        "        'informed_quality': lambda x: f\"{x.astype(int).mean():.1f}/5\",\n",
        "        'misinformed_quality': lambda x: f\"{x.astype(int).mean():.1f}/5\",\n",
        "        'fake_quality': lambda x: f\"{x.astype(int).mean():.1f}/5\",\n",
        "        'difficulty': lambda x: f\"{x.astype(int).mean():.1f}/5\"\n",
        "    }).reset_index()\n",
        "    overall_summary += by_category.to_markdown(index=False)\n",
        "    \n",
        "    # Save overall summary to a markdown file\n",
        "    overall_file = Path(\"Disinformer_Game_Clues_Quality_Summary.MD\")\n",
        "    with open(overall_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(overall_summary)\n",
        "    print(f\"‚úÖ Overall performance by category saved: {overall_file}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
