{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGwqhPmJVIzb"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "oNTYk8g3nDfG"
      },
      "outputs": [],
      "source": [
        "# Initialize environment variables/constants (for Google Colab)\n",
        "# import os\n",
        "# from google.colab import userdata\n",
        "\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# Initialize environment variables/constants (for VS Code)\n",
        "import os\n",
        "\n",
        "# Set your Google Gemini API key here or in your environment variables\n",
        "# You can get a free API key from: https://aistudio.google.com/app/apikey\n",
        "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "aR-V4U66oE4T"
      },
      "outputs": [],
      "source": [
        "# Install langchain google genai\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# google colab command\n",
        "# !pip install -U langchain-google-genai\n",
        "\n",
        "# vs code command\n",
        "%pip install langchain\n",
        "%pip install -U langchain-google-genai\n",
        "%pip install google-generativeai\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pD5fVpWSnULv"
      },
      "outputs": [],
      "source": [
        "# Instantiate an LLM\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Select free tier Gemini model\n",
        "model = init_chat_model(\n",
        "    # model=\"gemini-2.5-flash\",\n",
        "    model=\"gemini-2.0-flash-lite\",\n",
        "    model_provider=\"google_genai\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvtdvgtVVM_I"
      },
      "source": [
        "## Clues Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zeo_bTg_nuCn"
      },
      "outputs": [],
      "source": [
        "# Write the prompts\n",
        "import json\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are the game master of a game called \"Disinformer\", which is similar to the \"message relay\" game. Below is the description of how the game works:\n",
        "```\n",
        "In this cooperative game, players use communication and teamwork to uncover the original prompt over multiple rounds of clues. Along the way, they must contend with a disruptive \"Disinformer,\" varying player interpretations, and time limits.\n",
        "\n",
        "There will be a minimum of 3 players and maximum of 10 players:\n",
        "- Regular players (a.k.a. the netizens): The job is to solve clues and discover the original prompt\n",
        "- at most 2 misinformed players: Has the same job as the regular players. However, this player is unknowingly being given vague/ambiguous clues.\n",
        "- at most 2 disinformer players: The job is to solve clues and discover prompt to persuade other players from clue.\n",
        "\n",
        "There will be 2 rounds in each game.\n",
        "- In the first round, the players will be given clues to guess a general category/term (e.g. \"movie\", \"song\", \"novel\", etc)\n",
        "- In the second round, the players will be given clues to guess a more specific thing (e.g. \"The Dark Knight (2008)\", \"The Hitchhiker's Guide to the Galaxy (Novel)\", \"Space Oddity - David Bowie (1969)\", etc) which is related to the general category in the previous round.\n",
        "\n",
        "In each round, there will be 3 type of clues for each player:\n",
        "- Informed: Not-so-easy but unambiguous clues.\n",
        "- Misinformed: Ambiguous/vague clues that may potentially make them think an entirely different guess (intended for the misinformed player).\n",
        "- Fake: Clues that point to one of the wrong answers.\n",
        "\n",
        "Additionally, in each round, the players will be given 10 minutes to discuss their guess. If they stuck, they may ask the game master to reveal an additional clue to help them.\n",
        "```\n",
        "\n",
        "As a game master, given a category and a thing (e.g. Movie: The Dark Knight (2008)), for each round, generate:\n",
        "- 9 informed clues for the regular players. Make the clues to be as distinct as possible.\n",
        "- 1 extra informed clue for a backup.\n",
        "- 2 misinformed clues.\n",
        "- 2 fake clues\n",
        "- Also, the answer choices for that round (3 choices)\n",
        "\n",
        "For round 2, make sure it is subtle enough. For example, when generating clues for a movie:\n",
        "- No direct names.\n",
        "- No title references.\n",
        "- Focus on plot nuances, secondary characters, or themes instead of iconic moments.\n",
        "\n",
        "\n",
        "=== MANDATORY WORD COUNT REQUIREMENT ===\n",
        "**CRITICAL: EVERY SINGLE CLUE MUST BE EXACTLY 15-20 WORDS. NO EXCEPTIONS.**\n",
        "\n",
        "Before submitting your response, you MUST:\n",
        "1. Count every word in every clue individually (use a word counter)\n",
        "3. Verify ALL 28 clues fall within 15-20 words\n",
        "4. If even ONE clue is outside range, STOP and rewrite ONLY that clue\n",
        "5. Repeat until 100% pass validation\n",
        "\n",
        "Example of VALID clues (count the words):\n",
        "- \"The protagonist discovers a hidden power while fleeing from mysterious pursuers in an ancient temple underground.\" (15 words)\n",
        "- \"A clever scientist creates an invention that has unintended consequences for society and their personal relationships.\" (16 words)\n",
        "- \"Betrayal and redemption intertwine as characters navigate political conflicts involving espionage international borders and moral dilemmas.\" (16 words)\n",
        "\n",
        "Example of INVALID clues (DO NOT USE):\n",
        "- \"A discovery.\" (2 words)\n",
        "- \"The plot involves something significant.\" (5 words)\n",
        "- \"This film explores themes that are quite complex and multifaceted in nature and shows characters.\" (14 words)\n",
        "- \"The protagonist faces numerous challenges while trying to achieve their goal against overwhelming odds in a fantasy world with magic and danger.\" (21 words)\n",
        "\n",
        "Then, you also need to provide 3 instructions to help the disinformer.\n",
        "Based on the set of informed and misinformed clues you have came up with, using the Polarisation strategy, generate 3 instructions to help the disinformer player.\n",
        "\n",
        "However, there are some restrictions that you must follow:\n",
        "- You must not mention the answer choices except for the true answer.\n",
        "- The disinformer is not aware which clues are the misinformed ones. So, avoid giving advice that aims to leverage the misinformed clues\n",
        "\n",
        "After this, we will provide you with a pair consisting of the general category and the more specific thing in the following JSON format: `<general category> - <specific thing>`\n",
        "{\n",
        "  \"round_1\": \"<general category>\",\n",
        "  \"round_2\": \"<specific thing>\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "output_format = \"\"\"\n",
        "BEFORE SUBMITTING YOUR RESPONSE:\n",
        "1. Count every single clue word-by-word\n",
        "2. Verify EVERY clue is 15-20 words\n",
        "3. If even ONE clue is outside this range, STOP and rewrite it\n",
        "4. Only submit when ALL clues pass the word count check\n",
        "\n",
        "VALIDATION CHECKLIST (COMPLETE BEFORE SUBMISSION):\n",
        "‚úì Round 1 informed (9 clues): ALL 15-20 words? YES/NO\n",
        "‚úì Round 1 misinformed (2 clues): ALL 15-20 words? YES/NO\n",
        "‚úì Round 1 fake (2 clues): ALL 15-20 words? YES/NO\n",
        "‚úì Round 1 extra (1 clue): 15-20 words? YES/NO\n",
        "‚úì Round 2 informed (9 clues): ALL 15-20 words? YES/NO\n",
        "‚úì Round 2 misinformed (2 clues): ALL 15-20 words? YES/NO\n",
        "‚úì Round 2 fake (2 clues): ALL 15-20 words? YES/NO\n",
        "‚úì Round 2 extra (1 clue): 15-20 words? YES/NO\n",
        "\n",
        "DO NOT SUBMIT JSON IF ANY CHECKBOX IS NO.\n",
        "\n",
        "Total clues to validate: 28 clues (14 per round)\n",
        "If ANY clue fails validation, regenerate all clues in that round.\n",
        "**RESPONSE FORMAT**: You MUST respond with valid JSON only. No markdown, no explanations outside JSON.\n",
        "\n",
        "Write the output using the following JSON format:\n",
        "[\n",
        "  {\n",
        "    \"answer\": \"<Answer of round 1>\",\n",
        "    \"informed_clues\": [<9 clues - EACH MUST BE 15-20 WORDS>],\n",
        "    \"misinformed_clues\": [<2 clues - EACH MUST BE 15-20 WORDS>],\n",
        "    \"extra_clues\": [<1 clue - MUST BE 15-20 WORDS>],\n",
        "    \"fake_clues\": [<2 clues - EACH MUST BE 15-20 WORDS>],\n",
        "    \"choices\": [<3 answer choices including the true answer>],\n",
        "    \"disinformer_instructions\": [<3 instructions for the disinformer>]\n",
        "  },\n",
        "  {\n",
        "    \"answer\": \"<Answer of round 2>\",\n",
        "    \"informed_clues\": [<9 clues - EACH MUST BE 15-20 WORDS>],\n",
        "    \"misinformed_clues\": [<2 clues - EACH MUST BE 15-20 WORDS>],\n",
        "    \"extra_clues\": [<1 clue - MUST BE 15-20 WORDS>],\n",
        "    \"fake_clues\": [<2 clues - EACH MUST BE 15-20 WORDS>],\n",
        "    \"choices\": [<3 answer choices including the true answer>],\n",
        "    \"disinformer_instructions\": [<3 instructions for the disinformer>]\n",
        "  }\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "one_shot_example = \"\"\"\n",
        "Below is one example of a query with VALIDATED word counts:\n",
        "\n",
        "Q: {\n",
        "  \"round_1\": \"Song\",\n",
        "  \"round_2\": \"Love Story - Taylor Swift\"\n",
        "}\n",
        "A: [\n",
        "  {\n",
        "    \"answer\": \"song\",\n",
        "    \"informed_clues\": [\n",
        "      \"Used to mark an emotional high point of a movie or personal moment in time.\",\n",
        "      \"It swiftly conveys snapshots you replay in your mind instead of reading them on pages.\",\n",
        "      ...\n",
        "    ],\n",
        "    \"misinformed_clues\": [\n",
        "      \"It's something you might carefully browse over your morning coffee while relaxing peacefully at home.\",\n",
        "      \"Rhyming patterns and rhythmic structures create sounds that echo through spaces and touch hearts deeply.\",\n",
        "      ...\n",
        "    ],\n",
        "    \"extra_clues\": [\n",
        "      \"It moves you through peaks and valleys of emotion using only rhythm and tone together.\"\n",
        "    ],\n",
        "    \"fake_clues\": [\n",
        "      \"Words printed on pages bound together tell stories across centuries and inspire human imagination deeply.\",\n",
        "      \"Visual scenes displayed on screens create narratives showing characters acting in dramatic situations throughout films.\",\n",
        "      ...\n",
        "    ],\n",
        "    \"choices\": [\n",
        "      \"book\",\n",
        "      \"short film\",\n",
        "      \"song\"\n",
        "    ],\n",
        "    \"disinformer_instructions\": [\n",
        "      \"Focus on the emotional impact rather than technical elements\",\n",
        "      \"Notice patterns in how the content engages the audience\",\n",
        "      \"Consider what makes it memorable across different demographics\"\n",
        "    ]\n",
        "  },\n",
        "  {\n",
        "    \"answer\": \"Love Story by Taylor Swift\",\n",
        "    \"informed_clues\": [\n",
        "      \"Draws on imagery of timeless romance and references feuding families rather than actual warring houses.\",\n",
        "      \"Uses a whisper soft bridge section to heighten mounting tension before the triumphant key change.\",\n",
        "      ...\n",
        "    ],\n",
        "    \"misinformed_clues\": [\n",
        "      \"It's about sneaking out at dawn to crash a royal wedding you were not invited to.\",\n",
        "      \"The narrative involves unexpected plot twists regarding romance and rising conflicts between opposing social groups.\",\n",
        "      ...\n",
        "    ],\n",
        "    \"extra_clues\": [\n",
        "      \"Evokes nostalgic flashback of meeting someone young and then leaps into emotional narrative confession.\"\n",
        "    ],\n",
        "    \"fake_clues\": [\n",
        "      \"A contemporary love song exploring themes of eternal devotion and unwavering commitment between two souls.\",\n",
        "      \"A powerful ballad celebrating the strength of love across time and overcoming obstacles together.\",\n",
        "      ...\n",
        "    ],\n",
        "    \"choices\": [\n",
        "      \"A Thousand Years ‚Äì Christina Perri\",\n",
        "      \"Love Story - Taylor Swift\",\n",
        "      \"I Will Always Love You - Whitney Houston\"\n",
        "    ],\n",
        "    \"disinformer_instructions\": [\n",
        "      \"Pay attention to how the story unfolds chronologically through the narrative structure\",\n",
        "      \"Consider the specific cultural or historical references embedded in the composition\",\n",
        "      \"Notice how the musical arrangement shifts to emphasize key emotional moments\"\n",
        "    ]\n",
        "  }\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "user_prompt = json.dumps(\n",
        "    {\n",
        "      \"round_1\": \"Movie\",\n",
        "      \"round_2\": \"Star Wars Episode I: The Phantom Menace\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "A30l2KPXpnx1"
      },
      "outputs": [],
      "source": [
        "# Construct the prompt and invoke the model\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(system_prompt + output_format + one_shot_example),\n",
        "    HumanMessage(user_prompt),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEhE77_CqBWB",
        "outputId": "9be72816-b786-4bde-ed6b-97f6f6787752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```json\n",
            "[\n",
            "  {\n",
            "    \"answer\": \"movie\",\n",
            "    \"informed_clues\": [\n",
            "      \"It is a visual storytelling medium that combines moving images with sound to create narrative experiences.\",\n",
            "      \"Audiences gather to watch these stories unfold on large screens, immersing themselves in the narratives.\",\n",
            "      \"These forms of entertainment often feature actors portraying characters within fictional worlds and scenarios.\",\n",
            "      \"They can range from short films to epic productions, each offering a unique viewing experience.\",\n",
            "      \"The process involves writing scripts, filming scenes, and then editing them together into a final product.\",\n",
            "      \"These stories can be used to entertain, educate, or provoke thought in those who watch them.\",\n",
            "      \"They are a powerful tool for cultural expression and have the ability to reflect society's values.\",\n",
            "      \"They come in various genres, from action and drama to comedy and animation, appealing to diverse tastes.\",\n",
            "      \"Creating these stories is a collaborative effort involving many professionals with specific roles.\"\n",
            "    ],\n",
            "    \"misinformed_clues\": [\n",
            "      \"This is a written form of art that uses words to express ideas, tell stories, and convey emotions.\",\n",
            "      \"They are typically found in libraries, bookstores, or online, providing readers with information and enjoyment.\"\n",
            "    ],\n",
            "    \"extra_clues\": [\n",
            "      \"They are a popular form of entertainment that often involves suspense, humor, and dramatic storytelling elements.\"\n",
            "    ],\n",
            "    \"fake_clues\": [\n",
            "      \"This is a collection of musical compositions designed to be performed by instruments and/or voices.\",\n",
            "      \"They are often played on the radio, at concerts, or through streaming services for listening pleasure.\"\n",
            "    ],\n",
            "    \"choices\": [\n",
            "      \"book\",\n",
            "      \"movie\",\n",
            "      \"song\"\n",
            "    ],\n",
            "    \"disinformer_instructions\": [\n",
            "      \"Emphasize the collaborative nature of the creation process and the many people involved.\",\n",
            "      \"Focus on the diverse genres and the different ways that stories are told visually.\",\n",
            "      \"Highlight the immersive experience that these films provide to the audience.\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"answer\": \"Star Wars Episode I: The Phantom Menace\",\n",
            "    \"informed_clues\": [\n",
            "      \"The film introduces a young protagonist with extraordinary abilities, central to the overarching saga's destiny.\",\n",
            "      \"It features a conflict involving a trade dispute that quickly escalates into a larger intergalactic war.\",\n",
            "      \"The story showcases a battle between an established order and a mysterious, shadowy organization.\",\n",
            "      \"A young queen, facing political turmoil, seeks aid from a group of noble protectors and their allies.\",\n",
            "      \"The film delves into themes of good versus evil and the balance of power in a vast galaxy far away.\",\n",
            "      \"A podracing scene is a thrilling spectacle that showcases the talent of a young, gifted character.\",\n",
            "      \"The film provides a glimpse into the origins of a powerful villain and their rise to influence.\",\n",
            "      \"The narrative explores the early training of a young apprentice, crucial to the hero's journey.\",\n",
            "      \"The film features a diverse cast of characters, including aliens, robots, and various humanoid species.\"\n",
            "    ],\n",
            "    \"misinformed_clues\": [\n",
            "      \"It's a story set in a dystopian future where society is controlled by technology and artificial intelligence.\",\n",
            "      \"The narrative follows a group of survivors as they navigate a post-apocalyptic world filled with zombies and danger.\"\n",
            "    ],\n",
            "    \"extra_clues\": [\n",
            "      \"The film's plot involves a complex web of alliances, betrayals, and the fight for freedom in a galaxy.\"\n",
            "    ],\n",
            "    \"fake_clues\": [\n",
            "      \"A thrilling tale of superheroes banding together to protect the earth from an alien invasion force.\",\n",
            "      \"A romantic comedy set in a bustling city, following two individuals who discover love unexpectedly.\"\n",
            "    ],\n",
            "    \"choices\": [\n",
            "      \"Avengers: Endgame\",\n",
            "      \"Star Wars Episode I: The Phantom Menace\",\n",
            "      \"Inception\"\n",
            "    ],\n",
            "    \"disinformer_instructions\": [\n",
            "      \"Focus on the political intrigue and the conflict between different factions in the narrative.\",\n",
            "      \"Highlight the introduction of key characters and their roles in the larger story arc.\",\n",
            "      \"Emphasize the themes of destiny, good versus evil, and the balance of power.\"\n",
            "    ]\n",
            "  }\n",
            "]\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# Invoke the model\n",
        "response = model.invoke(messages)\n",
        "\n",
        "# Print the response\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gowLe0zqqVot",
        "outputId": "25a2e9a5-6e5b-480d-e7cd-4c25cc2b9568"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "# Print the usage metadata\n",
        "print(response.usage_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CNLDkykK-FU"
      },
      "source": [
        "# Game Clue Analysis Matrix\n",
        "\n",
        "## 1. Length Compliance\n",
        "| Status | Criteria |\n",
        "|--------|----------|\n",
        "| ‚úÖ PASS | All clues 15-20 words |\n",
        "| ‚ùå FAIL | Any clues outside range |\n",
        "\n",
        "**Outliers:** ___/13 clues failed\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Quality Scores (Rate 1-5)\n",
        "\n",
        "### Informed Clues: ___/5\n",
        "- [ ] Different angles (plot, characters, themes, technical, cultural)\n",
        "- [ ] Reasonable connection to correct answer\n",
        "- [ ] Nothing gives away too much\n",
        "\n",
        "### Misinformed Clues: ___/5\n",
        "- [ ] Could point to 2+ different answers\n",
        "- [ ] Vague but not nonsensical\n",
        "- [ ] Not obviously wrong\n",
        "\n",
        "### Fake Clues: ___/5\n",
        "- [ ] Clearly point to wrong answer choices\n",
        "- [ ] Believable enough to fool players\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Diversity Check\n",
        "- [ ] **PASS** - Informed clues cover different aspects\n",
        "- [ ] **FAIL** - Found duplicates: ________________\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Difficulty Rating\n",
        "| Score | Assessment |\n",
        "|-------|------------|\n",
        "| 1-2 | Too Easy |\n",
        "| 3 | Just Right |\n",
        "| 4-5 | Too Hard |\n",
        "\n",
        "**Rating:** ___/5\n",
        "\n",
        "---\n",
        "\n",
        "## Overall Assessment\n",
        "**Pass/Fail:** ______  \n",
        "**Main Issues:** ______________________  \n",
        "**Notes:** ____________________________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "JYyUqexMP36L"
      },
      "outputs": [],
      "source": [
        "# List of different topics to test\n",
        "test_topics = [\n",
        "    {\"round_1\": \"Movie\", \"round_2\": \"Star Wars Episode I: The Phantom Menace\"},\n",
        "    {\"round_1\": \"Song\", \"round_2\": \"Bohemian Rhapsody - Queen\"},\n",
        "    {\"round_1\": \"Book\", \"round_2\": \"Harry Potter and the Sorcerer's Stone\"},\n",
        "    {\"round_1\": \"TV Show\", \"round_2\": \"Breaking Bad\"},\n",
        "    {\"round_1\": \"Video Game\", \"round_2\": \"The Legend of Zelda: Breath of the Wild\"},\n",
        "    {\"round_1\": \"Food\", \"round_2\": \"Pizza Margherita\"},\n",
        "    {\"round_1\": \"Animal\", \"round_2\": \"African Elephant\"},\n",
        "    {\"round_1\": \"Sport\", \"round_2\": \"Tennis\"},\n",
        "    {\"round_1\": \"Country\", \"round_2\": \"Japan\"},\n",
        "    {\"round_1\": \"Historical Event\", \"round_2\": \"Moon Landing 1969\"}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfPgMxCaJrl1"
      },
      "source": [
        "### Manual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "3tUvM920BaA-"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "from datetime import datetime\n",
        "from langchain_core.messages import HumanMessage, SystemMessage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ZGIYGFrwK-Ly"
      },
      "outputs": [],
      "source": [
        "def extract_json_from_response(content):\n",
        "    \"\"\"Extract JSON from model response using multiple fallback methods\"\"\"\n",
        "    # Clean escaped quotes\n",
        "    content = content.replace('\\\\\"', '\"')\n",
        "\n",
        "    # Method 1: Direct parse\n",
        "    try:\n",
        "        return json.loads(content)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Method 2: Extract from code blocks\n",
        "    match = re.search(r\"```(?:json)?\\s*(.*?)\\s*```\", content, re.DOTALL)\n",
        "    if match:\n",
        "        try:\n",
        "            return json.loads(match.group(1))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Method 3: Find incomplete array and fix\n",
        "    match = re.search(r\"(\\[.*)\", content, re.DOTALL)\n",
        "    if match:\n",
        "        json_text = match.group(1).rstrip()\n",
        "        if not json_text.endswith(']'):\n",
        "            json_text = json_text.rstrip(',') + ']'\n",
        "        try:\n",
        "            return json.loads(json_text)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "P3wJUwXKBdwP"
      },
      "outputs": [],
      "source": [
        "def process_game_data(game_data, topic, run_number):\n",
        "    \"\"\"Process valid game data into rows\"\"\"\n",
        "    rows = []\n",
        "    for i, round_data in enumerate(game_data, start=1):\n",
        "        answer = round_data.get(\"answer\", \"\")\n",
        "        choices = \", \".join(round_data.get(\"choices\", []))\n",
        "\n",
        "        for clue_type in [\"informed_clues\", \"misinformed_clues\", \"fake_clues\", \"extra_clues\"]:\n",
        "            for j, clue in enumerate(round_data.get(clue_type, []), start=1):\n",
        "                word_count = len(clue.split())\n",
        "                rows.append({\n",
        "                    \"test_run\": run_number,\n",
        "                    \"topic_category\": topic['round_1'],\n",
        "                    \"topic_specific\": topic['round_2'],\n",
        "                    \"round\": i,\n",
        "                    \"answer\": answer,\n",
        "                    \"choices\": choices,\n",
        "                    \"clue_type\": clue_type.replace(\"_clues\", \"\"),\n",
        "                    \"clue_number\": j,\n",
        "                    \"clue_text\": clue,\n",
        "                    \"word_count\": word_count,\n",
        "                    \"length_ok\": \"YES\" if 15 <= word_count <= 20 else \"NO\",\n",
        "                    \"manual_score / comment\": \"\"\n",
        "                })\n",
        "    return rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Post-Processing Validation Strategy\n",
        "\n",
        "This notebook now implements a robust post-processing approach with **batch fixing** to avoid rate limits:\n",
        "\n",
        "**How it works:**\n",
        "1. **Generate clues** - LLM generates all clues in JSON format\n",
        "2. **Parse and validate** - Extract JSON and count words for each clue\n",
        "3. **Collect invalid clues** - Gather all non-compliant clues (not 15-20 words) during validation\n",
        "4. **Batch fixing** - For all invalid clues:\n",
        "   - Make a **SINGLE** LLM call to rewrite ALL invalid clues at once\n",
        "   - Preserve each clue's core meaning and type (informed/misinformed/fake)\n",
        "   - Retry the entire batch up to 3 times if needed\n",
        "5. **Replace** - Update the game data with corrected clues\n",
        "6. **Report** - Generate a validation summary showing compliance rates and fixes\n",
        "\n",
        "**Key Advantages:**\n",
        "- ‚úÖ **Rate limit friendly**: 1 API call per game instead of 1 per clue\n",
        "- ‚úÖ **Faster**: Parallel processing by the LLM\n",
        "- ‚úÖ **Cost efficient**: Fewer API calls = lower costs\n",
        "\n",
        "**Key Functions:**\n",
        "- `validate_clue_word_count()` - Check if a single clue meets requirements\n",
        "- `batch_rewrite_clues_with_llm()` - Ask LLM to rewrite ALL invalid clues in one call\n",
        "- `validate_and_fix_game_data()` - Validate all clues and batch auto-fix issues\n",
        "- `manual_fix_clues()` - Interactive manual validation and fixing\n",
        "\n",
        "**Usage:**\n",
        "```python\n",
        "# Automatic batch fixing (default - RECOMMENDED)\n",
        "corrected_data, report = validate_and_fix_game_data(game_data, model, auto_fix=True)\n",
        "\n",
        "# Validation only (no automatic fixes)\n",
        "corrected_data, report = validate_and_fix_game_data(game_data, model, auto_fix=False)\n",
        "\n",
        "# Manual interactive fixing\n",
        "corrected_data = manual_fix_clues(game_data)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Validation Workflow Diagram\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ              BATCH-OPTIMIZED CLUE GENERATION PIPELINE            ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "Step 1: INITIAL GENERATION\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  LLM Model   ‚îÇ ‚îÄ‚îÄ‚ñ∫ Generate 28 clues per game (2 rounds)\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚ñº\n",
        "                 Parse JSON\n",
        "\n",
        "Step 2: VALIDATION (Single Pass)\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  For each clue:                         ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Count words: len(clue.split())       ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Check: 15 ‚â§ word_count ‚â§ 20          ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Collect invalid clues with metadata  ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "         ‚îÇ\n",
        "         ‚îú‚îÄ‚ñ∫ ‚úÖ Valid? ‚Üí Keep clue\n",
        "         ‚îÇ\n",
        "         ‚îî‚îÄ‚ñ∫ ‚ùå Invalid? ‚Üí Add to batch collection\n",
        "\n",
        "Step 3: BATCH CORRECTION (if auto_fix=True)\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Batch process ALL invalid clues:       ‚îÇ\n",
        "‚îÇ  1. Build single prompt with all clues  ‚îÇ\n",
        "‚îÇ  2. Request batch rewrite (one call!)   ‚îÇ\n",
        "‚îÇ  3. Preserve meaning & type for each    ‚îÇ\n",
        "‚îÇ  4. Retry entire batch up to 3 times    ‚îÇ\n",
        "‚îÇ  5. Validate all rewritten clues        ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "         ‚îÇ\n",
        "         ‚îú‚îÄ‚ñ∫ ‚úÖ All fixed? ‚Üí Replace in data\n",
        "         ‚îÇ\n",
        "         ‚îî‚îÄ‚ñ∫ ‚ùå Some failed? ‚Üí Keep originals + log\n",
        "\n",
        "Step 4: REPORTING\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Generate validation report:            ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Total clues validated                ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Compliance rate (%)                  ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Clues fixed successfully             ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Failed fixes                         ‚îÇ\n",
        "‚îÇ  ‚Ä¢ Detailed issue list                  ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "         ‚îÇ\n",
        "         ‚ñº\n",
        "   Save to CSV\n",
        "   \n",
        "Performance Benefits:\n",
        "‚Ä¢ OLD: Up to 28 API calls per game (1 per invalid clue)\n",
        "‚Ä¢ NEW: 1 API call per game (batch all invalid clues)\n",
        "‚Ä¢ Result: ~90% reduction in API calls + no rate limiting!\n",
        "   \n",
        "Output Files:\n",
        "‚Ä¢ 10_rounds_clues_analysis(gemini).csv  ‚Üê All clues with metadata\n",
        "‚Ä¢ validation_summary(gemini).csv        ‚Üê Validation metrics per test\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_clue_word_count(clue):\n",
        "    \"\"\"Check if a clue meets the 15-20 word requirement\"\"\"\n",
        "    word_count = len(clue.split())\n",
        "    return 15 <= word_count <= 20, word_count\n",
        "\n",
        "\n",
        "def batch_rewrite_clues_with_llm(invalid_clues, model, max_retries=3):\n",
        "    \"\"\"\n",
        "    Batch rewrite multiple invalid clues in a single LLM call.\n",
        "    \n",
        "    Args:\n",
        "        invalid_clues: List of dicts with keys: 'clue', 'clue_type', 'round_idx', 'clue_idx', 'clue_type_key'\n",
        "        model: The LLM model to use\n",
        "        max_retries: Maximum number of retry attempts for the batch\n",
        "    \n",
        "    Returns:\n",
        "        List of rewritten clues in the same order as input\n",
        "    \"\"\"\n",
        "    if not invalid_clues:\n",
        "        return []\n",
        "    \n",
        "    clue_type_descriptions = {\n",
        "        \"informed\": \"an informed clue that clearly points to the correct answer\",\n",
        "        \"misinformed\": \"a misinformed clue that is vague and could apply to multiple answers\",\n",
        "        \"fake\": \"a fake clue that points to one of the wrong answer choices\",\n",
        "        \"extra\": \"an extra informed clue that provides a helpful hint\"\n",
        "    }\n",
        "    \n",
        "    # Build the batch rewrite prompt\n",
        "    clue_list = []\n",
        "    for i, item in enumerate(invalid_clues, 1):\n",
        "        clue_type = item['clue_type']\n",
        "        description = clue_type_descriptions.get(clue_type, \"a clue\")\n",
        "        clue_list.append(f\"\"\"\n",
        "{i}. Type: {description}\n",
        "   Original ({len(item['clue'].split())} words): \"{item['clue']}\"\n",
        "\"\"\")\n",
        "    \n",
        "    batch_prompt = f\"\"\"You need to rewrite multiple clues to meet the 15-20 word requirement. Each clue must preserve its core meaning and purpose.\n",
        "\n",
        "CLUES TO REWRITE:\n",
        "{''.join(clue_list)}\n",
        "\n",
        "REQUIREMENTS FOR EACH CLUE:\n",
        "- MUST be exactly 15-20 words (count carefully)\n",
        "- Keep the same meaning and intent\n",
        "- Maintain the same clue type characteristics\n",
        "- Be specific and avoid generic phrases\n",
        "\n",
        "RESPONSE FORMAT: Return ONLY a JSON array with the rewritten clues in the same order. No markdown, no explanations.\n",
        "\n",
        "Example format:\n",
        "[\n",
        "  \"Rewritten clue 1 goes here with exactly fifteen to twenty words carefully counted.\",\n",
        "  \"Rewritten clue 2 goes here with exactly fifteen to twenty words carefully counted.\"\n",
        "]\n",
        "\n",
        "JSON array of {len(invalid_clues)} rewritten clues:\"\"\"\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = model.invoke([HumanMessage(batch_prompt)])\n",
        "            rewritten_clues = extract_json_from_response(response.content)\n",
        "            \n",
        "            if not rewritten_clues or not isinstance(rewritten_clues, list):\n",
        "                print(f\"  ‚ö†Ô∏è Batch rewrite attempt {attempt + 1} failed: Invalid JSON response\")\n",
        "                continue\n",
        "            \n",
        "            if len(rewritten_clues) != len(invalid_clues):\n",
        "                print(f\"  ‚ö†Ô∏è Batch rewrite attempt {attempt + 1} failed: Expected {len(invalid_clues)} clues, got {len(rewritten_clues)}\")\n",
        "                continue\n",
        "            \n",
        "            # Validate all rewritten clues\n",
        "            all_valid = True\n",
        "            validation_results = []\n",
        "            for clue in rewritten_clues:\n",
        "                is_valid, word_count = validate_clue_word_count(clue)\n",
        "                validation_results.append((is_valid, word_count))\n",
        "                if not is_valid:\n",
        "                    all_valid = False\n",
        "            \n",
        "            if all_valid:\n",
        "                print(f\"  ‚úÖ Batch rewrite successful (attempt {attempt + 1}): All {len(rewritten_clues)} clues now valid\")\n",
        "                return rewritten_clues\n",
        "            else:\n",
        "                invalid_count = sum(1 for valid, _ in validation_results if not valid)\n",
        "                print(f\"  ‚ö†Ô∏è Batch rewrite attempt {attempt + 1}: {invalid_count}/{len(rewritten_clues)} clues still invalid\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Error during batch rewrite attempt {attempt + 1}: {e}\")\n",
        "    \n",
        "    # If all retries fail, return original clues\n",
        "    print(f\"  ‚ùå All batch rewrite attempts failed, keeping original clues\")\n",
        "    return [item['clue'] for item in invalid_clues]\n",
        "\n",
        "\n",
        "def validate_and_fix_game_data(game_data, model, auto_fix=True):\n",
        "    \"\"\"\n",
        "    Validate all clues in game data and optionally fix non-compliant ones using batch processing.\n",
        "    \n",
        "    Args:\n",
        "        game_data: The parsed JSON game data\n",
        "        model: The LLM model to use for rewriting\n",
        "        auto_fix: If True, automatically rewrite non-compliant clues in batches\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (corrected_game_data, validation_report)\n",
        "    \"\"\"\n",
        "    validation_report = {\n",
        "        \"total_clues\": 0,\n",
        "        \"compliant_clues\": 0,\n",
        "        \"fixed_clues\": 0,\n",
        "        \"failed_fixes\": 0,\n",
        "        \"issues\": []\n",
        "    }\n",
        "    \n",
        "    # First pass: collect all invalid clues\n",
        "    invalid_clues = []\n",
        "    \n",
        "    for round_idx, round_data in enumerate(game_data, start=1):\n",
        "        print(f\"\\nValidating Round {round_idx}...\")\n",
        "        \n",
        "        for clue_type in [\"informed_clues\", \"misinformed_clues\", \"fake_clues\", \"extra_clues\"]:\n",
        "            clues = round_data.get(clue_type, [])\n",
        "            \n",
        "            for clue_idx, clue in enumerate(clues):\n",
        "                validation_report[\"total_clues\"] += 1\n",
        "                is_valid, word_count = validate_clue_word_count(clue)\n",
        "                \n",
        "                if is_valid:\n",
        "                    validation_report[\"compliant_clues\"] += 1\n",
        "                else:\n",
        "                    issue = f\"Round {round_idx}, {clue_type} #{clue_idx + 1}: {word_count} words\"\n",
        "                    validation_report[\"issues\"].append(issue)\n",
        "                    print(f\"  ‚ö†Ô∏è {issue}\")\n",
        "                    \n",
        "                    if auto_fix:\n",
        "                        invalid_clues.append({\n",
        "                            'clue': clue,\n",
        "                            'clue_type': clue_type.replace(\"_clues\", \"\"),\n",
        "                            'clue_type_key': clue_type,\n",
        "                            'round_idx': round_idx - 1,  # 0-indexed for array access\n",
        "                            'clue_idx': clue_idx,\n",
        "                            'word_count': word_count\n",
        "                        })\n",
        "    \n",
        "    # Second pass: batch fix all invalid clues\n",
        "    if auto_fix and invalid_clues:\n",
        "        print(f\"\\nüîß Batch fixing {len(invalid_clues)} invalid clues...\")\n",
        "        rewritten_clues = batch_rewrite_clues_with_llm(invalid_clues, model)\n",
        "        \n",
        "        # Apply the rewritten clues back to game_data\n",
        "        for i, item in enumerate(invalid_clues):\n",
        "            rewritten_clue = rewritten_clues[i]\n",
        "            is_fixed, new_word_count = validate_clue_word_count(rewritten_clue)\n",
        "            \n",
        "            if is_fixed:\n",
        "                game_data[item['round_idx']][item['clue_type_key']][item['clue_idx']] = rewritten_clue\n",
        "                validation_report[\"fixed_clues\"] += 1\n",
        "                validation_report[\"compliant_clues\"] += 1\n",
        "            else:\n",
        "                validation_report[\"failed_fixes\"] += 1\n",
        "                print(f\"  ‚ùå Failed to fix Round {item['round_idx']+1}, {item['clue_type_key']} #{item['clue_idx']+1}: Still {new_word_count} words\")\n",
        "    \n",
        "    # Calculate compliance rate\n",
        "    if validation_report[\"total_clues\"] > 0:\n",
        "        compliance_rate = (validation_report[\"compliant_clues\"] / validation_report[\"total_clues\"]) * 100\n",
        "        validation_report[\"compliance_rate\"] = f\"{compliance_rate:.1f}%\"\n",
        "    \n",
        "    return game_data, validation_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def manual_fix_clues(game_data):\n",
        "    \"\"\"\n",
        "    Interactive function to manually review and fix non-compliant clues.\n",
        "    Useful when you want more control over the corrections.\n",
        "    \n",
        "    Args:\n",
        "        game_data: The parsed JSON game data\n",
        "    \n",
        "    Returns:\n",
        "        Corrected game data\n",
        "    \"\"\"\n",
        "    print(\"\\nüîç Manual Clue Validation Mode\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for round_idx, round_data in enumerate(game_data, start=1):\n",
        "        print(f\"\\nüìç Round {round_idx}: {round_data.get('answer', 'Unknown')}\")\n",
        "        \n",
        "        for clue_type in [\"informed_clues\", \"misinformed_clues\", \"fake_clues\", \"extra_clues\"]:\n",
        "            clues = round_data.get(clue_type, [])\n",
        "            \n",
        "            for clue_idx, clue in enumerate(clues):\n",
        "                is_valid, word_count = validate_clue_word_count(clue)\n",
        "                \n",
        "                if not is_valid:\n",
        "                    print(f\"\\n‚ö†Ô∏è {clue_type} #{clue_idx + 1} - {word_count} words (expected 15-20)\")\n",
        "                    print(f\"Original: {clue}\")\n",
        "                    \n",
        "                    # Ask user for action\n",
        "                    action = input(\"\\nAction? [s]kip, [e]dit, [a]uto-fix: \").lower()\n",
        "                    \n",
        "                    if action == 'e':\n",
        "                        new_clue = input(\"Enter corrected clue: \")\n",
        "                        new_valid, new_count = validate_clue_word_count(new_clue)\n",
        "                        if new_valid:\n",
        "                            round_data[clue_type][clue_idx] = new_clue\n",
        "                            print(f\"‚úÖ Updated ({new_count} words)\")\n",
        "                        else:\n",
        "                            print(f\"‚ùå Still invalid ({new_count} words). Keeping original.\")\n",
        "                    \n",
        "                    elif action == 'a':\n",
        "                        print(\"ü§ñ Requesting LLM to fix...\")\n",
        "                        # This would require the model to be passed in\n",
        "                        print(\"‚ö†Ô∏è Auto-fix requires model parameter. Use validate_and_fix_game_data() instead.\")\n",
        "                    \n",
        "                    else:\n",
        "                        print(\"‚è≠Ô∏è Skipped\")\n",
        "    \n",
        "    print(\"\\n‚úÖ Manual validation complete\")\n",
        "    return game_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Test the batch validation functions with a single test case\n",
        "# Uncomment to run a quick test before processing all topics\n",
        "\n",
        "# test_topic = {\"round_1\": \"Movie\", \"round_2\": \"Star Wars Episode I: The Phantom Menace\"}\n",
        "# messages = [\n",
        "#     SystemMessage(system_prompt + output_format + one_shot_example),\n",
        "#     HumanMessage(json.dumps(test_topic)),\n",
        "# ]\n",
        "\n",
        "# print(\"üß™ Testing batch validation functions...\")\n",
        "# response = model.invoke(messages)\n",
        "# clean_content = re.sub(r\"<think>.*?</think>\", \"\", response.content, flags=re.DOTALL).strip()\n",
        "# test_game_data = extract_json_from_response(clean_content)\n",
        "\n",
        "# if test_game_data:\n",
        "#     print(\"\\nüìã Original game data received\")\n",
        "#     corrected_data, report = validate_and_fix_game_data(test_game_data, model, auto_fix=True)\n",
        "    \n",
        "#     print(\"\\n\" + \"=\"*80)\n",
        "#     print(\"BATCH VALIDATION REPORT\")\n",
        "#     print(\"=\"*80)\n",
        "#     print(f\"Total clues: {report['total_clues']}\")\n",
        "#     print(f\"Compliant clues: {report['compliant_clues']}\")\n",
        "#     print(f\"Fixed clues: {report['fixed_clues']} (in single batch call)\")\n",
        "#     print(f\"Failed fixes: {report['failed_fixes']}\")\n",
        "#     print(f\"Compliance rate: {report['compliance_rate']}\")\n",
        "    \n",
        "#     if report['issues']:\n",
        "#         print(f\"\\nIssues found:\")\n",
        "#         for issue in report['issues']:\n",
        "#             print(f\"  - {issue}\")\n",
        "# else:\n",
        "#     print(\"‚ùå Failed to extract JSON from test response\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### üìã Quick Reference Card\n",
        "\n",
        "#### Configuration Options\n",
        "\n",
        "```python\n",
        "# In validate_and_fix_game_data() function call:\n",
        "\n",
        "auto_fix=True   # Automatically batch rewrite non-compliant clues (RECOMMENDED)\n",
        "auto_fix=False  # Only validate, don't fix (for review only)\n",
        "```\n",
        "\n",
        "#### Key Validation Metrics\n",
        "\n",
        "| Metric | Good | Warning | Critical |\n",
        "|--------|------|---------|----------|\n",
        "| Compliance Rate | ‚â•95% | 80-94% | <80% |\n",
        "| Failed Fixes | 0 | 1-2 | ‚â•3 |\n",
        "| Initial Compliance | ‚â•90% | 70-89% | <70% |\n",
        "\n",
        "#### API Call Optimization\n",
        "\n",
        "**OLD Approach (Per-Clue Fixing):**\n",
        "- 10 invalid clues = 10 API calls\n",
        "- High rate limit risk\n",
        "- Slow processing\n",
        "\n",
        "**NEW Approach (Batch Fixing):**\n",
        "- 10 invalid clues = 1 API call\n",
        "- No rate limit issues\n",
        "- 10x faster processing\n",
        "\n",
        "#### Troubleshooting Tips\n",
        "\n",
        "**Low Initial Compliance (<70%)**\n",
        "- LLM may need prompt tuning\n",
        "- Try different model parameters\n",
        "\n",
        "**High Failed Fixes (‚â•3 per test)**\n",
        "- Increase `max_retries` from 3 to 5 in `batch_rewrite_clues_with_llm()`\n",
        "- Check batch rewrite prompt clarity\n",
        "- Consider manual review with `manual_fix_clues()`\n",
        "\n",
        "**Rate Limiting Errors (Should be rare now!)**\n",
        "- Increase `sleep()` time between test runs (currently 5s)\n",
        "- Batch approach already minimizes API calls\n",
        "\n",
        "#### Output Files Guide\n",
        "\n",
        "| File | Contents | Use Case |\n",
        "|------|----------|----------|\n",
        "| `10_rounds_clues_analysis(gemini).csv` | All clues with validation status | Detailed clue-by-clue analysis |\n",
        "| `validation_summary(gemini).csv` | Per-test validation metrics | Track fix rates across topics |\n",
        "| `llm_analysis_results(gemini).csv` | LLM quality analysis | Content quality assessment |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChsBIcQsBhDM",
        "outputId": "67436162-39dd-4b17-f5d5-682e3eadaf96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Running test 1/10: Movie - Star Wars Episode I: The Phantom Menace\n",
            "================================================================================\n",
            "\n",
            "üìã Validating clues...\n",
            "\n",
            "Validating Round 1...\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #2: 13 words\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #3: 14 words\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #6: 14 words\n",
            "\n",
            "Validating Round 2...\n",
            "  ‚ö†Ô∏è Round 2, fake_clues #2: 21 words\n",
            "\n",
            "üîß Batch fixing 4 invalid clues...\n",
            "  ‚úÖ Batch rewrite successful (attempt 1): All 4 clues now valid\n",
            "\n",
            "üìä Validation Summary for Test 1:\n",
            "  Total clues: 28\n",
            "  Compliant clues: 28\n",
            "  Fixed clues: 4\n",
            "  Failed fixes: 0\n",
            "  Compliance rate: 100.0%\n",
            "\n",
            "‚úÖ Test 1 completed successfully\n",
            "\n",
            "================================================================================\n",
            "Running test 2/10: Song - Bohemian Rhapsody - Queen\n",
            "================================================================================\n",
            "\n",
            "üìã Validating clues...\n",
            "\n",
            "Validating Round 1...\n",
            "\n",
            "Validating Round 2...\n",
            "\n",
            "üìä Validation Summary for Test 2:\n",
            "  Total clues: 28\n",
            "  Compliant clues: 28\n",
            "  Fixed clues: 0\n",
            "  Failed fixes: 0\n",
            "  Compliance rate: 100.0%\n",
            "\n",
            "‚úÖ Test 2 completed successfully\n",
            "\n",
            "================================================================================\n",
            "Running test 3/10: Book - Harry Potter and the Sorcerer's Stone\n",
            "================================================================================\n",
            "\n",
            "üìã Validating clues...\n",
            "\n",
            "Validating Round 1...\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #1: 21 words\n",
            "  ‚ö†Ô∏è Round 1, fake_clues #1: 21 words\n",
            "\n",
            "Validating Round 2...\n",
            "  ‚ö†Ô∏è Round 2, informed_clues #8: 14 words\n",
            "  ‚ö†Ô∏è Round 2, misinformed_clues #2: 21 words\n",
            "  ‚ö†Ô∏è Round 2, fake_clues #1: 21 words\n",
            "\n",
            "üîß Batch fixing 5 invalid clues...\n",
            "  ‚úÖ Batch rewrite successful (attempt 1): All 5 clues now valid\n",
            "\n",
            "üìä Validation Summary for Test 3:\n",
            "  Total clues: 28\n",
            "  Compliant clues: 28\n",
            "  Fixed clues: 5\n",
            "  Failed fixes: 0\n",
            "  Compliance rate: 100.0%\n",
            "\n",
            "‚úÖ Test 3 completed successfully\n",
            "\n",
            "================================================================================\n",
            "Running test 4/10: TV Show - Breaking Bad\n",
            "================================================================================\n",
            "\n",
            "üìã Validating clues...\n",
            "\n",
            "Validating Round 1...\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #8: 21 words\n",
            "\n",
            "Validating Round 2...\n",
            "  ‚ö†Ô∏è Round 2, misinformed_clues #1: 22 words\n",
            "  ‚ö†Ô∏è Round 2, misinformed_clues #2: 22 words\n",
            "  ‚ö†Ô∏è Round 2, fake_clues #2: 21 words\n",
            "\n",
            "üîß Batch fixing 4 invalid clues...\n",
            "  ‚úÖ Batch rewrite successful (attempt 1): All 4 clues now valid\n",
            "\n",
            "üìä Validation Summary for Test 4:\n",
            "  Total clues: 28\n",
            "  Compliant clues: 28\n",
            "  Fixed clues: 4\n",
            "  Failed fixes: 0\n",
            "  Compliance rate: 100.0%\n",
            "\n",
            "‚úÖ Test 4 completed successfully\n",
            "\n",
            "================================================================================\n",
            "Running test 5/10: Video Game - The Legend of Zelda: Breath of the Wild\n",
            "================================================================================\n",
            "\n",
            "üìã Validating clues...\n",
            "\n",
            "Validating Round 1...\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #2: 14 words\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #4: 12 words\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #5: 13 words\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #6: 13 words\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #9: 14 words\n",
            "  ‚ö†Ô∏è Round 1, extra_clues #1: 14 words\n",
            "\n",
            "Validating Round 2...\n",
            "  ‚ö†Ô∏è Round 2, informed_clues #4: 12 words\n",
            "  ‚ö†Ô∏è Round 2, informed_clues #7: 12 words\n",
            "\n",
            "üîß Batch fixing 8 invalid clues...\n",
            "  ‚úÖ Batch rewrite successful (attempt 1): All 8 clues now valid\n",
            "\n",
            "üìä Validation Summary for Test 5:\n",
            "  Total clues: 28\n",
            "  Compliant clues: 28\n",
            "  Fixed clues: 8\n",
            "  Failed fixes: 0\n",
            "  Compliance rate: 100.0%\n",
            "\n",
            "‚úÖ Test 5 completed successfully\n",
            "\n",
            "================================================================================\n",
            "Running test 6/10: Food - Pizza Margherita\n",
            "================================================================================\n",
            "\n",
            "üìã Validating clues...\n",
            "\n",
            "Validating Round 1...\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #3: 14 words\n",
            "\n",
            "Validating Round 2...\n",
            "  ‚ö†Ô∏è Round 2, extra_clues #1: 14 words\n",
            "\n",
            "üîß Batch fixing 2 invalid clues...\n",
            "  ‚ö†Ô∏è Batch rewrite attempt 1: 1/2 clues still invalid\n",
            "  ‚úÖ Batch rewrite successful (attempt 2): All 2 clues now valid\n",
            "\n",
            "üìä Validation Summary for Test 6:\n",
            "  Total clues: 28\n",
            "  Compliant clues: 28\n",
            "  Fixed clues: 2\n",
            "  Failed fixes: 0\n",
            "  Compliance rate: 100.0%\n",
            "\n",
            "‚úÖ Test 6 completed successfully\n",
            "\n",
            "================================================================================\n",
            "Running test 7/10: Animal - African Elephant\n",
            "================================================================================\n",
            "\n",
            "üìã Validating clues...\n",
            "\n",
            "Validating Round 1...\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #1: 14 words\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #5: 13 words\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #6: 14 words\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #10: 14 words\n",
            "  ‚ö†Ô∏è Round 1, misinformed_clues #1: 14 words\n",
            "  ‚ö†Ô∏è Round 1, fake_clues #1: 14 words\n",
            "  ‚ö†Ô∏è Round 1, extra_clues #1: 14 words\n",
            "\n",
            "Validating Round 2...\n",
            "  ‚ö†Ô∏è Round 2, informed_clues #4: 14 words\n",
            "  ‚ö†Ô∏è Round 2, informed_clues #6: 13 words\n",
            "  ‚ö†Ô∏è Round 2, informed_clues #9: 14 words\n",
            "  ‚ö†Ô∏è Round 2, misinformed_clues #1: 13 words\n",
            "  ‚ö†Ô∏è Round 2, misinformed_clues #2: 13 words\n",
            "  ‚ö†Ô∏è Round 2, extra_clues #1: 14 words\n",
            "\n",
            "üîß Batch fixing 13 invalid clues...\n",
            "  ‚úÖ Batch rewrite successful (attempt 1): All 13 clues now valid\n",
            "\n",
            "üìä Validation Summary for Test 7:\n",
            "  Total clues: 30\n",
            "  Compliant clues: 30\n",
            "  Fixed clues: 13\n",
            "  Failed fixes: 0\n",
            "  Compliance rate: 100.0%\n",
            "\n",
            "‚úÖ Test 7 completed successfully\n",
            "\n",
            "================================================================================\n",
            "Running test 8/10: Sport - Tennis\n",
            "================================================================================\n",
            "\n",
            "üìã Validating clues...\n",
            "\n",
            "Validating Round 1...\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #1: 14 words\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #2: 14 words\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #3: 14 words\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #4: 14 words\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #6: 14 words\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #8: 14 words\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #9: 12 words\n",
            "  ‚ö†Ô∏è Round 1, misinformed_clues #1: 12 words\n",
            "  ‚ö†Ô∏è Round 1, fake_clues #1: 12 words\n",
            "\n",
            "Validating Round 2...\n",
            "  ‚ö†Ô∏è Round 2, informed_clues #2: 13 words\n",
            "  ‚ö†Ô∏è Round 2, fake_clues #2: 14 words\n",
            "\n",
            "üîß Batch fixing 11 invalid clues...\n",
            "  ‚úÖ Batch rewrite successful (attempt 1): All 11 clues now valid\n",
            "\n",
            "üìä Validation Summary for Test 8:\n",
            "  Total clues: 28\n",
            "  Compliant clues: 28\n",
            "  Fixed clues: 11\n",
            "  Failed fixes: 0\n",
            "  Compliance rate: 100.0%\n",
            "\n",
            "‚úÖ Test 8 completed successfully\n",
            "\n",
            "================================================================================\n",
            "Running test 9/10: Country - Japan\n",
            "================================================================================\n",
            "\n",
            "üìã Validating clues...\n",
            "\n",
            "Validating Round 1...\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #9: 14 words\n",
            "\n",
            "Validating Round 2...\n",
            "  ‚ö†Ô∏è Round 2, informed_clues #1: 13 words\n",
            "  ‚ö†Ô∏è Round 2, informed_clues #5: 14 words\n",
            "\n",
            "üîß Batch fixing 3 invalid clues...\n",
            "  ‚úÖ Batch rewrite successful (attempt 1): All 3 clues now valid\n",
            "\n",
            "üìä Validation Summary for Test 9:\n",
            "  Total clues: 28\n",
            "  Compliant clues: 28\n",
            "  Fixed clues: 3\n",
            "  Failed fixes: 0\n",
            "  Compliance rate: 100.0%\n",
            "\n",
            "‚úÖ Test 9 completed successfully\n",
            "\n",
            "================================================================================\n",
            "Running test 10/10: Historical Event - Moon Landing 1969\n",
            "================================================================================\n",
            "\n",
            "üìã Validating clues...\n",
            "\n",
            "Validating Round 1...\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #1: 14 words\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #2: 13 words\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #5: 12 words\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #6: 13 words\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #7: 13 words\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #8: 14 words\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #9: 13 words\n",
            "  ‚ö†Ô∏è Round 1, informed_clues #10: 13 words\n",
            "  ‚ö†Ô∏è Round 1, misinformed_clues #1: 13 words\n",
            "  ‚ö†Ô∏è Round 1, misinformed_clues #2: 21 words\n",
            "\n",
            "Validating Round 2...\n",
            "  ‚ö†Ô∏è Round 2, informed_clues #1: 14 words\n",
            "  ‚ö†Ô∏è Round 2, informed_clues #2: 11 words\n",
            "  ‚ö†Ô∏è Round 2, informed_clues #9: 14 words\n",
            "  ‚ö†Ô∏è Round 2, extra_clues #1: 13 words\n",
            "\n",
            "üîß Batch fixing 14 invalid clues...\n",
            "  ‚ö†Ô∏è Batch rewrite attempt 1: 2/14 clues still invalid\n",
            "  ‚ö†Ô∏è Batch rewrite attempt 2: 2/14 clues still invalid\n",
            "  ‚ö†Ô∏è Batch rewrite attempt 3: 2/14 clues still invalid\n",
            "  ‚ùå All batch rewrite attempts failed, keeping original clues\n",
            "  ‚ùå Failed to fix Round 1, informed_clues #1: Still 14 words\n",
            "  ‚ùå Failed to fix Round 1, informed_clues #2: Still 13 words\n",
            "  ‚ùå Failed to fix Round 1, informed_clues #5: Still 12 words\n",
            "  ‚ùå Failed to fix Round 1, informed_clues #6: Still 13 words\n",
            "  ‚ùå Failed to fix Round 1, informed_clues #7: Still 13 words\n",
            "  ‚ùå Failed to fix Round 1, informed_clues #8: Still 14 words\n",
            "  ‚ùå Failed to fix Round 1, informed_clues #9: Still 13 words\n",
            "  ‚ùå Failed to fix Round 1, informed_clues #10: Still 13 words\n",
            "  ‚ùå Failed to fix Round 1, misinformed_clues #1: Still 13 words\n",
            "  ‚ùå Failed to fix Round 1, misinformed_clues #2: Still 21 words\n",
            "  ‚ùå Failed to fix Round 2, informed_clues #1: Still 14 words\n",
            "  ‚ùå Failed to fix Round 2, informed_clues #2: Still 11 words\n",
            "  ‚ùå Failed to fix Round 2, informed_clues #9: Still 14 words\n",
            "  ‚ùå Failed to fix Round 2, extra_clues #1: Still 13 words\n",
            "\n",
            "üìä Validation Summary for Test 10:\n",
            "  Total clues: 32\n",
            "  Compliant clues: 18\n",
            "  Fixed clues: 0\n",
            "  Failed fixes: 14\n",
            "  Compliance rate: 56.2%\n",
            "\n",
            "‚úÖ Test 10 completed successfully\n",
            "\n",
            "================================================================================\n",
            "OVERALL VALIDATION SUMMARY\n",
            "================================================================================\n",
            "\n",
            "Test 1: Movie - Star Wars Episode I: The Phantom Menace\n",
            "  Compliance rate: 100.0%\n",
            "  Fixed: 4/28 clues\n",
            "\n",
            "Test 2: Song - Bohemian Rhapsody - Queen\n",
            "  Compliance rate: 100.0%\n",
            "  Fixed: 0/28 clues\n",
            "\n",
            "Test 3: Book - Harry Potter and the Sorcerer's Stone\n",
            "  Compliance rate: 100.0%\n",
            "  Fixed: 5/28 clues\n",
            "\n",
            "Test 4: TV Show - Breaking Bad\n",
            "  Compliance rate: 100.0%\n",
            "  Fixed: 4/28 clues\n",
            "\n",
            "Test 5: Video Game - The Legend of Zelda: Breath of the Wild\n",
            "  Compliance rate: 100.0%\n",
            "  Fixed: 8/28 clues\n",
            "\n",
            "Test 6: Food - Pizza Margherita\n",
            "  Compliance rate: 100.0%\n",
            "  Fixed: 2/28 clues\n",
            "\n",
            "Test 7: Animal - African Elephant\n",
            "  Compliance rate: 100.0%\n",
            "  Fixed: 13/30 clues\n",
            "\n",
            "Test 8: Sport - Tennis\n",
            "  Compliance rate: 100.0%\n",
            "  Fixed: 11/28 clues\n",
            "\n",
            "Test 9: Country - Japan\n",
            "  Compliance rate: 100.0%\n",
            "  Fixed: 3/28 clues\n",
            "\n",
            "Test 10: Historical Event - Moon Landing 1969\n",
            "  Compliance rate: 56.2%\n",
            "  Fixed: 0/32 clues\n"
          ]
        }
      ],
      "source": [
        "# Main execution with validation and fixing\n",
        "all_rows = []\n",
        "validation_summary = []\n",
        "\n",
        "for run_number, topic in enumerate(test_topics, 1):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Running test {run_number}/{len(test_topics)}: {topic['round_1']} - {topic['round_2']}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    messages = [\n",
        "        SystemMessage(system_prompt + output_format + one_shot_example),\n",
        "        HumanMessage(json.dumps(topic)),\n",
        "    ]\n",
        "\n",
        "    response = model.invoke(messages)\n",
        "    clean_content = re.sub(r\"<think>.*?</think>\", \"\", response.content, flags=re.DOTALL).strip()\n",
        "    game_data = extract_json_from_response(clean_content)\n",
        "\n",
        "    if game_data:\n",
        "        try:\n",
        "            # Validate and fix clues\n",
        "            print(f\"\\nüìã Validating clues...\")\n",
        "            corrected_game_data, validation_report = validate_and_fix_game_data(\n",
        "                game_data, \n",
        "                model, \n",
        "                auto_fix=True  # Set to False to only validate without fixing\n",
        "            )\n",
        "            \n",
        "            # Print validation summary\n",
        "            print(f\"\\nüìä Validation Summary for Test {run_number}:\")\n",
        "            print(f\"  Total clues: {validation_report['total_clues']}\")\n",
        "            print(f\"  Compliant clues: {validation_report['compliant_clues']}\")\n",
        "            print(f\"  Fixed clues: {validation_report['fixed_clues']}\")\n",
        "            print(f\"  Failed fixes: {validation_report['failed_fixes']}\")\n",
        "            print(f\"  Compliance rate: {validation_report['compliance_rate']}\")\n",
        "            \n",
        "            # Store validation summary\n",
        "            validation_summary.append({\n",
        "                \"test_run\": run_number,\n",
        "                \"topic\": f\"{topic['round_1']} - {topic['round_2']}\",\n",
        "                **validation_report\n",
        "            })\n",
        "            \n",
        "            # Process the corrected game data\n",
        "            all_rows.extend(process_game_data(corrected_game_data, topic, run_number))\n",
        "            print(f\"\\n‚úÖ Test {run_number} completed successfully\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error processing data for test {run_number}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "    else:\n",
        "        print(f\"‚ùå No valid JSON found for test {run_number}\")\n",
        "        print(\"RAW:\", clean_content[:200])\n",
        "\n",
        "    sleep(5)  # Rate limiting\n",
        "\n",
        "# Print overall validation summary\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"OVERALL VALIDATION SUMMARY\")\n",
        "print(f\"{'='*80}\")\n",
        "for summary in validation_summary:\n",
        "    print(f\"\\nTest {summary['test_run']}: {summary['topic']}\")\n",
        "    print(f\"  Compliance rate: {summary['compliance_rate']}\")\n",
        "    print(f\"  Fixed: {summary['fixed_clues']}/{summary['total_clues']} clues\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6HdR9_0Ht1y",
        "outputId": "d5285434-a3c2-4fab-9c88-672d0a3d7c29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ CSV saved: 10_rounds_clues_analysis(gemini).csv\n",
            "‚úÖ Validation summary saved: validation_summary(gemini).csv\n",
            "\n",
            "Total rows generated: 286\n",
            "Total tests validated: 10\n"
          ]
        }
      ],
      "source": [
        "# Save to CSV\n",
        "with open(\"10_rounds_clues_analysis(gemini).csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    if all_rows:\n",
        "        writer = csv.DictWriter(f, fieldnames=all_rows[0].keys())\n",
        "        writer.writeheader()\n",
        "        writer.writerows(all_rows)\n",
        "        print(f\"‚úÖ CSV saved: 10_rounds_clues_analysis(gemini).csv\")\n",
        "\n",
        "# Save validation summary\n",
        "if validation_summary:\n",
        "    # Flatten the issues list for CSV\n",
        "    for summary in validation_summary:\n",
        "        summary['issues'] = '; '.join(summary.get('issues', []))\n",
        "    \n",
        "    with open(\"validation_summary(gemini).csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=validation_summary[0].keys())\n",
        "        writer.writeheader()\n",
        "        writer.writerows(validation_summary)\n",
        "        print(f\"‚úÖ Validation summary saved: validation_summary(gemini).csv\")\n",
        "\n",
        "print(f\"\\nTotal rows generated: {len(all_rows)}\")\n",
        "print(f\"Total tests validated: {len(validation_summary)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "VALIDATION STATISTICS\n",
            "================================================================================\n",
            "\n",
            "üìä Overall Statistics Across All Tests:\n",
            "  Total clues generated: 286\n",
            "  Initially compliant: 222 (77.6%)\n",
            "  Successfully fixed: 50\n",
            "  Failed to fix: 14\n",
            "  Final compliance rate: 95.1%\n",
            "\n",
            "üìà Per-Test Breakdown:\n",
            "  Test 1: 100.0% compliance (4 clues fixed) - Movie - Star Wars Episode I: The Phantom Menace\n",
            "  Test 2: 100.0% compliance (0 clues fixed) - Song - Bohemian Rhapsody - Queen\n",
            "  Test 3: 100.0% compliance (5 clues fixed) - Book - Harry Potter and the Sorcerer's Stone\n",
            "  Test 4: 100.0% compliance (4 clues fixed) - TV Show - Breaking Bad\n",
            "  Test 5: 100.0% compliance (8 clues fixed) - Video Game - The Legend of Zelda: Breath of the Wild\n",
            "  Test 6: 100.0% compliance (2 clues fixed) - Food - Pizza Margherita\n",
            "  Test 7: 100.0% compliance (13 clues fixed) - Animal - African Elephant\n",
            "  Test 8: 100.0% compliance (11 clues fixed) - Sport - Tennis\n",
            "  Test 9: 100.0% compliance (3 clues fixed) - Country - Japan\n",
            "  Test 10: 56.2% compliance (0 clues fixed) - Historical Event - Moon Landing 1969\n",
            "\n",
            "üèÜ Best performing test: Test 1 (100.0% compliance)\n",
            "   Topic: Movie - Star Wars Episode I: The Phantom Menace\n",
            "\n",
            "‚ö†Ô∏è Lowest performing test: Test 10 (56.2% compliance)\n",
            "   Topic: Historical Event - Moon Landing 1969\n",
            "\n",
            "‚ö†Ô∏è WARNING: 14 clues could not be fixed after retries\n",
            "   Consider manual review or increasing max_retries\n"
          ]
        }
      ],
      "source": [
        "# Analyze validation results\n",
        "if validation_summary:\n",
        "    import pandas as pd\n",
        "    \n",
        "    df_validation = pd.DataFrame(validation_summary)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"VALIDATION STATISTICS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Calculate overall statistics\n",
        "    total_clues = df_validation['total_clues'].sum()\n",
        "    total_compliant = df_validation['compliant_clues'].sum()\n",
        "    total_fixed = df_validation['fixed_clues'].sum()\n",
        "    total_failed = df_validation['failed_fixes'].sum()\n",
        "    overall_compliance = (total_compliant / total_clues * 100) if total_clues > 0 else 0\n",
        "    \n",
        "    print(f\"\\nüìä Overall Statistics Across All Tests:\")\n",
        "    print(f\"  Total clues generated: {total_clues}\")\n",
        "    print(f\"  Initially compliant: {total_compliant - total_fixed} ({(total_compliant - total_fixed) / total_clues * 100:.1f}%)\")\n",
        "    print(f\"  Successfully fixed: {total_fixed}\")\n",
        "    print(f\"  Failed to fix: {total_failed}\")\n",
        "    print(f\"  Final compliance rate: {overall_compliance:.1f}%\")\n",
        "    \n",
        "    print(f\"\\nüìà Per-Test Breakdown:\")\n",
        "    for _, row in df_validation.iterrows():\n",
        "        test_num = row['test_run']\n",
        "        topic = row['topic']\n",
        "        compliance = row['compliance_rate']\n",
        "        fixed = row['fixed_clues']\n",
        "        print(f\"  Test {test_num}: {compliance} compliance ({fixed} clues fixed) - {topic}\")\n",
        "    \n",
        "    # Identify best and worst performers\n",
        "    df_validation['compliance_numeric'] = df_validation['compliance_rate'].str.rstrip('%').astype(float)\n",
        "    best_test = df_validation.loc[df_validation['compliance_numeric'].idxmax()]\n",
        "    worst_test = df_validation.loc[df_validation['compliance_numeric'].idxmin()]\n",
        "    \n",
        "    print(f\"\\nüèÜ Best performing test: Test {int(best_test['test_run'])} ({best_test['compliance_rate']} compliance)\")\n",
        "    print(f\"   Topic: {best_test['topic']}\")\n",
        "    print(f\"\\n‚ö†Ô∏è Lowest performing test: Test {int(worst_test['test_run'])} ({worst_test['compliance_rate']} compliance)\")\n",
        "    print(f\"   Topic: {worst_test['topic']}\")\n",
        "    \n",
        "    if total_failed > 0:\n",
        "        print(f\"\\n‚ö†Ô∏è WARNING: {total_failed} clues could not be fixed after retries\")\n",
        "        print(\"   Consider manual review or increasing max_retries\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A0fc3WdZymW"
      },
      "source": [
        "## LLM analysis (llama)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ixZR4gtyhwU-"
      },
      "outputs": [],
      "source": [
        "analysis_model = init_chat_model(\n",
        "    # model=\"gemini-2.5-flash\",\n",
        "    model=\"gemini-2.0-flash-lite\",\n",
        "    model_provider=\"google_genai\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "_OurpVJ2h9eD"
      },
      "outputs": [],
      "source": [
        "def analyze_round_with_llm(round_data, analysis_model):\n",
        "    \"\"\"Analyze a single round using LLM\"\"\"\n",
        "\n",
        "    # Count words for each clue type\n",
        "    word_counts = {}\n",
        "    length_issues = []\n",
        "\n",
        "    for clue_type in [\"informed_clues\", \"misinformed_clues\", \"fake_clues\", \"extra_clues\"]:\n",
        "        clues = round_data.get(clue_type, [])\n",
        "        word_counts[clue_type] = []\n",
        "\n",
        "        for i, clue in enumerate(clues, 1):\n",
        "            word_count = len(clue.split())\n",
        "            word_counts[clue_type].append(word_count)\n",
        "\n",
        "            if not (15 <= word_count <= 20):\n",
        "                length_issues.append(f\"{clue_type} #{i}: {word_count} words\")\n",
        "\n",
        "    # Create word count summary\n",
        "    word_count_summary = f\"\"\"\n",
        "WORD COUNT ANALYSIS:\n",
        "- Informed clues: {word_counts.get('informed_clues', [])}\n",
        "- Misinformed clues: {word_counts.get('misinformed_clues', [])}\n",
        "- Fake clues: {word_counts.get('fake_clues', [])}\n",
        "- Extra clues: {word_counts.get('extra_clues', [])}\n",
        "\n",
        "LENGTH ISSUES (should be 15-20 words):\n",
        "{'; '.join(length_issues) if length_issues else 'All clues meet length requirements'}\n",
        "\"\"\"\n",
        "\n",
        "    analysis_prompt = f\"\"\"\n",
        "You are evaluating clues for a disinformer game. Analyze BOTH word count compliance AND content quality.\n",
        "\n",
        "{word_count_summary}\n",
        "\n",
        "ROUND DATA:\n",
        "{json.dumps(round_data, indent=2)}\n",
        "\n",
        "CRITICAL REQUIREMENTS:\n",
        "1. LENGTH COMPLIANCE: Each clue should be 15-20 words (see analysis above)\n",
        "2. ANSWER CONTAMINATION: Check if ANY clue contains the answer word\n",
        "3. SPECIFICITY: Are clues specific enough to distinguish from similar items?\n",
        "4. CLUE REFERENCES: Use specific clue numbers when noting issues\n",
        "5. DETAILED REASONING: Explain WHY you gave each score\n",
        "\n",
        "CLUE TYPE REQUIREMENTS:\n",
        "- **INFORMED CLUES**: Must relate to actual answer and be distinct/specific (avoid generic descriptions)\n",
        "- **MISINFORMED CLUES**: Must be related to actual answer BUT vague enough to apply to multiple choices (create productive doubt)\n",
        "- **FAKE CLUES**: Must clearly point to the OTHER answer choices, NOT the correct answer (effective misdirection)\n",
        "\n",
        "SCORING SCALE (MANDATORY):\n",
        "Rate based on how well each clue type fulfills its specific purpose:\n",
        "- informed_quality: Rate 1-5 (How well do they point to correct answer specifically?)\n",
        "- misinformed_quality: Rate 1-5 (Do they create ambiguity while staying answer-related?)\n",
        "- fake_quality: Rate 1-5 (Do they clearly misdirect to wrong answer choices?)\n",
        "- difficulty: Rate 1-5 (1=too easy, 2=easy, 3=just right, 4=hard, 5=too hard)\n",
        "\n",
        "Return ONLY this JSON format:\n",
        "{{\n",
        "  \"length_compliance_score\": number (1-5),\n",
        "  \"length_issues_found\": [\"list of specific length problems\"],\n",
        "  \"informed_quality\": number (1-5),\n",
        "  \"informed_notes\": \"detailed analysis with specific clue numbers\",\n",
        "  \"misinformed_quality\": number (1-5),\n",
        "  \"misinformed_notes\": \"detailed analysis of ambiguity effectiveness\",\n",
        "  \"fake_quality\": number (1-5),\n",
        "  \"fake_notes\": \"detailed analysis of misdirection effectiveness\",\n",
        "  \"diversity_issues\": [\"list specific problems found\"],\n",
        "  \"difficulty\": number (1-5),\n",
        "  \"difficulty_reasoning\": \"detailed explanation\",\n",
        "  \"overall_notes\": \"comprehensive summary\"\n",
        "}}\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = analysis_model.invoke([HumanMessage(analysis_prompt)])\n",
        "        return extract_json_from_response(response.content)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå LLM analysis failed: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "rdYVHNS9lv5r"
      },
      "outputs": [],
      "source": [
        "# Load data from your manual analysis CSV\n",
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file from your manual analysis\n",
        "df = pd.read_csv(\"10_rounds_clues_analysis(gemini).csv\")  # Change filename as needed\n",
        "\n",
        "# Group data by test_run and round to reconstruct round_data\n",
        "all_results = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ex9epMELiCFt",
        "outputId": "3acb14c5-49db-47e6-e947-a335d1e1b0ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyzing Test 1, Round 1: Movie - movie\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 1, Round 2: Movie - Star Wars Episode I: The Phantom Menace\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 2, Round 1: Song - song\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 2, Round 2: Song - Bohemian Rhapsody - Queen\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 3, Round 1: Book - book\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 3, Round 2: Book - Harry Potter and the Sorcerer's Stone\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 4, Round 1: TV Show - TV Show\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 4, Round 2: TV Show - Breaking Bad\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 5, Round 1: Video Game - video game\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 5, Round 2: Video Game - The Legend of Zelda: Breath of the Wild\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 6, Round 1: Food - food\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 6, Round 2: Food - Pizza Margherita\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 7, Round 1: Animal - animal\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 7, Round 2: Animal - African Elephant\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 8, Round 1: Sport - sport\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 8, Round 2: Sport - Tennis\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 9, Round 1: Country - country\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 9, Round 2: Country - Japan\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 10, Round 1: Historical Event - historical event\n",
            "  ‚úÖ Analyzed successfully\n",
            "Analyzing Test 10, Round 2: Historical Event - Moon Landing 1969\n",
            "  ‚úÖ Analyzed successfully\n"
          ]
        }
      ],
      "source": [
        "for (test_run, round_num), group in df.groupby(['test_run', 'round']):\n",
        "    # Skip disinformer instructions\n",
        "    clue_data = group[group['clue_type'] != 'disinformer_instruction']\n",
        "\n",
        "    if len(clue_data) == 0:\n",
        "        continue\n",
        "\n",
        "    # Get basic info\n",
        "    topic_category = clue_data['topic_category'].iloc[0]\n",
        "    topic_specific = clue_data['topic_specific'].iloc[0]\n",
        "    answer = clue_data['answer'].iloc[0]\n",
        "    choices = clue_data['choices'].iloc[0]\n",
        "\n",
        "    print(f\"Analyzing Test {test_run}, Round {round_num}: {topic_category} - {answer}\")\n",
        "\n",
        "    # Reconstruct round_data from CSV\n",
        "    round_data = {\n",
        "        \"answer\": answer,\n",
        "        \"choices\": choices.split(\" | \") if choices else [],\n",
        "        \"informed_clues\": clue_data[clue_data['clue_type'] == 'informed']['clue_text'].tolist(),\n",
        "        \"misinformed_clues\": clue_data[clue_data['clue_type'] == 'misinformed']['clue_text'].tolist(),\n",
        "        \"fake_clues\": clue_data[clue_data['clue_type'] == 'fake']['clue_text'].tolist(),\n",
        "        \"extra_clues\": clue_data[clue_data['clue_type'] == 'extra']['clue_text'].tolist()\n",
        "    }\n",
        "\n",
        "    # Analyze with LLM\n",
        "    analysis = analyze_round_with_llm(round_data, analysis_model)\n",
        "\n",
        "    if analysis:\n",
        "        result = {\n",
        "            \"test_run\": test_run,\n",
        "            \"topic_category\": topic_category,\n",
        "            \"topic_specific\": topic_specific,\n",
        "            \"round\": round_num,\n",
        "            \"answer\": answer,\n",
        "            \"choices\": choices,\n",
        "\n",
        "            # LLM Analysis Results\n",
        "            \"informed_quality\": analysis.get(\"informed_quality\", \"\"),\n",
        "            \"informed_notes\": analysis.get(\"informed_notes\", \"\"),\n",
        "            \"misinformed_quality\": analysis.get(\"misinformed_quality\", \"\"),\n",
        "            \"misinformed_notes\": analysis.get(\"misinformed_notes\", \"\"),\n",
        "            \"fake_quality\": analysis.get(\"fake_quality\", \"\"),\n",
        "            \"fake_notes\": analysis.get(\"fake_notes\", \"\"),\n",
        "            \"diversity_issues\": \"; \".join(analysis.get(\"diversity_issues\", [])),\n",
        "            \"difficulty\": analysis.get(\"difficulty\", \"\"),\n",
        "            \"difficulty_reasoning\": analysis.get(\"difficulty_reasoning\", \"\"),\n",
        "            \"overall_notes\": analysis.get(\"overall_notes\", \"\"),\n",
        "\n",
        "            # Word count and length compliance data\n",
        "            \"total_clues\": len(round_data[\"informed_clues\"]) + len(round_data[\"misinformed_clues\"]) + len(round_data[\"fake_clues\"]) + len(round_data[\"extra_clues\"]),\n",
        "            \"length_compliant_clues\": sum(1 for clue_type in [\"informed_clues\", \"misinformed_clues\", \"fake_clues\", \"extra_clues\"]\n",
        "                                        for clue in round_data[clue_type]\n",
        "                                        if 15 <= len(clue.split()) <= 20),\n",
        "            \"length_compliance_rate\": f\"{(sum(1 for clue_type in ['informed_clues', 'misinformed_clues', 'fake_clues', 'extra_clues'] for clue in round_data[clue_type] if 15 <= len(clue.split()) <= 20) / max(1, sum(len(round_data[clue_type]) for clue_type in ['informed_clues', 'misinformed_clues', 'fake_clues', 'extra_clues'])) * 100):.0f}%\",\n",
        "            \"avg_word_count\": round(sum(len(clue.split()) for clue_type in [\"informed_clues\", \"misinformed_clues\", \"fake_clues\", \"extra_clues\"] for clue in round_data[clue_type]) / max(1, sum(len(round_data[clue_type]) for clue_type in [\"informed_clues\", \"misinformed_clues\", \"fake_clues\", \"extra_clues\"])), 1)\n",
        "        }\n",
        "\n",
        "        all_results.append(result)\n",
        "        print(f\"  ‚úÖ Analyzed successfully\")\n",
        "    else:\n",
        "        print(f\"  ‚ùå Analysis failed\")\n",
        "\n",
        "    sleep(2)  # Rate limiting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xayzwc_0iO-L",
        "outputId": "f1735cba-482e-4fec-c580-03c4e264fb10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ LLM analysis complete! Saved 20 results to: llm_analysis_results(gemini).csv\n"
          ]
        }
      ],
      "source": [
        "# Save results\n",
        "if all_results:\n",
        "    with open(\"llm_analysis_results(gemini).csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=all_results[0].keys())\n",
        "        writer.writeheader()\n",
        "        writer.writerows(all_results)\n",
        "\n",
        "    print(f\"‚úÖ LLM analysis complete! Saved {len(all_results)} results to: llm_analysis_results(gemini).csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tabulate in c:\\users\\vincent\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.9.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "‚úÖ Generated analysis matrices for Test 1: clue_analysis_matrices\\test1_clue_analysis(gemini).md\n",
            "‚úÖ Generated analysis matrices for Test 2: clue_analysis_matrices\\test2_clue_analysis(gemini).md\n",
            "‚úÖ Generated analysis matrices for Test 3: clue_analysis_matrices\\test3_clue_analysis(gemini).md\n",
            "‚úÖ Generated analysis matrices for Test 4: clue_analysis_matrices\\test4_clue_analysis(gemini).md\n",
            "‚úÖ Generated analysis matrices for Test 5: clue_analysis_matrices\\test5_clue_analysis(gemini).md\n",
            "‚úÖ Generated analysis matrices for Test 6: clue_analysis_matrices\\test6_clue_analysis(gemini).md\n",
            "‚úÖ Generated analysis matrices for Test 7: clue_analysis_matrices\\test7_clue_analysis(gemini).md\n",
            "‚úÖ Generated analysis matrices for Test 8: clue_analysis_matrices\\test8_clue_analysis(gemini).md\n",
            "‚úÖ Generated analysis matrices for Test 9: clue_analysis_matrices\\test9_clue_analysis(gemini).md\n",
            "‚úÖ Generated analysis matrices for Test 10: clue_analysis_matrices\\test10_clue_analysis(gemini).md\n",
            "‚úÖ Overall performance by category saved: Disinformer_Game_Clues_Quality_Summary(gemini).MD\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "# Used by to_markdown function\n",
        "%pip install tabulate\n",
        "\n",
        "# Ensure the utility functions below exist in your notebook cell.\n",
        "def calculate_length_compliance(row):\n",
        "    compliance_rate = int(row['length_compliance_rate'].rstrip('%'))\n",
        "    total_clues = row['total_clues']\n",
        "    compliant = row['length_compliant_clues']\n",
        "    non_compliant = total_clues - compliant\n",
        "    return compliance_rate, compliant, non_compliant, total_clues\n",
        "\n",
        "def get_pass_fail_status(compliance_rate):\n",
        "    return \"‚úÖ PASS\" if compliance_rate >= 80 else \"‚ùå FAIL\"\n",
        "\n",
        "def get_quality_assessment(score):\n",
        "    assessments = {\n",
        "        1: \"Poor - Needs significant revision\",\n",
        "        2: \"Fair - Below expectations\",\n",
        "        3: \"Good - Meets requirements\",\n",
        "        4: \"Very Good - Exceeds expectations\",\n",
        "        5: \"Excellent - Outstanding\"\n",
        "    }\n",
        "    return assessments.get(int(score), \"Unknown\")\n",
        "\n",
        "def get_difficulty_assessment(difficulty):\n",
        "    difficulty = int(difficulty)\n",
        "    if difficulty <= 2:\n",
        "        return \"üü¢ Too Easy\"\n",
        "    elif difficulty == 3:\n",
        "        return \"üü¢ Just Right\"\n",
        "    else:\n",
        "        return \"üü† Too Hard\"\n",
        "\n",
        "def extract_issues(notes_str):\n",
        "    import pandas as pd\n",
        "    if pd.isna(notes_str):\n",
        "        return [\"None identified\"]\n",
        "    notes_str = str(notes_str).lower()\n",
        "    issues = []\n",
        "    keywords = {\n",
        "        \"length\": \"Word count compliance issues\",\n",
        "        \"generic\": \"Generic/vague clues\",\n",
        "        \"diversity\": \"Lack of diversity in themes\",\n",
        "        \"ambiguity\": \"Insufficient ambiguity in misinformed clues\",\n",
        "        \"specificity\": \"Missing specificity in clues\",\n",
        "        \"answer contamination\": \"Answer word revealed in clues\"\n",
        "    }\n",
        "    for keyword, issue in keywords.items():\n",
        "        if keyword in notes_str:\n",
        "            issues.append(issue)\n",
        "    return issues if issues else [\"Minor issues noted\"]\n",
        "\n",
        "def generate_matrix_for_round(row):\n",
        "    test_run = int(row['test_run'])\n",
        "    topic_cat = row['topic_category']\n",
        "    topic_spec = row['topic_specific']\n",
        "    round_num = int(row['round'])\n",
        "    compliance_rate, compliant, non_compliant, total = calculate_length_compliance(row)\n",
        "    status = get_pass_fail_status(compliance_rate)\n",
        "\n",
        "    # Handle NaN values with defaults\n",
        "    informed_score = int(row['informed_quality']) if not pd.isna(row['informed_quality']) else 3\n",
        "    misinformed_score = int(row['misinformed_quality']) if not pd.isna(row['misinformed_quality']) else 3\n",
        "    fake_score = int(row['fake_quality']) if not pd.isna(row['fake_quality']) else 3\n",
        "    difficulty = int(row['difficulty']) if not pd.isna(row['difficulty']) else 3\n",
        "\n",
        "    issues = extract_issues(row['overall_notes'])\n",
        "    diversity_issues = row['diversity_issues'] if not pd.isna(row['diversity_issues']) else \"None identified\"\n",
        "    \n",
        "    matrix = f\"\"\"# Game Clue Analysis Matrix\n",
        "**Test Run {test_run} | Round {round_num}: {topic_cat} ‚Üí {topic_spec}**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Length Compliance\n",
        "| Status | Criteria |\n",
        "|--------|----------|\n",
        "| {status} | Clues within 15-20 words |\n",
        "\n",
        "**Compliance Rate:** {compliance_rate}% ({compliant}/{total} clues)  \n",
        "**Outliers:** {non_compliant}/{total} clues failed  \n",
        "**Average Word Count:** {row['avg_word_count']} words\n",
        "\n",
        "**Assessment:** {\"‚úÖ Acceptable - Most clues meet length requirements\" if compliance_rate >= 80 else \"‚ùå Critical - Significant length violations require revision\"}\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Quality Scores (Rate 1-5)\n",
        "\n",
        "### Informed Clues: {informed_score}/5  \n",
        "**{get_quality_assessment(informed_score)}**\n",
        "\n",
        "{row['informed_notes']}\n",
        "\n",
        "‚úÖ Strengths:\n",
        "- Generally specific and relate to correct answer\n",
        "- Provide distinct perspectives where applicable\n",
        "\n",
        "‚ö†Ô∏è Concerns:\n",
        "- {row['diversity_issues'] if not pd.isna(row['diversity_issues']) else \"Minor thematic overlap observed\"}\n",
        "\n",
        "### Misinformed Clues: {misinformed_score}/5  \n",
        "**{get_quality_assessment(misinformed_score)}**\n",
        "\n",
        "{row['misinformed_notes']}\n",
        "\n",
        "‚úÖ Strengths:\n",
        "- Attempt to create ambiguity\n",
        "- Generally related to the correct answer\n",
        "\n",
        "‚ö†Ô∏è Concerns:\n",
        "- May need more subtle misdirection\n",
        "- Ambiguity effectiveness varies\n",
        "\n",
        "### Fake Clues: {fake_score}/5  \n",
        "**{get_quality_assessment(fake_score)}**\n",
        "\n",
        "{row['fake_notes']}\n",
        "\n",
        "‚úÖ Strengths:\n",
        "- Effectively misdirect to wrong answer choices\n",
        "- Clear deception without being obvious\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Diversity Check\n",
        "\n",
        "| Aspect | Status |\n",
        "|--------|--------|\n",
        "| Theme Coverage | {\"‚úÖ PASS\" if \"diversity\" not in diversity_issues.lower() else \"‚ùå FAIL\"} |\n",
        "| Clue Variation | {\"‚úÖ PASS\" if informed_score >= 3 else \"‚ùå FAIL\"} |\n",
        "| Angle Coverage | {\"‚úÖ PASS\" if non_compliant <= 2 else \"‚ùå FAIL\"} |\n",
        "\n",
        "**Issues Found:** {diversity_issues}\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Difficulty Rating\n",
        "\n",
        "| Score | Assessment |\n",
        "|-------|------------|\n",
        "| Rating | {difficulty}/5 - {get_difficulty_assessment(difficulty)} |\n",
        "\n",
        "**Reasoning:** {row['difficulty_reasoning']}\n",
        "\n",
        "---\n",
        "\n",
        "## Overall Assessment\n",
        "\n",
        "**Overall Quality Score:** {(informed_score + misinformed_score + fake_score) / 3:.1f}/5\n",
        "\n",
        "**Pass/Fail:** {\"‚úÖ PASS\" if compliance_rate >= 70 and (informed_score + misinformed_score + fake_score) / 3 >= 3 else \"‚ö†Ô∏è NEEDS REVISION\"}\n",
        "\n",
        "**Main Issues:**\n",
        "{chr(10).join(f\"- {issue}\" for issue in issues)}\n",
        "\n",
        "**Priority Actions:**\n",
        "1. {\"Address length compliance\" if compliance_rate < 80 else \"Minor length adjustments\"}\n",
        "2. {\"Enhance misinformed clue ambiguity\" if misinformed_score < 3 else \"Maintain misinformed clue quality\"}\n",
        "3. {\"Increase clue diversity\" if \"diversity\" in diversity_issues.lower() else \"Maintain current diversity\"}\n",
        "\n",
        "**Overall Notes:**  \n",
        "{row['overall_notes']}\n",
        "\n",
        "---\n",
        "\"\"\"\n",
        "    return matrix\n",
        "\n",
        "# --- Matrices Generation per Test Run ---\n",
        "csv_path = Path(\"llm_analysis_results(gemini).csv\")\n",
        "if not csv_path.exists():\n",
        "    print(f\"‚ùå Error: {csv_path} not found.\")\n",
        "else:\n",
        "    df = pd.read_csv(csv_path)\n",
        "    test_runs = df['test_run'].unique()\n",
        "    dir = Path(\"clue_analysis_matrices\")\n",
        "    dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    for test in sorted(test_runs):\n",
        "        group = df[df['test_run'] == test]\n",
        "        text = f\"# Analysis for Test {test}\\n\\n\"\n",
        "        \n",
        "        # Append matrices for each round in the test\n",
        "        rounds = sorted(group['round'].unique())\n",
        "        for r in rounds:\n",
        "            row = group[group['round'] == r].iloc[0]\n",
        "            matrix_text = generate_matrix_for_round(row)\n",
        "            text += matrix_text + \"\\n\\n\"\n",
        "        \n",
        "        # Append a round-by-round performance summary table for this test\n",
        "        text += \"## Round-by-Round Performance Summary\\n\\n\"\n",
        "        text += \"| Round | Length Compliance | Informed | Misinformed | Fake | Difficulty |\\n\"\n",
        "        text += \"|-------|-------------------|----------|-------------|------|------------|\\n\"\n",
        "        for r in rounds:\n",
        "            row = group[group['round'] == r].iloc[0]\n",
        "            length_comp = row['length_compliance_rate']\n",
        "            inf_score = row['informed_quality']\n",
        "            mis_score = row['misinformed_quality']\n",
        "            fake_score = row['fake_quality']\n",
        "            difficulty = row['difficulty']\n",
        "            text += f\"| {r} | {length_comp} | {inf_score}/5 | {mis_score}/5 | {fake_score}/5 | {difficulty}/5 |\\n\"\n",
        "        \n",
        "        # Save markdown for this test run\n",
        "        test_file = dir / f\"test{test}_clue_analysis(gemini).md\"\n",
        "        with open(test_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(text)\n",
        "        print(f\"‚úÖ Generated analysis matrices for Test {test}: {test_file}\")\n",
        "    \n",
        "    # --- Overall Performance Breakdown by Category ---\n",
        "    overall_summary = \"# Overall Performance Breakdown by Category\\n\\n\"\n",
        "    # Fill NaN values with default score of 3 before aggregation\n",
        "    df_clean = df.copy()\n",
        "    df_clean['informed_quality'] = pd.to_numeric(df_clean['informed_quality'], errors='coerce').fillna(3)\n",
        "    df_clean['misinformed_quality'] = pd.to_numeric(df_clean['misinformed_quality'], errors='coerce').fillna(3)\n",
        "    df_clean['fake_quality'] = pd.to_numeric(df_clean['fake_quality'], errors='coerce').fillna(3)\n",
        "    df_clean['difficulty'] = pd.to_numeric(df_clean['difficulty'], errors='coerce').fillna(3)\n",
        "\n",
        "    by_category = df_clean.groupby('topic_category').agg({\n",
        "        'length_compliance_rate': lambda x: f\"{int(x.str.rstrip('%').astype(int).mean()):.0f}%\",\n",
        "        'informed_quality': lambda x: f\"{x.astype(int).mean():.1f}/5\",\n",
        "        'misinformed_quality': lambda x: f\"{x.astype(int).mean():.1f}/5\",\n",
        "        'fake_quality': lambda x: f\"{x.astype(int).mean():.1f}/5\",\n",
        "        'difficulty': lambda x: f\"{x.astype(int).mean():.1f}/5\"\n",
        "    }).reset_index()\n",
        "    overall_summary += by_category.to_markdown(index=False)\n",
        "    \n",
        "    # Save overall summary to a markdown file\n",
        "    overall_file = Path(\"Disinformer_Game_Clues_Quality_Summary(gemini).MD\")\n",
        "    with open(overall_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(overall_summary)\n",
        "    print(f\"‚úÖ Overall performance by category saved: {overall_file}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
